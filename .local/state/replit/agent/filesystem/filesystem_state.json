{"file_contents":{"report_generator.py":{"content":"from datetime import datetime\nfrom typing import Dict, Any, List\nfrom format_utils import format_currency\nfrom fpdf import FPDF\n\n\ndef sanitize_for_pdf(text: str) -> str:\n    \"\"\"Replace Unicode characters that FPDF can't handle with ASCII equivalents.\"\"\"\n    if not text:\n        return \"\"\n    \n    replacements = {\n        'âœ“': '[+]',\n        'âœ—': '[x]',\n        'â†’': '->',\n        'âš ': '[!]',\n        'ðŸ’Ž': '',\n        'â›°ï¸': '',\n        'ðŸ’°': '',\n        'âš–ï¸': '',\n        'ðŸŒ¿': '',\n        'ðŸ“Š': '',\n        'âˆ’': '-',\n        'â€“': '-',\n        'â€”': '-',\n        '\"': '\"',\n        '\"': '\"',\n        ''': \"'\",\n        ''': \"'\",\n        'Î£': 'Sum',\n        'Ã—': 'x',\n    }\n    for unicode_char, ascii_char in replacements.items():\n        text = text.replace(unicode_char, ascii_char)\n    \n    try:\n        text = text.encode('latin-1', errors='ignore').decode('latin-1')\n    except:\n        pass\n    \n    if len(text) > 500:\n        text = text[:497] + \"...\"\n    \n    return text\n\n\nclass MiningDueDiligenceReport(FPDF):\n    \n    def header(self):\n        self.set_font('Arial', 'B', 16)\n        self.cell(0, 10, 'Mining Due Diligence Report', 0, 1, 'C')\n        self.ln(5)\n    \n    def footer(self):\n        self.set_y(-15)\n        self.set_font('Arial', 'I', 8)\n        self.cell(0, 10, f'Page {self.page_no()}', 0, 0, 'C')\n    \n    def chapter_title(self, title: str):\n        self.set_font('Arial', 'B', 14)\n        self.set_fill_color(200, 220, 255)\n        self.cell(0, 10, sanitize_for_pdf(title), 0, 1, 'L', True)\n        self.ln(4)\n    \n    def section_title(self, title: str):\n        self.set_font('Arial', 'B', 12)\n        self.cell(0, 8, sanitize_for_pdf(title), 0, 1, 'L')\n        self.ln(2)\n    \n    def body_text(self, text: str):\n        self.set_font('Arial', '', 11)\n        try:\n            self.multi_cell(190, 6, sanitize_for_pdf(text))\n        except:\n            self.multi_cell(190, 6, \"Error rendering text\")\n        self.ln(2)\n    \n    def add_bullet_list(self, items: List[str]):\n        self.set_font('Arial', '', 10)\n        for item in items:\n            try:\n                self.multi_cell(190, 6, sanitize_for_pdf(f'  - {item}'))\n            except:\n                continue\n\n\nclass ReportGenerator:\n    \n    @staticmethod\n    def generate_pdf_report(\n        project_name: str,\n        analysis: Dict[str, Any],\n        scoring_result: Dict[str, Any],\n        uploaded_files: List[str],\n        recommendations: List[str],\n        narrative: Dict[str, Any] = None,\n        comparables: List[Dict[str, Any]] = None,\n        sustainability_analysis: Dict[str, Any] = None,\n        sustainability_scoring: Dict[str, Any] = None,\n        advanced_valuation: Dict[str, Any] = None,\n        analysis_type: str = 'light_ai'\n    ) -> bytes:\n        try:\n            pdf = MiningDueDiligenceReport()\n            pdf.add_page()\n        except Exception as e:\n            print(f\"Error initializing PDF: {e}\")\n            raise\n        \n        # Add AI type indicator at the top\n        ai_type_display = \"Oreplot Advanced Analysis\" if analysis_type == 'advanced_ai' else \"Oreplot Light Analysis\"\n        pdf.set_font('Arial', 'I', 10)\n        pdf.set_text_color(100, 100, 100)\n        pdf.cell(0, 8, f'Analysis Type: {ai_type_display}', 0, 1, 'R')\n        pdf.set_text_color(0, 0, 0)\n        pdf.ln(2)\n        \n        pdf.chapter_title('Executive Summary')\n        pdf.section_title(f'Project: {sanitize_for_pdf(project_name)}')\n        pdf.body_text(f'Report Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n        pdf.ln(5)\n        \n        if narrative and narrative.get('executive_summary'):\n            pdf.section_title('Strategic Overview')\n            pdf.body_text(sanitize_for_pdf(narrative['executive_summary']))\n            pdf.ln(5)\n        \n        pdf.chapter_title('Dual Scoring System')\n        \n        pdf.section_title('Investment Score')\n        pdf.set_font('Arial', 'B', 24)\n        score_color = (0, 150, 0) if scoring_result['total_score'] >= 70 else (255, 140, 0) if scoring_result['total_score'] >= 50 else (200, 0, 0)\n        pdf.set_text_color(*score_color)\n        pdf.cell(0, 15, f\"{scoring_result['total_score']} / 100\", 0, 1, 'C')\n        pdf.set_text_color(0, 0, 0)\n        \n        pdf.set_font('Arial', 'B', 14)\n        pdf.cell(0, 10, f\"Risk Band: {scoring_result['risk_band']}\", 0, 1, 'C')\n        pdf.set_font('Arial', '', 11)\n        pdf.cell(0, 8, f\"Probability of Success: {scoring_result['probability_of_success']*100:.2f}%\", 0, 1, 'C')\n        pdf.ln(8)\n        \n        if sustainability_scoring:\n            pdf.section_title('Sustainability Score')\n            pdf.set_font('Arial', 'B', 24)\n            sust_score = sustainability_scoring['sustainability_score']\n            sust_color = (0, 150, 0) if sust_score >= 80 else (100, 180, 150) if sust_score >= 65 else (255, 140, 0) if sust_score >= 50 else (200, 0, 0)\n            pdf.set_text_color(*sust_color)\n            pdf.cell(0, 15, f\"{sust_score} / 100\", 0, 1, 'C')\n            pdf.set_text_color(0, 0, 0)\n            \n            pdf.set_font('Arial', 'B', 14)\n            pdf.cell(0, 10, f\"Rating: {sustainability_scoring['rating']}\", 0, 1, 'C')\n            pdf.set_font('Arial', '', 11)\n            pdf.cell(0, 8, sanitize_for_pdf(sustainability_scoring['description']), 0, 1, 'C')\n        \n        pdf.ln(5)\n        \n        pdf.chapter_title('Score Breakdown by Category')\n        \n        category_names = {\n            \"geology_prospectivity\": \"Geology / Prospectivity\",\n            \"resource_potential\": \"Resource Potential / Model Confidence\",\n            \"economics\": \"Economics / Unit-Cost & Upside\",\n            \"legal_title\": \"Legal & Title Risk\",\n            \"permitting_esg\": \"Permitting & ESG / Community\",\n            \"data_quality\": \"Data Quality & QAQC\"\n        }\n        \n        for cat_key, cat_contrib in scoring_result['category_contributions'].items():\n            cat_name = category_names.get(cat_key, cat_key)\n            pdf.section_title(f\"{cat_name}\")\n            pdf.set_font('Arial', '', 10)\n            pdf.cell(0, 6, f\"Raw Score: {cat_contrib['raw_score']}/10  |  Weight: {cat_contrib['weight']}%  |  Contribution: {cat_contrib['contribution']}\", 0, 1)\n            \n            cat_data = analysis.get('categories', {}).get(cat_key, {})\n            \n            if cat_data.get('rationale'):\n                pdf.set_font('Arial', 'I', 10)\n                try:\n                    pdf.multi_cell(190, 5, sanitize_for_pdf(f\"Rationale: {cat_data['rationale']}\"))\n                except:\n                    pass\n            \n            if cat_data.get('facts_found'):\n                pdf.set_font('Arial', '', 9)\n                pdf.cell(0, 5, \"Evidence Found:\", 0, 1)\n                for fact in cat_data['facts_found'][:5]:\n                    try:\n                        sanitized_fact = sanitize_for_pdf(str(fact))\n                        pdf.multi_cell(190, 5, f'  [+] {sanitized_fact}')\n                    except:\n                        continue\n            \n            if cat_data.get('missing_info'):\n                pdf.set_font('Arial', 'B', 9)\n                pdf.set_text_color(200, 0, 0)\n                pdf.cell(0, 5, \"Missing Information:\", 0, 1)\n                pdf.set_text_color(0, 0, 0)\n                pdf.set_font('Arial', '', 9)\n                for missing in cat_data['missing_info'][:5]:\n                    try:\n                        sanitized_missing = sanitize_for_pdf(str(missing))\n                        pdf.multi_cell(190, 5, f'  [x] {sanitized_missing}')\n                    except:\n                        continue\n            \n            pdf.ln(3)\n        \n        if sustainability_scoring and sustainability_analysis:\n            pdf.add_page()\n            pdf.chapter_title('Sustainability Category Breakdown')\n            \n            sustainability_category_names = {\n                \"environmental\": \"Environmental Performance\",\n                \"social\": \"Social Performance\",\n                \"governance\": \"Governance\",\n                \"climate\": \"Climate & Energy\"\n            }\n            \n            sust_categories = sustainability_analysis.get('sustainability_categories', {})\n            sust_contributions = sustainability_scoring.get('category_contributions', {})\n            \n            for cat_key, cat_contrib in sust_contributions.items():\n                cat_name = sustainability_category_names.get(cat_key, cat_key)\n                pdf.section_title(f\"{cat_name}\")\n                pdf.set_font('Arial', '', 10)\n                pdf.cell(0, 6, f\"Raw Score: {cat_contrib['raw_score']}/10  |  Weight: {cat_contrib['weight']}%  |  Contribution: {cat_contrib['contribution']}\", 0, 1)\n                \n                cat_data = sust_categories.get(cat_key, {})\n                \n                if cat_data.get('rationale'):\n                    pdf.set_font('Arial', 'I', 10)\n                    try:\n                        pdf.multi_cell(190, 5, sanitize_for_pdf(f\"Rationale: {cat_data['rationale']}\"))\n                    except:\n                        pass\n                \n                if cat_data.get('facts_found'):\n                    pdf.set_font('Arial', '', 9)\n                    pdf.cell(0, 5, \"Evidence Found:\", 0, 1)\n                    for fact in cat_data['facts_found'][:5]:\n                        try:\n                            sanitized_fact = sanitize_for_pdf(str(fact))\n                            pdf.multi_cell(190, 5, f'  [+] {sanitized_fact}')\n                        except:\n                            continue\n                \n                if cat_data.get('missing_info'):\n                    pdf.set_font('Arial', 'B', 9)\n                    pdf.set_text_color(200, 0, 0)\n                    pdf.cell(0, 5, \"Missing Information:\", 0, 1)\n                    pdf.set_text_color(0, 0, 0)\n                    pdf.set_font('Arial', '', 9)\n                    for missing in cat_data['missing_info'][:5]:\n                        try:\n                            sanitized_missing = sanitize_for_pdf(str(missing))\n                            pdf.multi_cell(190, 5, f'  [x] {sanitized_missing}')\n                        except:\n                            continue\n                \n                pdf.ln(3)\n            \n            if sustainability_analysis.get('overall_sustainability_notes'):\n                pdf.section_title('Overall Sustainability Assessment')\n                pdf.body_text(sanitize_for_pdf(sustainability_analysis['overall_sustainability_notes']))\n                pdf.ln(5)\n        \n        pdf.add_page()\n        \n        if narrative and narrative.get('strategic_signals'):\n            pdf.chapter_title('Strategic Signals & Context')\n            if narrative.get('project_timeline'):\n                pdf.section_title('Project Timeline')\n                pdf.body_text(sanitize_for_pdf(narrative['project_timeline']))\n                pdf.ln(3)\n            \n            if narrative.get('jurisdictional_context'):\n                pdf.section_title('Jurisdictional Context')\n                pdf.body_text(sanitize_for_pdf(narrative['jurisdictional_context']))\n                pdf.ln(3)\n            \n            pdf.section_title('Key Strategic Signals')\n            pdf.add_bullet_list(narrative['strategic_signals'])\n            pdf.ln(5)\n        \n        if narrative and narrative.get('strategic_recommendations'):\n            pdf.chapter_title('Strategic Recommendations')\n            pdf.add_bullet_list(narrative['strategic_recommendations'])\n            pdf.ln(5)\n        \n        pdf.chapter_title('Technical Recommendations & Next Steps')\n        pdf.add_bullet_list(recommendations)\n        pdf.ln(5)\n        \n        if comparables and len(comparables) > 0:\n            pdf.add_page()\n            pdf.chapter_title('Comparable Projects Benchmarking')\n            pdf.body_text(f\"This project has been benchmarked against {len(comparables)} similar mining projects:\")\n            pdf.ln(3)\n            \n            for idx, comp in enumerate(comparables, 1):\n                pdf.section_title(f\"{idx}. {sanitize_for_pdf(comp.get('name', 'Unknown'))}\")\n                pdf.set_font('Arial', '', 10)\n                \n                details = []\n                if comp.get('company'):\n                    details.append(f\"Company: {comp['company']}\")\n                if comp.get('location'):\n                    details.append(f\"Location: {comp['location']}\")\n                if comp.get('commodity'):\n                    details.append(f\"Commodity: {comp['commodity']}\")\n                if comp.get('project_stage'):\n                    details.append(f\"Stage: {comp['project_stage']}\")\n                if comp.get('geology_type'):\n                    details.append(f\"Deposit Type: {comp['geology_type']}\")\n                \n                pdf.multi_cell(190, 5, sanitize_for_pdf(' | '.join(details)))\n                pdf.ln(2)\n                \n                metrics = []\n                if comp.get('total_resource_mt'):\n                    metrics.append(f\"Resource: {comp['total_resource_mt']:.1f} Mt\")\n                if comp.get('grade') and comp.get('grade_unit'):\n                    metrics.append(f\"Grade: {comp['grade']:.2f} {comp['grade_unit']}\")\n                if comp.get('npv_millions_usd'):\n                    metrics.append(f\"NPV: ${comp['npv_millions_usd']:.0f}M\")\n                if comp.get('irr_percent'):\n                    metrics.append(f\"IRR: {comp['irr_percent']:.1f}%\")\n                if comp.get('capex_millions_usd'):\n                    metrics.append(f\"CAPEX: ${comp['capex_millions_usd']:.0f}M\")\n                \n                if metrics:\n                    pdf.set_font('Arial', 'I', 9)\n                    pdf.multi_cell(190, 5, sanitize_for_pdf('  ' + ' | '.join(metrics)))\n                \n                if comp.get('similarity_score'):\n                    pdf.set_font('Arial', 'B', 9)\n                    pdf.cell(0, 5, f\"  Similarity Score: {comp['similarity_score']*100:.0f}%\", 0, 1)\n                \n                pdf.ln(3)\n\n        if advanced_valuation:\n            pdf.add_page()\n            pdf.chapter_title('Advanced AI Valuation Analysis')\n            pdf.body_text(\"PwC-style mining valuation using multiple methodologies:\")\n            pdf.ln(3)\n            \n            if advanced_valuation.get('market_multiples'):\n                mm = advanced_valuation['market_multiples']\n                pdf.section_title('Market Multiples (EV/Resource)')\n                pdf.set_font('Arial', '', 10)\n                pdf.cell(0, 6, f\"Commodity: {mm.get('commodity', 'N/A')} | Category: {mm.get('resource_category', 'N/A')}\", 0, 1)\n                pdf.cell(0, 6, f\"Resource Estimate: {mm.get('resource_estimate', 0):,.0f}\", 0, 1)\n                pdf.cell(0, 6, f\"Base Multiple: ${mm.get('base_multiple', 0):.2f}/unit | Adjusted Multiple: ${mm.get('final_multiple', 0):.2f}/unit\", 0, 1)\n                \n                if mm.get('value_range'):\n                    vr = mm['value_range']\n                    pdf.set_font('Arial', 'B', 11)\n                    pdf.cell(0, 8, f\"Implied Value: {format_currency(vr.get('mid', 0)/1e6, decimals=1)} (Range: {format_currency(vr.get('low', 0)/1e6, decimals=1)} - {format_currency(vr.get('high', 0)/1e6, decimals=1)})\", 0, 1)\n                pdf.ln(3)\n            \n            if advanced_valuation.get('kilburn'):\n                kb = advanced_valuation['kilburn']\n                pdf.section_title('Kilburn Method (Cost Approach)')\n                pdf.set_font('Arial', '', 10)\n                \n                gr = kb.get('geoscientific_rating', {})\n                pdf.cell(0, 6, f\"Geoscientific Rating: {gr.get('composite_rating', 0):.2f}/4.0 ({gr.get('category', 'N/A').replace('_', ' ').title()})\", 0, 1)\n                pdf.cell(0, 6, f\"PEM Multiplier: {kb.get('pem', 0):.2f}x\", 0, 1)\n                \n                if kb.get('mee_valuation'):\n                    mee = kb['mee_valuation']\n                    pdf.cell(0, 6, f\"MEE Appraised Value: ${mee.get('appraised_value', 0):,.0f}\", 0, 1)\n                \n                if kb.get('bac_valuation'):\n                    bac = kb['bac_valuation']\n                    pdf.cell(0, 6, f\"BAC Appraised Value: ${bac.get('appraised_value', 0):,.0f}\", 0, 1)\n                \n                if kb.get('preferred_valuation'):\n                    pdf.set_font('Arial', 'B', 11)\n                    pdf.cell(0, 8, f\"Preferred Valuation: ${kb['preferred_valuation']:,.0f} ({kb.get('preferred_methodology', 'N/A')})\", 0, 1)\n                pdf.ln(3)\n            \n            if advanced_valuation.get('monte_carlo'):\n                mc = advanced_valuation['monte_carlo']\n                pdf.section_title('Monte Carlo Risk Modeling')\n                pdf.set_font('Arial', '', 10)\n                \n                inputs = mc.get('input_parameters', {})\n                pdf.cell(0, 6, f\"Commodity: {mc.get('commodity', 'N/A')} | Project Life: {inputs.get('project_life', 0)} years\", 0, 1)\n                pdf.cell(0, 6, f\"Simulations: {inputs.get('num_simulations', 0):,} | Discount Rate: {inputs.get('discount_rate', 0)*100:.1f}%\", 0, 1)\n                \n                npv = mc.get('npv_statistics', {})\n                pdf.ln(2)\n                pdf.set_font('Arial', 'B', 10)\n                pdf.cell(0, 6, \"NPV Distribution:\", 0, 1)\n                pdf.set_font('Arial', '', 10)\n                pdf.cell(0, 6, f\"  Mean NPV: {format_currency(npv.get('mean', 0)/1e6, decimals=1)} | Median NPV: {format_currency(npv.get('median', 0)/1e6, decimals=1)}\", 0, 1)\n                pdf.cell(0, 6, f\"  P10: {format_currency(npv.get('p10', 0)/1e6, decimals=1)} | P90: {format_currency(npv.get('p90', 0)/1e6, decimals=1)}\", 0, 1)\n                pdf.cell(0, 6, f\"  Probability of Positive NPV: {npv.get('prob_positive', 0)*100:.1f}%\", 0, 1)\n                pdf.cell(0, 6, f\"  Value at Risk (5%): {format_currency(npv.get('var_5', 0)/1e6, decimals=1)}\", 0, 1)\n                \n                if mc.get('real_options_value'):\n                    pdf.ln(2)\n                    pdf.set_font('Arial', 'B', 11)\n                    pdf.cell(0, 8, f\"Real Options Value: {format_currency(mc['real_options_value']/1e6, decimals=1)} (+{mc.get('option_premium_pct', 0):.0f}% vs static NPV)\", 0, 1)\n                pdf.ln(3)\n            \n            if advanced_valuation.get('valuation_summary'):\n                pdf.section_title('Valuation Summary')\n                summary = advanced_valuation['valuation_summary']\n                pdf.set_font('Arial', 'B', 12)\n                if summary.get('recommended_value'):\n                    pdf.cell(0, 10, f\"Recommended Fair Value: {format_currency(summary['recommended_value']/1e6, decimals=1)}\", 0, 1)\n                if summary.get('value_range'):\n                    pdf.set_font('Arial', '', 10)\n                    vr = summary['value_range']\n                    pdf.cell(0, 6, f\"Range: {format_currency(vr.get('low', 0)/1e6, decimals=1)} to {format_currency(vr.get('high', 0)/1e6, decimals=1)}\", 0, 1)\n                if summary.get('methodology_note'):\n                    pdf.set_font('Arial', 'I', 9)\n                    pdf.multi_cell(190, 5, sanitize_for_pdf(summary['methodology_note']))\n                pdf.ln(3)\n        \n        pdf.chapter_title('Documents Analyzed')\n        pdf.set_font('Arial', '', 10)\n        for i, file_name in enumerate(uploaded_files, 1):\n            pdf.cell(0, 6, sanitize_for_pdf(f\"{i}. {file_name}\"), 0, 1)\n        pdf.ln(5)\n        \n        if analysis.get('overall_observations'):\n            pdf.chapter_title('Overall Observations')\n            pdf.body_text(sanitize_for_pdf(analysis['overall_observations']))\n        \n        pdf.add_page()\n        pdf.chapter_title('Assessment Methodology')\n        pdf.body_text(\"This report uses a weighted scoring methodology with the following categories:\")\n        pdf.ln(2)\n        \n        methodology_items = [\n            \"Geology / Prospectivity (35% weight) - Geological favorability and ore body characteristics\",\n            \"Resource Potential / Model Confidence (20% weight) - Resource estimates and modeling quality\",\n            \"Economics / Unit-Cost & Upside (15% weight) - Financial projections and unit costs\",\n            \"Legal & Title Risk (10% weight) - Ownership clarity and concession validity\",\n            \"Permitting & ESG / Community (10% weight) - Permits status and community relations\",\n            \"Data Quality & QAQC (10% weight) - Sampling protocols and data integrity\"\n        ]\n        pdf.add_bullet_list(methodology_items)\n        pdf.ln(5)\n        \n        pdf.body_text(\"Formula: Investment Score = Sum(Score_i / 10 x Weight_i)\")\n        pdf.body_text(\"Probability of Success = Investment Score / 100\")\n        pdf.ln(5)\n        \n        pdf.section_title(\"Score Interpretation:\")\n        pdf.body_text(\"> 70: Favourable - Fast-track or term sheet\")\n        pdf.body_text(\"50-70: Moderate - Proceed to deeper due diligence\")\n        pdf.body_text(\"< 50: High Risk - Reject or restructure\")\n        \n        try:\n            # Use dest='S' to return as string instead of printing to stdout\n            pdf_output = pdf.output(dest='S').encode('latin-1')\n            return pdf_output\n        except Exception as e:\n            print(f\"Error generating PDF output: {e}\")\n            import traceback\n            traceback.print_exc()\n            raise\n","path":null,"size_bytes":21489,"size_tokens":null},"ai_analyzer.py":{"content":"import os\nimport json\nfrom typing import Dict, List, Any\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception\n\nAI_INTEGRATIONS_OPENAI_API_KEY = os.environ.get(\"AI_INTEGRATIONS_OPENAI_API_KEY\")\nAI_INTEGRATIONS_OPENAI_BASE_URL = os.environ.get(\"AI_INTEGRATIONS_OPENAI_BASE_URL\")\n\nopenai_client = OpenAI(\n    api_key=AI_INTEGRATIONS_OPENAI_API_KEY,\n    base_url=AI_INTEGRATIONS_OPENAI_BASE_URL\n)\n\n\ndef is_rate_limit_error(exception: BaseException) -> bool:\n    error_msg = str(exception)\n    return (\n        \"429\" in error_msg\n        or \"RATELIMIT_EXCEEDED\" in error_msg\n        or \"quota\" in error_msg.lower()\n        or \"rate limit\" in error_msg.lower()\n        or (hasattr(exception, \"status_code\") and exception.status_code == 429)\n    )\n\n\nclass MiningProjectAnalyzer:\n    \n    SCORING_CATEGORIES = {\n        \"geology_prospectivity\": {\n            \"name\": \"Geology / Prospectivity\",\n            \"weight\": 35,\n            \"description\": \"Geological favorability, mineralization indicators, ore body characteristics\"\n        },\n        \"resource_potential\": {\n            \"name\": \"Resource Potential / Model Confidence\",\n            \"weight\": 20,\n            \"description\": \"Resource estimates, geological modeling quality, confidence level\"\n        },\n        \"economics\": {\n            \"name\": \"Economics / Unit-Cost & Upside Case\",\n            \"weight\": 15,\n            \"description\": \"CAPEX/OPEX estimates, unit costs, financial projections, upside scenarios\"\n        },\n        \"legal_title\": {\n            \"name\": \"Legal & Title Risk\",\n            \"weight\": 10,\n            \"description\": \"Ownership clarity, concession validity, encumbrances, chain of title\"\n        },\n        \"permitting_esg\": {\n            \"name\": \"Permitting & ESG / Community\",\n            \"weight\": 10,\n            \"description\": \"Permits status, environmental compliance, community relations, social license\"\n        },\n        \"data_quality\": {\n            \"name\": \"Data Quality & QAQC\",\n            \"weight\": 10,\n            \"description\": \"Sampling protocols, assay quality, QA/QC procedures, data integrity\"\n        }\n    }\n    \n    SUSTAINABILITY_CATEGORIES = {\n        \"environmental\": {\n            \"name\": \"Environmental Performance\",\n            \"weight\": 35,\n            \"description\": \"Water management, biodiversity protection, tailings/waste management, emissions control\"\n        },\n        \"social\": {\n            \"name\": \"Social Performance\",\n            \"weight\": 30,\n            \"description\": \"Community relations, indigenous rights, worker safety, local employment, social license\"\n        },\n        \"governance\": {\n            \"name\": \"Governance\",\n            \"weight\": 20,\n            \"description\": \"Ethics, transparency, anti-corruption, stakeholder engagement, board oversight\"\n        },\n        \"climate\": {\n            \"name\": \"Climate & Energy\",\n            \"weight\": 15,\n            \"description\": \"Carbon footprint, renewable energy use, climate adaptation, emissions reduction targets\"\n        }\n    }\n    \n    @staticmethod\n    @retry(\n        stop=stop_after_attempt(7),\n        wait=wait_exponential(multiplier=1, min=2, max=128),\n        retry=retry_if_exception(is_rate_limit_error),\n        reraise=True\n    )\n    def analyze_documents(documents_text: List[Dict[str, str]]) -> Dict[str, Any]:\n        drill_databases = [doc for doc in documents_text if doc.get('is_drill_database')]\n        qaqc_scores = [db.get('qaqc_score', 0) for db in drill_databases if db.get('success')]\n        avg_qaqc_score = sum(qaqc_scores) / len(qaqc_scores) if qaqc_scores else None\n        \n        combined_text = \"\\n\\n\".join([\n            f\"=== Document: {doc['file_name']} ===\\n{doc['text']}\"\n            for doc in documents_text if doc.get('success', False)\n        ])\n        \n        if not combined_text.strip():\n            return {\n                \"error\": \"No valid text extracted from documents. Please check file formats.\",\n                \"categories\": {},\n                \"extraction_errors\": [doc['file_name'] for doc in documents_text if not doc.get('success', False)]\n            }\n        \n        qaqc_context = \"\"\n        if avg_qaqc_score is not None:\n            qaqc_context = f\"\\n\\nIMPORTANT: Drill database QAQC analysis has been performed automatically. The average QAQC score is {avg_qaqc_score}/10. Use this score as a strong indicator for the Data Quality & QAQC category (item 6 below). Consider the QAQC report findings in your rationale.\"\n        \n        training_context = \"\"\n        try:\n            from training_rag import build_enhanced_context, get_training_statistics\n            stats = get_training_statistics()\n            if stats.get('total_chunks', 0) > 0:\n                training_context = build_enhanced_context(\n                    document_text=combined_text[:10000],\n                    category=None,\n                    commodity=None\n                )\n        except Exception:\n            pass\n        \n        prompt = f\"\"\"You are a mining industry expert conducting due diligence on a mining project. Analyze the following documents using OBJECTIVE CRITERIA and EXPLICIT THRESHOLDS.\n\n{training_context}\n\nIMPORTANT: Search the ENTIRE document thoroughly for each category. NI 43-101 reports contain detailed information throughout. Pay special attention to:\n- Section 7: Geology and Mineralization (grades, intercepts, deposit types)\n- Section 14: Mineral Resource Estimates (resource tables, tonnage, grade)\n- Section 16-17: Mining and Recovery Methods  \n- Section 21-22: Costs and Economic Analysis (CAPEX, OPEX, NPV, IRR)\n- Tables and appendices often contain the quantitative data\n\nDocuments:\n{combined_text[:500000]}{qaqc_context}\n\nOBJECTIVE SCORING CRITERIA (0-10 scale):\n\nScore 9-10: Exceptional - ALL critical info present with high detail + quantitative data\nScore 7-8: Good - MOST critical info present with quantitative data, minor gaps only\nScore 5-6: Moderate - Key info present but lacks detail or has notable gaps\nScore 3-4: Weak - Significant gaps, minimal quantitative data, incomplete analysis\nScore 0-2: Poor - Critical info mostly missing, cannot assess project properly\n\nSCORING CATEGORIES:\n\n1. GEOLOGY / PROSPECTIVITY (35%)\n   Score 9-10 requires: Quantitative grades (g/t), thickness/continuity, deposit type classification, genetic model, maps/sections, best intercepts\n   Score 7-8 requires: Deposit type mentioned, some grades/intercepts, geological description\n   Score 5-6 requires: Multiple zones identified, geological sections present\n   Score 3-4 requires: Basic geology described, zones listed\n   Score 0-2: Minimal geological information\n\n2. RESOURCE POTENTIAL (20%)\n   Score 9-10 requires: Resource statement (tonnes, grade, oz by category), classification details, model validation, sensitivity analysis\n   Score 7-8 requires: Resource estimate present, domains defined, some validation\n   Score 5-6 requires: Resource modeling discussed, no detailed numbers\n   Score 3-4 requires: Mention of resource work, no estimates\n   Score 0-2: No resource information\n\n3. ECONOMICS (15%)\n   Score 9-10 requires: CAPEX, OPEX, unit costs, NPV, IRR, payback, sensitivity\n   Score 7-8 requires: Capital/operating costs present, some financial metrics\n   Score 5-6 requires: Economic sections exist, minimal numbers\n   Score 3-4 requires: Economics mentioned, no detail\n   Score 0-2: No economic information\n\n4. LEGAL & TITLE (10%)\n   Score 9-10 requires: Tenure IDs, expiry dates, ownership %, royalty terms, no disputes\n   Score 7-8 requires: Tenures mapped, ownership clear, agreements described\n   Score 5-6 requires: Property description, some tenure info\n   Score 3-4 requires: Basic location info, minimal title detail\n   Score 0-2: No title information\n\n5. PERMITTING & ESG (10%)\n   Score 9-10 requires: Permit list with status/expiry, baseline studies, community agreements, closure plans\n   Score 7-8 requires: Key permits identified, environmental/social sections present\n   Score 5-6 requires: Permitting discussed, some environmental info\n   Score 3-4 requires: Permits mentioned, minimal detail\n   Score 0-2: No permitting information\n\n6. DATA QUALITY & QAQC (10%)\n   Score 9-10 requires: QA/QC performance metrics (CRM pass rates, duplicate precision), certified labs, data audit\n   Score 7-8 requires: QA/QC procedures described, reputable labs, verification done\n   Score 5-6 requires: Sampling methods described, labs identified\n   Score 3-4 requires: Some mention of QA/QC\n   Score 0-2: No QA/QC information\n\nFor each category, provide:\n- A score from 0-10 using the EXACT thresholds above\n- List ALL specific facts found (be thorough - minimum 5 items for scores 7+)\n- List ALL missing critical information items\n- Brief rationale citing the threshold criteria\n\nReturn JSON:\n{{\n  \"project_name\": \"extracted project name or 'Unknown Project'\",\n  \"categories\": {{\n    \"geology_prospectivity\": {{\n      \"score\": <0-10>,\n      \"facts_found\": [\"fact1\", \"fact2\", ...],\n      \"missing_info\": [\"missing1\", \"missing2\", ...],\n      \"rationale\": \"brief explanation citing threshold\"\n    }},\n    \"resource_potential\": {{ ... }},\n    \"economics\": {{ ... }},\n    \"legal_title\": {{ ... }},\n    \"permitting_esg\": {{ ... }},\n    \"data_quality\": {{ ... }}\n  }},\n  \"overall_observations\": \"general notes about document quality\"\n}}\n\nCRITICAL: Match your score to the threshold criteria. If critical info is missing, you CANNOT give a high score.\"\"\"\n\n        try:\n            response = openai_client.chat.completions.create(\n                model=\"gpt-5.1\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                response_format={\"type\": \"json_object\"},\n                max_completion_tokens=8192,\n                reasoning_effort=\"high\"\n            )\n            \n            analysis = json.loads(response.choices[0].message.content or \"{}\")\n            return analysis\n        except Exception as e:\n            return {\n                \"error\": f\"AI analysis failed: {str(e)}. Please try again or contact support if the issue persists.\",\n                \"categories\": {}\n            }\n    \n    @staticmethod\n    def generate_recommendations(analysis: Dict[str, Any], score: float) -> List[str]:\n        recommendations = []\n        \n        if score >= 70:\n            recommendations.append(\"âœ“ Strong project - recommend proceeding to term sheet negotiation\")\n            recommendations.append(\"âœ“ Consider fast-track due diligence process\")\n        elif score >= 50:\n            recommendations.append(\"â†’ Moderate potential - proceed to deeper due diligence\")\n            recommendations.append(\"â†’ Recommend drill program or PEA to strengthen confidence\")\n            recommendations.append(\"â†’ Focus on filling critical data gaps identified below\")\n        else:\n            recommendations.append(\"âš  High risk - recommend restructuring deal terms\")\n            recommendations.append(\"âš  Consider farm-out, lower valuation, or request more data\")\n            recommendations.append(\"âš  Address major gaps before proceeding\")\n        \n        categories = analysis.get('categories', {})\n        for cat_key, cat_data in categories.items():\n            if cat_data.get('score', 0) < 5:\n                cat_name = MiningProjectAnalyzer.SCORING_CATEGORIES[cat_key]['name']\n                recommendations.append(f\"âš  Critical gap in {cat_name} - score {cat_data.get('score', 0)}/10\")\n        \n        return recommendations\n    \n    @staticmethod\n    @retry(\n        stop=stop_after_attempt(5),\n        wait=wait_exponential(multiplier=1, min=2, max=64),\n        retry=retry_if_exception(is_rate_limit_error),\n        reraise=True\n    )\n    def generate_executive_narrative(\n        documents_text: List[Dict[str, str]],\n        analysis: Dict[str, Any],\n        score: float\n    ) -> Dict[str, Any]:\n        combined_text = \"\\n\\n\".join([\n            f\"=== Document: {doc['file_name']} ===\\n{doc['text']}\"\n            for doc in documents_text if doc.get('success', False)\n        ])[:20000]\n        \n        project_name = analysis.get('project_name', 'Unknown Project')\n        \n        prompt = f\"\"\"You are a senior mining investment analyst creating an executive summary narrative for: {project_name}\n\nBased on the technical analysis (score: {score}/100) and document excerpts below, generate a strategic context narrative.\n\nDOCUMENTS:\n{combined_text}\n\nTECHNICAL SCORES:\n{json.dumps({k: v.get('score', 0) for k, v in analysis.get('categories', {}).items()}, indent=2)}\n\nGenerate a comprehensive executive narrative with:\n\n1. PROJECT TIMELINE & HISTORY (2-3 sentences)\n   - Key milestones (discovery, drilling campaigns, resource updates)\n   - Ownership evolution and transaction history\n   - Development stage progression\n\n2. JURISDICTIONAL CONTEXT (2-3 sentences)\n   - Country/region mining jurisdiction analysis\n   - Political stability and mining policy\n   - Infrastructure access and logistics\n   - Permitting regime (timeline, complexity)\n\n3. STRATEGIC SIGNALS (bullet list)\n   - Government funding or strategic support\n   - Critical minerals designation or defense interest\n   - Major partnerships or offtake agreements\n   - Proximity to infrastructure or producing mines\n   - Brownfield vs greenfield advantages\n\n4. STRATEGIC RECOMMENDATIONS (3-5 items)\n   - Investment timing considerations\n   - Deal structure guidance (JV, earn-in, outright acquisition)\n   - Ownership clarity actions needed\n   - Policy alignment opportunities\n   - Risk mitigation priorities\n\nReturn JSON:\n{{\n  \"executive_summary\": \"2-3 paragraph narrative tying together timeline, context, and positioning\",\n  \"project_timeline\": \"concise timeline text\",\n  \"jurisdictional_context\": \"jurisdiction analysis text\",\n  \"strategic_signals\": [\"signal1\", \"signal2\", ...],\n  \"strategic_recommendations\": [\"rec1\", \"rec2\", \"rec3\", \"rec4\", \"rec5\"]\n}}\n\nBe specific. Use evidence from documents. If information is missing, note it explicitly.\"\"\"\n\n        try:\n            response = openai_client.chat.completions.create(\n                model=\"gpt-5.1\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                response_format={\"type\": \"json_object\"},\n                max_completion_tokens=4096,\n                reasoning_effort=\"high\",\n                temperature=0\n            )\n            \n            narrative = json.loads(response.choices[0].message.content or \"{}\")\n            return narrative\n        except Exception as e:\n            return {\n                \"executive_summary\": f\"Project: {project_name}. Investment score: {score}/100. Narrative generation failed.\",\n                \"project_timeline\": \"Timeline information not available.\",\n                \"jurisdictional_context\": \"Jurisdictional analysis not available.\",\n                \"strategic_signals\": [],\n                \"strategic_recommendations\": [\n                    \"Review technical due diligence findings\",\n                    \"Conduct site visit for verification\",\n                    \"Engage with management for missing data\"\n                ]\n            }\n    \n    @staticmethod\n    @retry(\n        stop=stop_after_attempt(7),\n        wait=wait_exponential(multiplier=1, min=2, max=128),\n        retry=retry_if_exception(is_rate_limit_error),\n        reraise=True\n    )\n    def analyze_sustainability(documents_text: List[Dict[str, str]]) -> Dict[str, Any]:\n        \"\"\"\n        Analyze documents for sustainability performance across ESG categories.\n        Based on industry standards: ICMM, GRI, SASB, TSM frameworks.\n        \"\"\"\n        combined_text = \"\\n\\n\".join([\n            f\"=== Document: {doc['file_name']} ===\\n{doc['text']}\"\n            for doc in documents_text if doc.get('success', False)\n        ])\n        \n        if not combined_text.strip():\n            return {\n                \"error\": \"No valid text extracted for sustainability analysis\",\n                \"sustainability_categories\": {}\n            }\n        \n        prompt = f\"\"\"You are a sustainability expert conducting ESG due diligence on a mining project. Analyze the following documents for sustainability performance.\n\nDOCUMENTS:\n{combined_text[:30000]}\n\nSUSTAINABILITY CATEGORIES (based on ICMM, GRI, SASB standards):\n\n1. ENVIRONMENTAL PERFORMANCE (Weight: 35%)\n   - Water management (consumption, recycling, discharge quality)\n   - Biodiversity protection (baseline studies, impact mitigation, rehabilitation plans)\n   - Tailings & waste management (storage facility design, monitoring, closure plans)\n   - Air quality & emissions control (dust, particulates, SO2/NOx)\n   - Land use & rehabilitation commitments\n\n2. SOCIAL PERFORMANCE (Weight: 30%)\n   - Community relations & consultation programs\n   - Indigenous peoples rights & engagement (if applicable)\n   - Worker health & safety (fatality rates, LTIFR, safety protocols)\n   - Local employment & skills development\n   - Social impact assessments & mitigation plans\n   - Resettlement procedures (if applicable)\n\n3. GOVERNANCE (Weight: 20%)\n   - Corporate ethics & anti-corruption policies\n   - Board oversight of sustainability\n   - Stakeholder engagement & transparency\n   - Compliance & regulatory adherence\n   - Grievance mechanisms\n   - Public disclosure of sustainability data\n\n4. CLIMATE & ENERGY (Weight: 15%)\n   - GHG emissions baseline & reduction targets\n   - Energy consumption & renewable energy use\n   - Climate risk assessment & adaptation plans\n   - Scope 1, 2, 3 emissions disclosure\n   - Net-zero commitments or transition plans\n\nFor each category, provide:\n- A score from 0-10 (0=poor/no evidence, 10=industry-leading)\n- Key facts found supporting the score\n- Missing critical information\n- Brief rationale based on industry best practices\n\nReturn JSON:\n{{\n  \"sustainability_categories\": {{\n    \"environmental\": {{\n      \"score\": <0-10>,\n      \"facts_found\": [\"fact1\", \"fact2\", ...],\n      \"missing_info\": [\"missing1\", \"missing2\", ...],\n      \"rationale\": \"brief explanation\"\n    }},\n    \"social\": {{ ... }},\n    \"governance\": {{ ... }},\n    \"climate\": {{ ... }}\n  }},\n  \"overall_sustainability_notes\": \"general assessment of sustainability maturity\"\n}}\n\nIf information is sparse, give conservative scores (1-3) and list what's missing. Use evidence from documents.\"\"\"\n\n        try:\n            response = openai_client.chat.completions.create(\n                model=\"gpt-5.1\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                response_format={\"type\": \"json_object\"},\n                max_completion_tokens=8192,\n                reasoning_effort=\"high\"\n            )\n            \n            sustainability_analysis = json.loads(response.choices[0].message.content or \"{}\")\n            return sustainability_analysis\n        except Exception as e:\n            return {\n                \"error\": f\"Sustainability analysis failed: {str(e)}\",\n                \"sustainability_categories\": {}\n            }\n","path":null,"size_bytes":18956,"size_tokens":null},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"anthropic>=0.75.0\",\n    \"apscheduler>=3.11.1\",\n    \"bcrypt>=5.0.0\",\n    \"docx>=0.2.4\",\n    \"fpdf>=1.7.2\",\n    \"fpdf2>=2.8.5\",\n    \"numpy>=2.3.4\",\n    \"openai>=2.6.1\",\n    \"openpyxl>=3.1.5\",\n    \"pandas>=2.3.3\",\n    \"pdf2image>=1.17.0\",\n    \"pillow>=12.0.0\",\n    \"poppler-utils>=0.1.0\",\n    \"psutil>=7.1.3\",\n    \"psycopg2-binary>=2.9.11\",\n    \"pypdf2>=3.0.1\",\n    \"pytesseract>=0.3.13\",\n    \"python-docx>=1.2.0\",\n    \"requests>=2.32.5\",\n    \"sqlalchemy>=2.0.44\",\n    \"streamlit>=1.51.0\",\n    \"tenacity>=9.1.2\",\n]\n","path":null,"size_bytes":659,"size_tokens":null},"main.py":{"content":"def main():\n    print(\"Hello from repl-nix-workspace!\")\n\n\nif __name__ == \"__main__\":\n    main()\n","path":null,"size_bytes":96,"size_tokens":null},"database.py":{"content":"import os\nimport time\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, scoped_session, declarative_base\nfrom sqlalchemy.exc import OperationalError\nfrom contextlib import contextmanager\n\nDATABASE_URL = os.getenv('DATABASE_URL')\n\nengine = create_engine(\n    DATABASE_URL,\n    pool_size=2,\n    max_overflow=3,\n    pool_pre_ping=True,\n    pool_recycle=300,\n    connect_args={\n        'connect_timeout': 10,\n        'keepalives': 1,\n        'keepalives_idle': 30,\n        'keepalives_interval': 10,\n        'keepalives_count': 5,\n    },\n    echo=False\n)\n\nSessionLocal = scoped_session(\n    sessionmaker(autocommit=False, autoflush=False, bind=engine)\n)\n\nBase = declarative_base()\n\n@contextmanager\ndef get_db_session():\n    session = SessionLocal()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\ndef init_db():\n    from models import User, Project, Analysis, Document, ScoringTemplate, Team, TeamMember, ComparableProject, FinancialModel, FinancialScenario, CommodityPriceSnapshot, ComparableMatch, IngestionJob, ComparableIngestion, AdvancedValuation\n    Base.metadata.create_all(bind=engine)\n","path":null,"size_bytes":1228,"size_tokens":null},"replit.md":{"content":"# Mining Due Diligence AI Agent\n\n## Overview\n\nThis application is a Streamlit-based Mining Due Diligence AI Agent designed to analyze technical documents (PDFs, DOCX, XLSX) of mining projects. It generates dual quantified scores (Investment Score 0-100 and Sustainability Score 0-100) and produces comprehensive due diligence reports. The agent evaluates projects across weighted categories for both investment and sustainability, providing actionable recommendations. Key capabilities include a global comparables database, custom scoring templates, comprehensive financial modeling (NPV/IRR, sensitivity analysis, real-time commodity pricing), AI-powered strategic narratives, comparables benchmarking, and a two-tier AI system for standard and advanced valuation methodologies.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### UI/UX Decisions\n\nThe application utilizes Streamlit for an interactive, chat-style web interface with a modern white-themed UI, Inter typography, and blue/purple gradients. It features a professional landing page with full navigation.\n\n### Technical Implementations\n\nThe system is modular, with components for document extraction, AI analysis, scoring, report generation, financial modeling, market data, financial exports, and comparables matching, enabling independent testing and scalability.\n\n#### AI Analysis Component (Hybrid AI System)\n\nThe platform uses a hybrid AI approach with two complementary models:\n\n**Oreplot Light (GPT-5.1)**:\n- OpenAI's GPT-5.1 with `reasoning_effort=\"high\"` for all document extraction and analysis\n- Superior vision capabilities for PDF/image processing and table extraction\n- Reliable JSON structured output for financial data parsing\n- Optimized for mining technical reports, processing PDFs up to 500 pages\n- Enhanced prompts for NI 43-101 reports, resource/reserve statements, financial data, drill hole data\n\n**Oreplot Advanced (Hybrid: GPT-5.1 + Claude Opus 4.5)**:\n- GPT-5.1: Document extraction and financial data parsing (superior vision & JSON reliability)\n- Claude Opus 4.5: Investment narratives and strategic analysis (superior reasoning & writing)\n- Claude generates executive summaries, key value drivers, risk assessments, and investment theses\n- Fallback to GPT-5.1 for narratives if Claude unavailable\n\nBoth models use Replit AI Integrations (no separate API keys required - charges billed to credits). Robust retry logic with exponential backoff handles API rate limits.\n\n#### Document Processing Pipeline\n\nAdvanced multi-format processing leverages GPT-5.1 Vision for PDFs (up to 500 pages), DOCX/XLSX for native parsing, and high-detail extraction for images. An enhanced `pytesseract` OCR provides a fallback for improved accuracy on technical documents. The system supports bulk uploads with maximum 80 files per upload, processed in batches of 10 files with real-time progress tracking. This prevents timeouts and memory issues during large bulk uploads while maintaining stability. Specifically targets resource/reserve statements, financial tables, drill hole data, and production schedules while preserving structure and numerical accuracy. **Oreplot Advanced API text limit: 8MB per extraction** (respects OpenAI's 10.5MB message limit with safety margin).\n\n**Parallel Processing**: PDF page extraction uses `ThreadPoolExecutor` with 5 concurrent workers (`MAX_PARALLEL_PAGES=5`) for 5x faster processing. AI Training embeddings use 3 concurrent workers (`MAX_PARALLEL_EMBEDDINGS=3`) for faster document training while respecting API rate limits. Both Oreplot Light and Advanced benefit from parallel document extraction.\n\n#### Dual Scoring Engine\n\n**Investment Scoring**: A flexible, weighted categorical system evaluates projects across six customizable categories (Geology/Prospectivity, Resource Potential, Economics, Legal & Title, Permitting & ESG, Data Quality) to produce a normalized score (0-100).\n\n**Sustainability Scoring**: Evaluates projects against industry-standard ESG criteria across four weighted categories (Environmental Performance, Social Performance, Governance, Climate & Energy).\n\nBoth scores are calculated independently and displayed with category breakdowns, evidence traceability, and identified information gaps. A severity-based penalty engine ensures scoring consistency and accuracy by applying penalties for critical missing items while distinguishing from minor gaps, and validates scores based on evidence requirements.\n\n#### Financial Analysis & Valuation Module\n\nProvides comprehensive financial modeling, including NPV/IRR calculations, payback period, production profiles, cash flow generation, and sensitivity analysis. It integrates real-time commodity pricing and offers professional Excel export capabilities.\n\n#### Comparables Benchmarking\n\nAn intelligent project matching system uses a weighted similarity scoring (commodity, deposit style, development stage, jurisdiction, resource scale) to identify and present the top 3 most similar projects from a global database. This database is automatically updated weekly via an ingestion service that uses AI-powered research and an admin approval queue.\n\n#### Two-Tier AI System\n\nThe platform features a subscription-based two-tier AI system with clear visual differentiation throughout the UI:\n- **Light AI (Standard Tier)**: Includes all core platform capabilities (document analysis, dual scoring, financial analysis, comparables, PDF reports). Displayed with blue badges in the UI.\n- **Advanced AI (Professional Tier)**: Functions as an AI agent with document upload interface (up to 5GB), GPT-5.1 powered data extraction, and automatic valuation calculations. Displayed with purple badges in the UI. Includes 5 professional valuation methodologies:\n\nAdvanced AI includes 5 professional valuation methodologies:\n  1. **Probability-Weighted DCF**: Risk-adjusted NPV with stage-gate success probabilities\n  2. **Income Approach DCF**: Traditional cash flow projection model\n  3. **Monte Carlo Risk Modeling**: 10,000+ simulations with commodity price/cost uncertainty\n  4. **Cost Approach (Kilburn Method)**: Exploration asset floor value based on replacement cost\n  5. **Decision Tree EMV**: Stage-gate expected monetary value analysis\n\n**Analysis Type Tracking**: All analyses are tagged with their type ('light_ai' or 'advanced_ai') in the database. This enables:\n- Visual badges on Projects page showing which AI tier was used\n- Report listings with color-coded analysis type indicators\n- PDF reports with analysis type header (Light AI vs Advanced AI)\n- Advanced AI analyses can be saved to the project repository using \"Save to Projects\" feature\n\n#### Advanced AI Technical Implementation\n\nThe Advanced AI system (`advanced_ai_analyzer.py`) uses a unified analyzer that:\n1. Extracts financial and technical data from uploaded documents using structured GPT-5.1 prompts\n2. Runs all 5 valuation methods on extracted data with type-safe handling\n3. Generates AI-powered valuation narrative for investment committees\n4. Consolidates results with summary and cross-method recommendations\n\n**Strict Input Validation**: All valuation engines require THREE core inputs to calculate valuations:\n- `annual_production` - Annual production quantity (oz, tonnes, etc.)\n- `commodity_price` - Current or assumed commodity price\n- `operating_cost` or `AISC` - All-in sustaining cost or operating cost\n\nIf ANY of these inputs are missing or invalid, the engine returns an `insufficient_data` error instead of fabricating values. This prevents misleading valuations based on document-reported NPVs or synthetic defaults.\n\n**Valuation Engine Dependency Chain**:\n- Income DCF runs FIRST and calculates the base NPV from actual production/price/cost data\n- Probability DCF and Decision Tree EMV use the calculated NPV from Income DCF (NOT document-reported NPVs)\n- Monte Carlo requires all three inputs independently\n- Kilburn Method works independently for exploration asset floor values\n\n**Smart Data Normalization**: When extraction returns zero or missing values, the system attempts to derive them:\n- `annual_production` from life_of_mine_production / mine_life OR throughput Ã— grade Ã— recovery\n- `commodity_price` from commodity_price_assumption OR annual_revenue / annual_production\n- `operating_cost` from annual_opex / annual_production\n- Shows derivation notes to users when values are calculated from other fields\n\n**User Feedback**: When data is insufficient:\n- Each methodology shows specific missing inputs (not generic errors)\n- Missing Inputs Report section displays what's needed for complete valuations\n- Tips provided for what data to include in uploaded documents\n\nAll valuation engines include robust type safety with:\n- `safe_float()` and `safe_int()` helper functions to prevent crashes on None/missing data\n- Division-by-zero guards using conditional defaults for non-critical values\n- Graceful degradation when OpenAI API is not configured\n- Short-circuit logic when extraction fails (returns error-only payload)\n\n### System Design Choices\n\nThe application is a comprehensive platform with full navigation, user management (dashboard, project management, report repository, settings), and session state management for seamless transitions.\n\n#### AI Training System (Enhanced RAG)\n\nAdmin-only feature for uploading training documents (up to 4 GB) to improve AI extraction accuracy for both Oreplot Light and Advanced. Uses Enhanced RAG (Retrieval-Augmented Generation) with vector embeddings for automatic learning.\n\n**How It Works:**\n1. Admin uploads high-quality mining documents (NI 43-101 reports, feasibility studies)\n2. System automatically chunks documents and creates vector embeddings using OpenAI text-embedding-3-small\n3. When analyzing new documents, the AI retrieves similar training content via cosine similarity\n4. Relevant training examples are automatically injected into prompts\n5. No manual approval workflow - just upload and the AI improves\n\n**Key Features:**\n- Simplified 3-tab interface: Upload & Train, Training Library, Statistics\n- Automatic document chunking (1500 chars with 200 char overlap)\n- Vector similarity search with 0.5 threshold\n- Minimum 2 results fallback when threshold not met\n- Unique file identification using content hash (prevents duplicate filename collisions)\n- Category and commodity filtering for targeted retrieval\n- Real-time statistics tracking\n\n**Categories:**\n- 30-Geoscience Data\n- 40-Spatial Data\n- 50-3D and Analysis\n- 60-Reporting\n- 70-Literature\n- 2024 Environmental Report on Ishkoday\n- Geoscience Data\n- Metallurgical Data\n- Technical Reports\n\n**Database Tables:**\n- `training_embeddings` - Stores document chunks with vector embeddings\n- `training_stats` - Global training statistics\n\n**Integration:**\n- `training_rag.py` - Core module for embedding creation, retrieval, and context building\n- Integrated into `ai_analyzer.py` (Oreplot Light) and `advanced_ai_analyzer.py` (Oreplot Advanced)\n- `build_enhanced_context()` retrieves relevant training content before analysis\n\n**Legacy Tables (deprecated):**\n- `training_collections`, `training_documents`, `training_chunks`, `training_examples`, `training_categories`\n\n## External Dependencies\n\n### AI Services\n- **OpenAI API**: For AI analysis and document text extraction (GPT-5.1).\n\n### Document Processing Libraries\n- **PyPDF2**: For PDF text extraction (fallback).\n- **python-docx**: For Microsoft Word document parsing.\n- **openpyxl**: For Excel spreadsheet reading and exports.\n- **pytesseract**: For OCR.\n- **Pillow (PIL)**: For image handling in OCR.\n\n### Web Framework\n- **Streamlit**: For the web application and UI.\n\n### Report Generation\n- **FPDF**: For creating and formatting PDF reports.\n\n### Reliability & Error Handling\n- **tenacity**: For retry logic with exponential backoff for API calls.\n\n### Database Interaction\n- **SQLAlchemy**: ORM for database interactions.\n\n### Market Data\n- **Metals-API**: For real-time commodity pricing.\n\n### Scheduler & Background Jobs\n- **APScheduler**: For automated weekly comparables database updates.","path":null,"size_bytes":12127,"size_tokens":null},"document_extractor.py":{"content":"import io\nimport os\nimport base64\nfrom typing import Dict, List, Any, Tuple\nfrom PIL import Image\nimport pytesseract\nimport PyPDF2\nfrom pdf2image import convert_from_bytes\nfrom docx import Document\nfrom openpyxl import load_workbook\nfrom drill_qaqc_analyzer import DrillQAQCAnalyzer\nfrom openai import OpenAI\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nMAX_PARALLEL_PAGES = 5\n\n\nclass DocumentExtractor:\n    \n    @staticmethod\n    def _get_openai_client():\n        \"\"\"Get OpenAI client using Replit AI Integrations or standard OpenAI\"\"\"\n        api_key = os.environ.get('AI_INTEGRATIONS_OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY')\n        \n        if not api_key:\n            raise ValueError(\"OpenAI API key not configured. Set AI_INTEGRATIONS_OPENAI_API_KEY or OPENAI_API_KEY environment variable.\")\n        \n        base_url = os.environ.get('AI_INTEGRATIONS_OPENAI_BASE_URL')\n        \n        if base_url:\n            return OpenAI(api_key=api_key, base_url=base_url)\n        else:\n            return OpenAI(api_key=api_key)\n    \n    @staticmethod\n    def _extract_text_with_vision(image_bytes: bytes, context: str = \"document\") -> str:\n        \"\"\"Use GPT-4 Vision to extract text from images with better accuracy, especially for tables and structured data\"\"\"\n        try:\n            client = DocumentExtractor._get_openai_client()\n            \n            base64_image = base64.b64encode(image_bytes).decode('utf-8')\n            \n            response = client.chat.completions.create(\n                model=\"gpt-5.1\",\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"text\",\n                                \"text\": f\"\"\"Extract ALL text from this {context} image with MAXIMUM ACCURACY. Pay special attention to:\n\nCRITICAL - TABLES & STRUCTURED DATA:\n- Resource/Reserve statements (tonnage, grade, contained metal by category: Measured, Indicated, Inferred, Proven, Probable)\n- Financial tables (CAPEX, OPEX, NPV, IRR, cash flows)\n- Drill hole data tables (collar, assay, survey data)\n- Production schedules and metallurgical test results\n\nFORMATTING RULES:\n1. Preserve table structure using | delimiters (e.g., \"Column1 | Column2 | Column3\")\n2. Keep numerical data EXACT (don't round or modify values)\n3. Include units (tonnes, g/t, oz, %, meters, etc.)\n4. Preserve headers and row labels\n5. Include footnotes and notes\n\nALSO EXTRACT:\n- All text, headings, and paragraphs\n- Charts and diagram labels\n- Technical specifications\n- Legal disclaimers\n\nReturn ONLY the extracted text without commentary or analysis.\"\"\"\n                            },\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                                    \"detail\": \"high\"\n                                }\n                            }\n                        ]\n                    }\n                ],\n                max_completion_tokens=4096,\n                reasoning_effort=\"high\"\n            )\n            \n            return response.choices[0].message.content.strip()\n        except ValueError as ve:\n            return f\"VLM extraction unavailable: {str(ve)}\"\n        except Exception as e:\n            return f\"VLM extraction failed: {str(e)}\"\n    \n    @staticmethod\n    def _process_single_page(args: Tuple[bytes, int]) -> Tuple[int, str]:\n        \"\"\"Process a single page image and return (page_num, text)\"\"\"\n        img_bytes, page_num = args\n        page_text = DocumentExtractor._extract_text_with_vision(img_bytes, f\"mining technical report page {page_num}\")\n        if page_text.startswith(\"VLM extraction failed\") or page_text.startswith(\"VLM extraction unavailable\"):\n            return (page_num, \"\")\n        return (page_num, f\"\\n\\n========== PAGE {page_num} ==========\\n{page_text}\\n\")\n    \n    @staticmethod\n    def extract_text_from_pdf(file_bytes: bytes) -> str:\n        \"\"\"Enhanced PDF extraction using GPT-4 Vision for maximum accuracy on tables and structured data\"\"\"\n        try:\n            # ALWAYS use VLM for technical/mining PDFs to ensure accurate table extraction\n            # PyPDF2 is notoriously poor at extracting tables, resource statements, and structured data\n            try:\n                # Get total page count first\n                pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_bytes))\n                total_pages = len(pdf_reader.pages)\n                \n                # Process up to 500 pages with VLM for comprehensive extraction\n                # Ensures complete coverage of NI 43-101 reports (typically 200-300 pages)\n                # including resource estimates, drill results, economics, and appendices\n                pages_to_process = min(total_pages, 500)\n                \n                # CRITICAL: Process in batches to avoid memory exhaustion\n                # Each batch processes 50 pages to keep memory usage bounded\n                BATCH_SIZE = 50\n                page_results = {}\n                \n                # Progress tracking for large documents\n                print(f\"Processing {pages_to_process} pages with GPT-5.1 Vision extraction (parallel mode: {MAX_PARALLEL_PAGES} concurrent)...\")\n                print(f\"Estimated cost: ${pages_to_process * 0.00096:.3f} | Time: ~{pages_to_process * 2 / MAX_PARALLEL_PAGES:.0f} seconds\")\n                \n                for batch_start in range(1, pages_to_process + 1, BATCH_SIZE):\n                    batch_end = min(batch_start + BATCH_SIZE - 1, pages_to_process)\n                    batch_num = (batch_start - 1) // BATCH_SIZE + 1\n                    total_batches = (pages_to_process + BATCH_SIZE - 1) // BATCH_SIZE\n                    \n                    print(f\"Processing batch {batch_num}/{total_batches} (pages {batch_start}-{batch_end}) in parallel...\")\n                    \n                    batch_images = convert_from_bytes(\n                        file_bytes, \n                        dpi=200, \n                        fmt='jpeg', \n                        first_page=batch_start, \n                        last_page=batch_end\n                    )\n                    \n                    page_tasks = []\n                    for i, img in enumerate(batch_images):\n                        page_num = batch_start + i\n                        img_byte_arr = io.BytesIO()\n                        img.save(img_byte_arr, format='JPEG', quality=90)\n                        img_bytes = img_byte_arr.getvalue()\n                        page_tasks.append((img_bytes, page_num))\n                    \n                    with ThreadPoolExecutor(max_workers=MAX_PARALLEL_PAGES) as executor:\n                        futures = {executor.submit(DocumentExtractor._process_single_page, task): task[1] for task in page_tasks}\n                        for future in as_completed(futures):\n                            page_num, page_text = future.result()\n                            if page_text:\n                                page_results[page_num] = page_text\n                    \n                    del batch_images\n                \n                vlm_text = \"\".join(page_results[pn] for pn in sorted(page_results.keys()))\n                print(f\"âœ“ Completed processing {pages_to_process} pages\")\n                \n                if vlm_text.strip():\n                    return vlm_text.strip()\n                    \n            except Exception as vlm_error:\n                # VLM failed, try PyPDF2 as fallback\n                pass\n            \n            # Fallback: Try standard text extraction if VLM completely failed\n            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_bytes))\n            text = \"\"\n            for page in pdf_reader.pages:\n                page_text = page.extract_text()\n                if page_text:\n                    text += page_text + \"\\n\"\n            \n            if text.strip():\n                return text.strip()\n            else:\n                return \"PDF text extraction incomplete - VLM and PyPDF2 both failed\"\n            \n        except Exception as e:\n            return f\"Error extracting PDF: {str(e)}\"\n    \n    @staticmethod\n    def extract_text_from_docx(file_bytes: bytes) -> str:\n        try:\n            doc = Document(io.BytesIO(file_bytes))\n            text = \"\"\n            for paragraph in doc.paragraphs:\n                text += paragraph.text + \"\\n\"\n            for table in doc.tables:\n                for row in table.rows:\n                    for cell in row.cells:\n                        text += cell.text + \" \"\n                    text += \"\\n\"\n            return text.strip()\n        except Exception as e:\n            return f\"Error extracting DOCX: {str(e)}\"\n    \n    @staticmethod\n    def extract_text_from_xlsx(file_bytes: bytes) -> str:\n        try:\n            wb = load_workbook(filename=io.BytesIO(file_bytes), read_only=True)\n            text = \"\"\n            for sheet_name in wb.sheetnames:\n                sheet = wb[sheet_name]\n                text += f\"\\n--- Sheet: {sheet_name} ---\\n\"\n                for row in sheet.iter_rows(values_only=True):\n                    row_text = \" | \".join([str(cell) if cell is not None else \"\" for cell in row])\n                    if row_text.strip():\n                        text += row_text + \"\\n\"\n            return text.strip()\n        except Exception as e:\n            return f\"Error extracting XLSX: {str(e)}\"\n    \n    @staticmethod\n    def extract_text_from_image(file_bytes: bytes) -> str:\n        \"\"\"Enhanced image text extraction using VLM for better accuracy\"\"\"\n        try:\n            # Convert image to JPEG if needed\n            image = Image.open(io.BytesIO(file_bytes))\n            \n            # Convert to RGB if needed (for RGBA images)\n            if image.mode in ('RGBA', 'LA', 'P'):\n                background = Image.new('RGB', image.size, (255, 255, 255))\n                if image.mode == 'P':\n                    image = image.convert('RGBA')\n                background.paste(image, mask=image.split()[-1] if image.mode == 'RGBA' else None)\n                image = background\n            \n            # Convert to bytes\n            img_byte_arr = io.BytesIO()\n            image.save(img_byte_arr, format='JPEG', quality=95)\n            img_bytes = img_byte_arr.getvalue()\n            \n            # Use VLM for extraction\n            vlm_text = DocumentExtractor._extract_text_with_vision(img_bytes, \"image\")\n            \n            # Check if VLM succeeded (not an error message)\n            if not (vlm_text.startswith(\"VLM extraction failed\") or vlm_text.startswith(\"VLM extraction unavailable\")):\n                return vlm_text\n            \n            # Fallback to pytesseract with enhanced configuration if VLM fails or is unavailable\n            # Enhanced OCR configuration for better accuracy on technical documents\n            custom_config = r'--oem 3 --psm 6 -c preserve_interword_spaces=1'\n            # OEM 3: Default, based on what is available (LSTM + legacy)\n            # PSM 6: Assume uniform block of text\n            # preserve_interword_spaces: Better handling of tables and structured data\n            text = pytesseract.image_to_string(image, config=custom_config)\n            return text.strip() if text.strip() else f\"Image text extraction incomplete\"\n            \n        except Exception as e:\n            return f\"Error extracting text from image: {str(e)}\"\n    \n    @staticmethod\n    def extract_text_from_txt(file_bytes: bytes) -> str:\n        try:\n            text = file_bytes.decode('utf-8', errors='ignore')\n            return text.strip()\n        except Exception as e:\n            return f\"Error extracting text: {str(e)}\"\n    \n    @staticmethod\n    def is_drill_database(file_name: str, file_bytes: bytes) -> bool:\n        \"\"\"Check if file appears to be a drill database\"\"\"\n        file_ext = file_name.lower().split('.')[-1]\n        \n        if file_ext not in ['csv', 'xlsx', 'xls']:\n            return False\n        \n        keywords = ['drill', 'hole', 'collar', 'assay', 'sample', 'from_depth', 'to_depth', \n                   'dhid', 'hole_id', 'holeid', 'downhole', 'survey', 'azimuth', 'dip']\n        \n        file_name_lower = file_name.lower()\n        return any(keyword in file_name_lower for keyword in keywords)\n    \n    @staticmethod\n    def process_drill_database(file_name: str, file_bytes: bytes) -> Dict[str, Any]:\n        \"\"\"Process drill database and perform QAQC analysis\"\"\"\n        try:\n            parsed_data = DrillQAQCAnalyzer.parse_drill_database(file_bytes, file_name)\n            \n            if 'error' in parsed_data:\n                return {\n                    'file_name': file_name,\n                    'file_type': 'drill_database',\n                    'success': False,\n                    'error': parsed_data['error']\n                }\n            \n            qaqc_results = DrillQAQCAnalyzer.perform_full_analysis(parsed_data)\n            \n            return {\n                'file_name': file_name,\n                'file_type': 'drill_database',\n                'success': True,\n                'is_drill_database': True,\n                'qaqc_results': qaqc_results,\n                'text': qaqc_results.get('qaqc_report', 'QAQC analysis completed'),\n                'qaqc_score': qaqc_results.get('qaqc_score', 0),\n                'qaqc_rationale': qaqc_results.get('qaqc_rationale', '')\n            }\n        \n        except Exception as e:\n            return {\n                'file_name': file_name,\n                'file_type': 'drill_database',\n                'success': False,\n                'error': f'Error processing drill database: {str(e)}'\n            }\n    \n    @staticmethod\n    def extract_text(file_name: str, file_bytes: bytes) -> Dict[str, str]:\n        file_ext = file_name.lower().split('.')[-1]\n        \n        if DocumentExtractor.is_drill_database(file_name, file_bytes):\n            return DocumentExtractor.process_drill_database(file_name, file_bytes)\n        \n        extractors = {\n            'pdf': DocumentExtractor.extract_text_from_pdf,\n            'docx': DocumentExtractor.extract_text_from_docx,\n            'doc': DocumentExtractor.extract_text_from_docx,\n            'xlsx': DocumentExtractor.extract_text_from_xlsx,\n            'xls': DocumentExtractor.extract_text_from_xlsx,\n            'csv': DocumentExtractor.extract_text_from_txt,\n            'txt': DocumentExtractor.extract_text_from_txt,\n            'jpg': DocumentExtractor.extract_text_from_image,\n            'jpeg': DocumentExtractor.extract_text_from_image,\n            'png': DocumentExtractor.extract_text_from_image,\n        }\n        \n        extractor = extractors.get(file_ext)\n        \n        if extractor:\n            extracted_text = extractor(file_bytes)\n            return {\n                'file_name': file_name,\n                'file_type': file_ext,\n                'text': extracted_text,\n                'success': not extracted_text.startswith('Error'),\n                'is_drill_database': False\n            }\n        else:\n            return {\n                'file_name': file_name,\n                'file_type': file_ext,\n                'text': '',\n                'success': False,\n                'error': f'Unsupported file type: {file_ext}',\n                'is_drill_database': False\n            }\n","path":null,"size_bytes":15520,"size_tokens":null},"app.py":{"content":"import streamlit as st\nfrom datetime import datetime\nfrom document_extractor import DocumentExtractor\nfrom ai_analyzer import MiningProjectAnalyzer\nfrom scoring_engine import ScoringEngine\nfrom report_generator import ReportGenerator\nfrom auth import require_auth, render_user_info\nfrom project_manager import ProjectManager\nfrom template_manager import TemplateManager\nfrom comparables_manager import ComparablesManager\nfrom database import SessionLocal\nfrom comparables_scheduler import start_scheduler\n\n# Import page modules\nfrom page_modules.dashboard_page import render_dashboard\nfrom page_modules.projects_page import render_projects_page\nfrom page_modules.reports_page import render_reports_page\nfrom page_modules.comparables_page import render_comparables_page\nfrom page_modules.financials_page import render_financials_page\nfrom page_modules.profile_page import render_profile_page\nfrom page_modules.account_settings_page import render_account_settings_page\nfrom page_modules.app_settings_page import render_app_settings_page\nfrom page_modules.billing_page import render_billing_page\nfrom page_modules.team_page import render_team_page\nfrom page_modules.login_page import render_login_page\nfrom page_modules.admin_panel_page import render_admin_panel_page\nfrom page_modules.advanced_ai_page import render_advanced_ai_page\nfrom components.navigation import render_top_navigation, render_user_menu_dropdown\nfrom ai_access_control import has_light_ai_access, has_advanced_ai_access, get_user_ai_features, LIGHT_AI_FEATURES, ADVANCED_AI_FEATURES\nst.set_page_config(\n    page_title=\"Oreplot - AI Mining Due Diligence\",\n    page_icon=\"â›ï¸\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\ncurrent_user = require_auth()\n\n# Session timeout disabled - users stay logged in via authentication\n# Sessions are now persistent until user explicitly logs out\n\n# If not authenticated, show login page\nif current_user is None:\n    render_login_page()\n    st.stop()\n\n# Initialize current_page in session state if not set\nif 'current_page' not in st.session_state:\n    st.session_state.current_page = 'dashboard'\n\n# Start the comparables update scheduler (runs once on app startup)\nif 'scheduler_started' not in st.session_state:\n    try:\n        start_scheduler()\n        st.session_state.scheduler_started = True\n    except Exception as e:\n        pass  # Silently handle scheduler errors to avoid disrupting the app\n\n# Render top navigation and get current page\ncurrent_page = render_top_navigation()\n\n# Render user menu in sidebar\nrender_user_menu_dropdown(current_user)\n\n\nst.sidebar.markdown(\"## ðŸ“‚ Navigation\")\n\ncol_a, col_b = st.sidebar.columns(2)\nwith col_a:\n    if st.button(\"âž• New Analysis\", use_container_width=True):\n        st.session_state.current_page = 'ai_agent'\n        st.session_state.view_mode = 'new_analysis'\n        st.session_state.current_project = None\n        st.session_state.analysis_result = None\n        st.rerun()\nwith col_b:\n    if st.button(\"âš™ï¸ Templates\", use_container_width=True):\n        st.session_state.current_page = 'ai_agent'\n        st.session_state.view_mode = 'template_manager'\n        st.rerun()\n\ncol_c, col_d = st.sidebar.columns(2)\nwith col_c:\n    if st.button(\"ðŸŒ Comparables\", use_container_width=True):\n        st.session_state.current_page = 'ai_agent'\n        st.session_state.view_mode = 'comparables'\n        st.rerun()\n\nst.sidebar.markdown(\"## ðŸ“‚ Projects\")\nuser_projects = ProjectManager.get_user_projects(current_user['id'])\n\nif user_projects:\n    selected_project_id = st.sidebar.selectbox(\n        \"Select a project to view\",\n        options=[None] + [p['id'] for p in user_projects],\n        format_func=lambda x: \"-- Select --\" if x is None else next((p['name'] for p in user_projects if p['id'] == x), \"Unknown\")\n    )\n    \n    if selected_project_id:\n        project = next((p for p in user_projects if p['id'] == selected_project_id), None)\n        if project:\n            st.sidebar.markdown(f\"**{project['name']}**\")\n            st.sidebar.markdown(f\"ðŸ“ {project['location'] or 'N/A'}\")\n            st.sidebar.markdown(f\"âš’ï¸ {project['commodity'] or 'N/A'}\")\n            st.sidebar.markdown(f\"ðŸ“Š {project['analysis_count']} analysis(es)\")\n            \n            if st.sidebar.button(\"View Project\", use_container_width=True):\n                st.session_state.current_page = 'ai_agent'\n                st.session_state.view_mode = 'view_project'\n                st.session_state.current_project = project\n                st.rerun()\nelse:\n    st.sidebar.info(\"No projects yet. Create your first analysis!\")\n\nst.markdown(\"\"\"\n<style>\n    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&display=swap');\n    \n    * {\n        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;\n    }\n    \n    .main {\n        background-color: #FFFFFF;\n    }\n    .stApp {\n        background-color: #FFFFFF;\n    }\n    \n    [data-testid=\"stSidebar\"] {\n        background-color: #F8F9FB;\n        border-right: 1px solid #E5E7EB;\n    }\n    \n    h1 {\n        color: #0F172A;\n        font-weight: 800;\n        letter-spacing: -0.8px;\n        font-size: 2.5rem;\n    }\n    h2, h3 {\n        color: #1E293B;\n        font-weight: 700;\n    }\n    h4 {\n        color: #334155;\n        font-weight: 600;\n    }\n    \n    .chat-message {\n        padding: 1.25rem;\n        border-radius: 12px;\n        margin-bottom: 1rem;\n        border: 1px solid #E5E7EB;\n        background-color: #FFFFFF;\n        box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.05);\n    }\n    .user-message {\n        border-left: 3px solid #7C3AED;\n        background-color: #F5F3FF;\n    }\n    .agent-message {\n        border-left: 3px solid #3B82F6;\n        background-color: #EFF6FF;\n    }\n    \n    .score-display {\n        font-size: 3.5rem;\n        font-weight: 900;\n        text-align: center;\n        padding: 2.5rem;\n        border-radius: 16px;\n        background: linear-gradient(135deg, #F0F9FF 0%, #EFF6FF 100%);\n        border: 2px solid #BFDBFE;\n        margin: 1.5rem 0;\n    }\n    .category-card {\n        background-color: #FFFFFF;\n        padding: 1.25rem;\n        border-radius: 12px;\n        margin: 0.75rem 0;\n        border: 1px solid #E5E7EB;\n        box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.04);\n        border-left: 4px solid #3B82F6;\n    }\n    \n    .stButton>button {\n        background: linear-gradient(135deg, #3B82F6, #8B5CF6);\n        color: #FFFFFF;\n        font-weight: 600;\n        border-radius: 10px;\n        border: none;\n        padding: 0.625rem 1.5rem;\n        font-size: 0.95rem;\n        transition: all 0.2s ease;\n        box-shadow: 0 4px 6px -1px rgba(59, 130, 246, 0.2);\n    }\n    .stButton>button:hover {\n        transform: translateY(-1px);\n        box-shadow: 0 8px 12px -1px rgba(59, 130, 246, 0.3);\n    }\n    \n    .file-upload-section {\n        background-color: #F8F9FB;\n        padding: 2rem;\n        border-radius: 16px;\n        border: 2px dashed #CBD5E1;\n    }\n    \n    .stMarkdown p, .stMarkdown li {\n        color: #475569;\n        line-height: 1.7;\n    }\n    \n    .stSelectbox label, .stTextInput label, .stTextArea label {\n        color: #334155 !important;\n        font-weight: 500 !important;\n    }\n    \n    div[data-testid=\"stExpander\"] {\n        background-color: #FFFFFF;\n        border: 1px solid #E5E7EB;\n        border-radius: 12px;\n    }\n    \n    .oreplot-logo {\n        font-size: 2.2rem;\n        font-weight: 900;\n        background: linear-gradient(135deg, #3B82F6, #8B5CF6);\n        -webkit-background-clip: text;\n        -webkit-text-fill-color: transparent;\n        background-clip: text;\n        letter-spacing: -0.05em;\n        margin-bottom: 0.5rem;\n    }\n    \n    .oreplot-subtitle {\n        color: #64748B;\n        font-size: 1.05rem;\n        font-weight: 400;\n        margin-bottom: 2rem;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)\n\n# Page Routing Logic\nif current_page == 'dashboard':\n    render_dashboard(current_user)\n\nelif current_page == 'projects':\n    render_projects_page(current_user)\n\nelif current_page == 'reports':\n    render_reports_page(current_user)\n\nelif current_page == 'financials':\n    render_financials_page(current_user)\n\nelif current_page == 'profile':\n    render_profile_page(current_user)\n\nelif current_page == 'account_settings':\n    render_account_settings_page(current_user)\n\nelif current_page == 'app_settings':\n    render_app_settings_page(current_user)\n\nelif current_page == 'billing':\n    render_billing_page(current_user)\n\nelif current_page == 'team':\n    render_team_page(current_user)\n\nelif current_page == 'admin_panel':\n    render_admin_panel_page(current_user)\n\nelif current_page == 'admin_comparables':\n    from page_modules.admin_comparables_page import render_admin_comparables_page\n    render_admin_comparables_page(current_user)\n\nelif current_page == 'ai_agent':\n    # AI Agent page - Two-tier AI system (Light AI + Advanced AI)\n\n    if 'history' not in st.session_state:\n        st.session_state.history = []\n    if 'uploaded_files_data' not in st.session_state:\n        st.session_state.uploaded_files_data = []\n    if 'analysis_result' not in st.session_state:\n        st.session_state.analysis_result = None\n    if 'current_project' not in st.session_state:\n        st.session_state.current_project = None\n    if 'view_mode' not in st.session_state:\n        st.session_state.view_mode = 'new_analysis'\n    if 'ai_tier_mode' not in st.session_state:\n        st.session_state.ai_tier_mode = 'light_ai'\n\n    ai_features = get_user_ai_features(current_user)\n    light_access = ai_features['light_ai']['enabled']\n    advanced_access = ai_features['advanced_ai']['enabled']\n\n    st.markdown('<div class=\"oreplot-subtitle\" style=\"margin-bottom: 1rem;\">AI-Powered Mining Due Diligence Platform</div>', unsafe_allow_html=True)\n    \n    tier_col1, tier_col2, tier_spacer = st.columns([1.5, 1.5, 3])\n    \n    with tier_col1:\n        light_disabled = not light_access\n        light_btn_type = \"primary\" if st.session_state.ai_tier_mode == 'light_ai' and not light_disabled else \"secondary\"\n        if st.button(\"ðŸ”µ Oreplot Light\", use_container_width=True, type=light_btn_type, \n                    disabled=light_disabled, help=\"Standard analysis, scoring, and reports\" if light_access else \"Access not available\"):\n            st.session_state.ai_tier_mode = 'light_ai'\n            st.session_state.view_mode = 'new_analysis'\n            st.rerun()\n    \n    with tier_col2:\n        advanced_disabled = not advanced_access\n        advanced_btn_type = \"primary\" if st.session_state.ai_tier_mode == 'advanced_ai' and not advanced_disabled else \"secondary\"\n        if st.button(\"ðŸŸ£ Oreplot Advanced\", use_container_width=True, type=advanced_btn_type,\n                    disabled=advanced_disabled, help=\"PwC valuation, Monte Carlo, market multiples\" if advanced_access else \"Upgrade required\"):\n            st.session_state.ai_tier_mode = 'advanced_ai'\n            st.rerun()\n    \n    st.markdown(\"---\")\n    \n    if st.session_state.ai_tier_mode == 'advanced_ai':\n        render_advanced_ai_page(current_user)\n    else:\n        col_btn1, col_btn2, col_btn3, col_spacer = st.columns([1, 1, 1.2, 2])\n        with col_btn1:\n            if st.button(\"ðŸ“ New Analysis\", use_container_width=True, \n                        type=\"primary\" if st.session_state.view_mode == 'new_analysis' else \"secondary\"):\n                st.session_state.view_mode = 'new_analysis'\n                st.session_state.current_project = None\n                st.session_state.analysis_result = None\n                st.rerun()\n        with col_btn2:\n            if st.button(\"âš™ï¸ Template Manager\", use_container_width=True,\n                        type=\"primary\" if st.session_state.view_mode == 'template_manager' else \"secondary\"):\n                st.session_state.view_mode = 'template_manager'\n                st.rerun()\n        with col_btn3:\n            if st.button(\"ðŸŒ Comparables DB\", use_container_width=True,\n                        type=\"primary\" if st.session_state.view_mode == 'comparables' else \"secondary\"):\n                st.session_state.view_mode = 'comparables'\n                st.rerun()\n        \n        st.markdown(\"---\")\n\n    if st.session_state.ai_tier_mode == 'light_ai' and st.session_state.view_mode == 'view_project' and st.session_state.current_project:\n        project = st.session_state.current_project\n        st.markdown(f\"### ðŸ”ï¸ {project['name']}\")\n    \n        col_info1, col_info2 = st.columns(2)\n        with col_info1:\n            st.markdown(f\"**ðŸ“ Location:** {project['location'] or 'N/A'}\")\n            st.markdown(f\"**âš’ï¸ Commodity:** {project['commodity'] or 'N/A'}\")\n        with col_info2:\n            st.markdown(f\"**ðŸ“… Created:** {project['created_at'].strftime('%Y-%m-%d %H:%M')}\")\n            st.markdown(f\"**ðŸ“Š Analyses:** {project['analysis_count']}\")\n    \n        if project.get('description'):\n            st.markdown(f\"**Description:** {project['description']}\")\n    \n        st.markdown(\"---\")\n        st.markdown(\"### ðŸ“‹ Analysis History\")\n    \n        analyses = ProjectManager.get_project_analyses(project['id'])\n    \n        if not analyses:\n            st.info(\"No analyses yet for this project.\")\n        else:\n            for analysis_summary in analyses:\n                with st.expander(f\"Analysis from {analysis_summary['created_at'].strftime('%Y-%m-%d %H:%M')} - Score: {analysis_summary['total_score']}/100\"):\n                    st.markdown(f\"**Risk Category:** {analysis_summary['risk_category']}\")\n                    st.markdown(f\"**Probability of Success:** {analysis_summary['probability_of_success']*100:.1f}%\")\n                \n                    if st.button(f\"View Full Analysis\", key=f\"view_analysis_{analysis_summary['id']}\"):\n                        full_analysis = ProjectManager.get_analysis_details(analysis_summary['id'])\n                    \n                        category_contributions = {}\n                        for cat_key, cat_data in full_analysis['categories'].items():\n                            category_contributions[cat_key] = {\n                                'raw_score': cat_data['score'],\n                                'weight': cat_data['weight'],\n                                'contribution': cat_data['contribution']\n                            }\n                    \n                        st.session_state.analysis_result = {\n                            'analysis': {\n                                'categories': full_analysis['categories'],\n                                'overall_observations': full_analysis.get('ai_analysis_raw', {}).get('overall_observations', '')\n                            },\n                            'scoring': {\n                                'total_score': full_analysis['total_score'],\n                                'risk_category': full_analysis['risk_category'],\n                                'risk_band': full_analysis['risk_category'],\n                                'probability_of_success': full_analysis['probability_of_success'],\n                                'recommendation': f\"Historical analysis from {full_analysis['created_at'].strftime('%Y-%m-%d')}\",\n                                'categories': full_analysis['categories'],\n                                'category_contributions': category_contributions\n                            },\n                            'recommendations': full_analysis['recommendations'],\n                            'sustainability_scoring': full_analysis.get('sustainability_scoring'),\n                            'sustainability_analysis': full_analysis.get('sustainability_analysis'),\n                            'project_id': full_analysis['project_id'],\n                            'analysis_id': full_analysis['id']\n                        }\n                        st.rerun()\n\n    elif st.session_state.ai_tier_mode == 'light_ai' and st.session_state.view_mode == 'new_analysis':\n        st.markdown(\"### ðŸ“ New Project Analysis\")\n    \n        with st.expander(\"ðŸ”ï¸ Project Information\", expanded=True):\n            project_name = st.text_input(\"Project Name *\", value=st.session_state.get('project_name', ''), key=\"project_name_input\")\n            project_description = st.text_area(\"Description (Optional)\", value=st.session_state.get('project_description', ''), placeholder=\"To make proper analysis, please provide as much detail about the project as possible.\", key=\"project_description_input\")\n\n    if st.session_state.ai_tier_mode == 'light_ai' and st.session_state.view_mode == 'new_analysis':\n        col1, col2 = st.columns([2, 1])\n\n        with col1:\n            st.markdown('<div class=\"file-upload-section\">', unsafe_allow_html=True)\n            st.markdown(\"#### ðŸ“ Upload Project Documents\")\n            st.markdown(\"Accepted formats: PDF, DOCX, XLSX, CSV, TXT, JPEG, PNG\")\n            st.markdown(\"ðŸ’¡ **Bulk Upload Support:** Upload entire data rooms with hundreds of documents (up to 5GB total)\")\n        \n            uploaded_files = st.file_uploader(\n                \"Drag and drop files here\",\n                accept_multiple_files=True,\n                type=['pdf', 'docx', 'doc', 'xlsx', 'xls', 'csv', 'txt', 'jpg', 'jpeg', 'png'],\n                label_visibility=\"collapsed\",\n                help=\"Upload individual files or entire folders. Maximum 5GB total, 1GB per file.\"\n            )\n            st.markdown('</div>', unsafe_allow_html=True)\n        \n            if uploaded_files:\n                total_size_mb = sum(f.size for f in uploaded_files) / (1024 * 1024)\n                st.markdown(f\"**{len(uploaded_files)} file(s) uploaded** ({total_size_mb:.1f} MB total)\")\n                \n                if len(uploaded_files) <= 10:\n                    for file in uploaded_files:\n                        st.markdown(f\"- `{file.name}` ({file.size / (1024*1024):.2f} MB)\")\n                else:\n                    with st.expander(f\"ðŸ“‹ View all {len(uploaded_files)} files\"):\n                        for file in uploaded_files:\n                            st.markdown(f\"- `{file.name}` ({file.size / (1024*1024):.2f} MB)\")\n\n        with col2:\n            st.markdown(\"#### ðŸŽ¯ Quick Actions\")\n        \n            # Template selection\n            user_templates = TemplateManager.get_user_templates(current_user['id'])\n            default_template = TemplateManager.get_default_template(current_user['id'])\n        \n            if user_templates:\n                template_options = [{'id': None, 'name': 'Standard Weights'}] + user_templates\n                default_index = 0\n                if default_template:\n                    default_index = next((i for i, t in enumerate(template_options) if t.get('id') == default_template['id']), 0)\n            \n                selected_template_idx = st.selectbox(\n                    \"Scoring Template\",\n                    range(len(template_options)),\n                    index=default_index,\n                    format_func=lambda i: template_options[i]['name'] + (' â­' if template_options[i].get('is_default') else ''),\n                    key=\"selected_template\"\n                )\n                selected_template = template_options[selected_template_idx]\n            else:\n                st.info(\"Using standard weights. Create custom templates in Template Manager.\")\n                selected_template = None\n        \n            can_generate = uploaded_files and st.session_state.get('project_name_input', '').strip()\n            if not st.session_state.get('project_name_input', '').strip():\n                st.warning(\"âš ï¸ Project name required\")\n    \n        if st.button(\"ðŸš€ Generate Analysis\", use_container_width=True, disabled=not can_generate):\n            total_files = len(uploaded_files)\n            \n            st.markdown(\"### ðŸ“Š Processing Progress\")\n            progress_bar = st.progress(0)\n            status_text = st.empty()\n            \n            extracted_docs = []\n            failed_files = []\n            \n            batch_size = 10\n            for batch_idx in range(0, total_files, batch_size):\n                batch_end = min(batch_idx + batch_size, total_files)\n                batch_files = uploaded_files[batch_idx:batch_end]\n                \n                status_text.markdown(f\"ðŸ” Processing batch {batch_idx // batch_size + 1} of {(total_files + batch_size - 1) // batch_size} ({batch_idx + 1}-{batch_end} of {total_files} files)...\")\n                \n                for idx, file in enumerate(batch_files):\n                    try:\n                        file_bytes = file.read()\n                        result = DocumentExtractor.extract_text(file.name, file_bytes)\n                        extracted_docs.append(result)\n                        if not result.get('success', False):\n                            failed_files.append(file.name)\n                        file.seek(0)\n                    except Exception as e:\n                        failed_files.append(file.name)\n                        extracted_docs.append({\n                            'file_name': file.name,\n                            'success': False,\n                            'error': str(e),\n                            'text': ''\n                        })\n                    \n                    current_progress = (batch_idx + idx + 1) / total_files\n                    progress_bar.progress(current_progress)\n            \n            progress_bar.progress(1.0)\n            status_text.markdown(f\"âœ… Processed {total_files} files ({len(extracted_docs) - len(failed_files)} successful, {len(failed_files)} failed)\")\n            \n            st.session_state.uploaded_files_data = extracted_docs\n            \n            drill_databases = [doc for doc in extracted_docs if doc.get('is_drill_database')]\n            if drill_databases:\n                st.success(f\"ðŸ”¬ Detected {len(drill_databases)} drill database(s) - QAQC analysis performed\")\n                for db in drill_databases:\n                    with st.expander(f\"ðŸ“Š QAQC Report: {db['file_name']}\", expanded=True):\n                        st.markdown(f\"**QAQC Score: {db.get('qaqc_score', 0)}/10**\")\n                        st.markdown(f\"_{db.get('qaqc_rationale', '')}_\")\n                        st.code(db.get('text', ''), language='text')\n            \n            if failed_files:\n                with st.expander(f\"âš ï¸ {len(failed_files)} file(s) could not be processed\", expanded=False):\n                    for failed_file in failed_files:\n                        st.markdown(f\"- `{failed_file}`\")\n            \n            st.session_state.history.append({\n                'type': 'user',\n                'content': f\"Uploaded {len(uploaded_files)} documents for analysis\",\n                'files': [f.name for f in uploaded_files],\n                'failed_files': failed_files,\n                'drill_databases': len(drill_databases),\n                'timestamp': datetime.now()\n            })\n        \n            with st.spinner(\"ðŸ¤– AI analyzing project data...\"):\n                analysis = MiningProjectAnalyzer.analyze_documents(extracted_docs)\n            \n                if analysis.get('error'):\n                    st.error(f\"âŒ {analysis['error']}\")\n                    if analysis.get('extraction_errors'):\n                        st.error(f\"Failed files: {', '.join(analysis['extraction_errors'])}\")\n                    st.session_state.analysis_result = None\n                else:\n                    # Get weights from selected template or use defaults\n                    custom_weights = None\n                    template_id = None\n                    if selected_template and selected_template.get('id'):\n                        custom_weights = TemplateManager.get_weights_dict(selected_template)\n                        template_id = selected_template['id']\n                \n                    scoring = ScoringEngine.calculate_investment_score(\n                        analysis.get('categories', {}),\n                        custom_weights=custom_weights\n                    )\n                    recommendations = MiningProjectAnalyzer.generate_recommendations(analysis, scoring['total_score'])\n                    \n                    with st.spinner(\"ðŸŒ± Analyzing sustainability & ESG performance...\"):\n                        sustainability_analysis = MiningProjectAnalyzer.analyze_sustainability(extracted_docs)\n                        \n                        if sustainability_analysis.get('error'):\n                            st.warning(f\"âš ï¸ Sustainability analysis partial: {sustainability_analysis['error']}\")\n                            sustainability_scoring = None\n                        else:\n                            sustainability_scoring = ScoringEngine.calculate_sustainability_score(\n                                sustainability_analysis.get('sustainability_categories', {})\n                            )\n                    \n                    with st.spinner(\"ðŸ“ Generating executive summary and strategic recommendations...\"):\n                        narrative = MiningProjectAnalyzer.generate_executive_narrative(\n                            extracted_docs,\n                            analysis,\n                            scoring['total_score']\n                        )\n                \n                    project = ProjectManager.create_project(\n                        user_id=current_user['id'],\n                        name=st.session_state.get('project_name_input', 'Untitled Project'),\n                        description=st.session_state.get('project_description_input', ''),\n                        location=None,\n                        commodity=None\n                    )\n                \n                    saved_analysis = ProjectManager.save_analysis(\n                        project_id=project['id'],\n                        analysis_data=analysis,\n                        scoring_data=scoring,\n                        recommendations=recommendations,\n                        scoring_template_id=template_id,\n                        narrative_data=narrative,\n                        sustainability_data=sustainability_analysis if not sustainability_analysis.get('error') else None,\n                        sustainability_scoring=sustainability_scoring,\n                        analysis_type='light_ai'\n                    )\n                \n                    ProjectManager.save_documents(project['id'], extracted_docs)\n                    \n                    with st.spinner(\"ðŸ” Finding comparable projects for benchmarking...\"):\n                        from comparables_matcher import ComparablesMatchingService\n                        comparables = ComparablesMatchingService.find_top_comparables(\n                            analysis_id=saved_analysis['id'],\n                            analysis_data=analysis,\n                            top_n=3\n                        )\n                \n                    st.session_state.analysis_result = {\n                        'analysis': analysis,\n                        'scoring': scoring,\n                        'recommendations': recommendations,\n                        'narrative': narrative,\n                        'comparables': comparables,\n                        'sustainability_analysis': sustainability_analysis if not sustainability_analysis.get('error') else None,\n                        'sustainability_scoring': sustainability_scoring,\n                        'project_id': project['id'],\n                        'analysis_id': saved_analysis['id']\n                    }\n                \n                    project_name = project['name']\n                    st.session_state.history.append({\n                        'type': 'agent',\n                        'content': f'Analysis complete and saved to project \"{project_name}\"',\n                        'timestamp': datetime.now()\n                    })\n                \n                    st.success(f\"âœ… Project '{project_name}' created and analysis saved!\")\n        \n            st.rerun()\n    \n        if st.session_state.analysis_result:\n            if st.button(\"ðŸ“„ Download PDF Report\", use_container_width=True):\n                result = st.session_state.analysis_result\n                project_name = result['analysis'].get('project_name', 'Mining Project')\n            \n                pdf_bytes = ReportGenerator.generate_pdf_report(\n                    project_name=project_name,\n                    analysis=result['analysis'],\n                    scoring_result=result['scoring'],\n                    uploaded_files=[f.name for f in uploaded_files] if uploaded_files else [],\n                    recommendations=result['recommendations'],\n                    narrative=result.get('narrative'),\n                    comparables=result.get('comparables', []),\n                    sustainability_analysis=result.get('sustainability_analysis'),\n                    sustainability_scoring=result.get('sustainability_scoring')\n                )\n            \n                st.download_button(\n                    label=\"â¬‡ï¸ Download Report\",\n                    data=pdf_bytes,\n                    file_name=f\"mining_dd_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\",\n                    mime=\"application/pdf\",\n                    use_container_width=True\n                )\n    \n        if st.button(\"ðŸ”„ Clear Session\", use_container_width=True):\n            st.session_state.history = []\n            st.session_state.uploaded_files_data = []\n            st.session_state.analysis_result = None\n            st.rerun()\n\n    st.markdown(\"---\")\n\n    if st.session_state.analysis_result:\n        result = st.session_state.analysis_result\n        scoring = result['scoring']\n        analysis = result['analysis']\n        sustainability_scoring = result.get('sustainability_scoring')\n    \n        st.markdown(\"### ðŸ“Š Dual Scoring Analysis Results\")\n        \n        col_inv, col_sust = st.columns(2)\n        \n        with col_inv:\n            st.markdown(\"#### ðŸ’° Investment Score\")\n            score_color = \"#00FF88\" if scoring['total_score'] >= 70 else \"#FFB800\" if scoring['total_score'] >= 50 else \"#FF4444\"\n            \n            st.markdown(f\"\"\"\n            <div class=\"score-display\">\n                <div style=\"color: {score_color};\">{scoring['total_score']}/100</div>\n                <div style=\"font-size: 1.2rem; color: #888; margin-top: 0.5rem;\">{scoring['risk_band']}</div>\n                <div style=\"font-size: 0.9rem; color: #666; margin-top: 0.5rem;\">Probability of Success: {scoring['probability_of_success']*100:.1f}%</div>\n            </div>\n            \"\"\", unsafe_allow_html=True)\n            \n            st.markdown(f\"**Recommendation:** {scoring['recommendation']}\")\n        \n        with col_sust:\n            if sustainability_scoring:\n                st.markdown(\"#### ðŸŒ± Sustainability Score\")\n                sust_score = sustainability_scoring['sustainability_score']\n                sust_color = \"#00FF88\" if sust_score >= 80 else \"#7FD8BE\" if sust_score >= 65 else \"#FFB800\" if sust_score >= 50 else \"#FF4444\"\n                \n                st.markdown(f\"\"\"\n                <div class=\"score-display\">\n                    <div style=\"color: {sust_color};\">{sust_score}/100</div>\n                    <div style=\"font-size: 1.2rem; color: #888; margin-top: 0.5rem;\">{sustainability_scoring['rating']}</div>\n                    <div style=\"font-size: 0.9rem; color: #666; margin-top: 0.5rem;\">{sustainability_scoring['description']}</div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n            else:\n                st.info(\"Sustainability analysis not available for this project\")\n    \n        st.markdown(\"---\")\n        \n        tab1, tab2 = st.tabs([\"ðŸ“ˆ Investment Categories\", \"ðŸŒ± Sustainability Categories\"])\n        \n        with tab1:\n            st.markdown(\"### Investment Category Breakdown\")\n            \n            category_names = {\n                \"geology_prospectivity\": \"â›°ï¸ Geology / Prospectivity\",\n                \"resource_potential\": \"ðŸ’Ž Resource Potential\",\n                \"economics\": \"ðŸ’° Economics\",\n                \"legal_title\": \"âš–ï¸ Legal & Title\",\n                \"permitting_esg\": \"ðŸŒ¿ Permitting & ESG\",\n                \"data_quality\": \"ðŸ“Š Data Quality\"\n            }\n        \n            for cat_key, cat_contrib in scoring['category_contributions'].items():\n                cat_name = category_names.get(cat_key, cat_key)\n                cat_data = analysis.get('categories', {}).get(cat_key, {})\n            \n                with st.expander(f\"{cat_name} - Score: {cat_contrib['raw_score']}/10 (Contribution: {cat_contrib['contribution']})\", expanded=False):\n                    st.markdown(f\"**Weight:** {cat_contrib['weight']}%\")\n                \n                    if cat_data.get('rationale'):\n                        st.markdown(f\"**Rationale:** {cat_data['rationale']}\")\n                \n                    if cat_data.get('facts_found'):\n                        st.markdown(\"**âœ“ Evidence Found:**\")\n                        for fact in cat_data['facts_found']:\n                            st.markdown(f\"- {fact}\")\n                \n                    if cat_data.get('missing_info'):\n                        st.markdown(\"**âš ï¸ Missing Information:**\")\n                        for missing in cat_data['missing_info']:\n                            st.markdown(f\"- {missing}\")\n        \n        with tab2:\n            if sustainability_scoring:\n                st.markdown(\"### Sustainability Category Breakdown\")\n                \n                sustainability_analysis = result.get('sustainability_analysis', {})\n                sust_categories = sustainability_analysis.get('sustainability_categories', {})\n                sust_contributions = sustainability_scoring.get('category_contributions', {})\n                \n                sustainability_names = {\n                    \"environmental\": \"ðŸŒ Environmental Performance\",\n                    \"social\": \"ðŸ‘¥ Social Performance\",\n                    \"governance\": \"âš–ï¸ Governance\",\n                    \"climate\": \"â˜€ï¸ Climate & Energy\"\n                }\n                \n                for cat_key, cat_contrib in sust_contributions.items():\n                    cat_name = sustainability_names.get(cat_key, cat_key)\n                    cat_data = sust_categories.get(cat_key, {})\n                \n                    with st.expander(f\"{cat_name} - Score: {cat_contrib['raw_score']}/10 (Contribution: {cat_contrib['contribution']})\", expanded=False):\n                        st.markdown(f\"**Weight:** {cat_contrib['weight']}%\")\n                    \n                        if cat_data.get('rationale'):\n                            st.markdown(f\"**Rationale:** {cat_data['rationale']}\")\n                    \n                        if cat_data.get('facts_found'):\n                            st.markdown(\"**âœ“ Evidence Found:**\")\n                            for fact in cat_data['facts_found']:\n                                st.markdown(f\"- {fact}\")\n                    \n                        if cat_data.get('missing_info'):\n                            st.markdown(\"**âš ï¸ Missing Information:**\")\n                            for missing in cat_data['missing_info']:\n                                st.markdown(f\"- {missing}\")\n                \n                if sustainability_analysis.get('overall_sustainability_notes'):\n                    st.markdown(\"---\")\n                    st.markdown(\"### ðŸ“ Overall Sustainability Assessment\")\n                    st.markdown(sustainability_analysis['overall_sustainability_notes'])\n            else:\n                st.info(\"Sustainability analysis not available for this project. Upload documents with ESG/sustainability information for comprehensive analysis.\")\n    \n        st.markdown(\"---\")\n        st.markdown(\"### ðŸ’¡ Recommendations\")\n        for rec in result['recommendations']:\n            st.markdown(f\"- {rec}\")\n    \n        if analysis.get('overall_observations'):\n            st.markdown(\"---\")\n            st.markdown(\"### ðŸ“ Overall Observations\")\n            st.markdown(analysis['overall_observations'])\n        \n        # Benchmarking against comparables\n        st.markdown(\"---\")\n        st.markdown(\"### ðŸŒ Global Benchmarking\")\n        \n        commodity = analysis.get('project_commodity', '')\n        if commodity:\n            db = SessionLocal()\n            try:\n                comparables = ComparablesManager.get_similar_comparables(db, commodity, limit=10)\n                \n                if comparables:\n                    # Convert scoring result to analysis dict format for comparison\n                    current_analysis_data = {\n                        'total_score': scoring['total_score'],\n                        'geology_score': scoring['category_contributions'].get('geology_prospectivity', {}).get('raw_score', 0),\n                        'resource_score': scoring['category_contributions'].get('resource_potential', {}).get('raw_score', 0),\n                        'economics_score': scoring['category_contributions'].get('economics', {}).get('raw_score', 0),\n                        'legal_score': scoring['category_contributions'].get('legal_title', {}).get('raw_score', 0),\n                        'permitting_score': scoring['category_contributions'].get('permitting_esg', {}).get('raw_score', 0),\n                        'data_quality_score': scoring['category_contributions'].get('data_quality', {}).get('raw_score', 0),\n                    }\n                    \n                    comparison = ComparablesManager.compare_project_to_benchmarks(current_analysis_data, comparables)\n                    \n                    if comparison.get('comparison_available'):\n                        st.markdown(f\"**Compared against {comparison['comparables_count']} similar {commodity} projects**\")\n                        \n                        col1, col2, col3 = st.columns(3)\n                        \n                        with col1:\n                            st.metric(\n                                \"Overall Score Percentile\",\n                                f\"{comparison['percentiles']['overall']}%\" if comparison['percentiles']['overall'] else \"N/A\",\n                                help=\"Higher percentile means better performance relative to peers\"\n                            )\n                        \n                        with col2:\n                            benchmark_avg = comparison['benchmarks']['overall_avg']\n                            current_score = scoring['total_score'] / 10  # Convert to 0-10 scale\n                            delta = current_score - benchmark_avg\n                            st.metric(\n                                \"vs Industry Average\",\n                                f\"{current_score:.1f}/10\",\n                                f\"{delta:+.1f}\",\n                                help=f\"Industry average: {benchmark_avg:.1f}/10\"\n                            )\n                        \n                        with col3:\n                            if comparison['percentiles']['overall']:\n                                if comparison['percentiles']['overall'] >= 75:\n                                    performance = \"Top Quartile ðŸŒŸ\"\n                                elif comparison['percentiles']['overall'] >= 50:\n                                    performance = \"Above Average âœ“\"\n                                elif comparison['percentiles']['overall'] >= 25:\n                                    performance = \"Below Average\"\n                                else:\n                                    performance = \"Bottom Quartile\"\n                                st.metric(\"Performance\", performance)\n                        \n                        with st.expander(\"ðŸ“Š Detailed Benchmarking Analysis\", expanded=False):\n                            st.markdown(\"**Category-by-Category Comparison**\")\n                            \n                            categories = [\n                                ('geology', 'â›°ï¸ Geology'),\n                                ('resource', 'ðŸ’Ž Resource'),\n                                ('economics', 'ðŸ’° Economics'),\n                                ('legal', 'âš–ï¸ Legal'),\n                                ('permitting', 'ðŸŒ¿ Permitting'),\n                                ('data_quality', 'ðŸ“Š Data Quality')\n                            ]\n                            \n                            for cat_key, cat_name in categories:\n                                percentile = comparison['percentiles'].get(cat_key)\n                                current = comparison['current_scores'].get(f'{cat_key}_score')\n                                benchmark = comparison['benchmarks'].get(f'{cat_key}_avg')\n                                \n                                if percentile and current and benchmark:\n                                    col_a, col_b, col_c = st.columns([2, 1, 1])\n                                    with col_a:\n                                        st.markdown(f\"**{cat_name}**\")\n                                    with col_b:\n                                        st.markdown(f\"{current:.1f}/10 (Avg: {benchmark:.1f})\")\n                                    with col_c:\n                                        st.markdown(f\"**{percentile}th percentile**\")\n                else:\n                    st.info(f\"No comparable {commodity} projects found in database for benchmarking.\")\n            finally:\n                db.close()\n        else:\n            st.info(\"Commodity information not available for benchmarking. Add commodity details to enable comparison.\")\n\n    elif st.session_state.ai_tier_mode == 'light_ai' and st.session_state.view_mode == 'template_manager':\n        st.markdown(\"### âš™ï¸ Scoring Template Manager\")\n        st.markdown(\"Create and manage custom scoring templates with different category weights.\")\n        st.markdown(\"---\")\n    \n        # Display existing templates\n        user_templates = TemplateManager.get_user_templates(current_user['id'])\n    \n        tab1, tab2 = st.tabs([\"ðŸ“‹ My Templates\", \"âž• Create New Template\"])\n    \n        with tab1:\n            if not user_templates:\n                st.info(\"No custom templates yet. Create your first template in the 'Create New Template' tab!\")\n            else:\n                for template in user_templates:\n                    with st.expander(f\"{'â­ ' if template['is_default'] else ''}{template['name']}\", expanded=False):\n                        st.markdown(f\"**Description:** {template.get('description', 'No description')}\")\n                        st.markdown(f\"**Created:** {template['created_at'].strftime('%Y-%m-%d %H:%M')}\")\n                    \n                        st.markdown(\"**Category Weights:**\")\n                        weights = template['weights']\n                        col1, col2 = st.columns(2)\n                        with col1:\n                            st.markdown(f\"- â›°ï¸ Geology: **{weights['geology_prospectivity']}%**\")\n                            st.markdown(f\"- ðŸ’Ž Resource: **{weights['resource_potential']}%**\")\n                            st.markdown(f\"- ðŸ’° Economics: **{weights['economics']}%**\")\n                        with col2:\n                            st.markdown(f\"- âš–ï¸ Legal: **{weights['legal_title']}%**\")\n                            st.markdown(f\"- ðŸŒ¿ Permitting: **{weights['permitting_esg']}%**\")\n                            st.markdown(f\"- ðŸ“Š Data Quality: **{weights['data_quality']}%**\")\n                    \n                        st.markdown(\"---\")\n                    \n                        col_a, col_b, col_c = st.columns(3)\n                        with col_a:\n                            if not template['is_default']:\n                                if st.button(f\"â­ Set as Default\", key=f\"default_{template['id']}\"):\n                                    TemplateManager.update_template(template['id'], is_default=True)\n                                    st.success(\"Template set as default!\")\n                                    st.rerun()\n                        with col_b:\n                            if st.button(f\"âœï¸ Edit\", key=f\"edit_{template['id']}\"):\n                                st.session_state.editing_template = template\n                                st.rerun()\n                        with col_c:\n                            if st.button(f\"ðŸ—‘ï¸ Delete\", key=f\"delete_{template['id']}\"):\n                                result = TemplateManager.delete_template(template['id'])\n                                if result['success']:\n                                    st.success(result['message'])\n                                    st.rerun()\n                                else:\n                                    st.error(result['message'])\n    \n        with tab2:\n            # Check if we're editing an existing template\n            editing = st.session_state.get('editing_template')\n        \n            if editing:\n                st.info(f\"Editing template: {editing['name']}\")\n                template_name = st.text_input(\"Template Name *\", value=editing['name'])\n                template_description = st.text_area(\"Description\", value=editing.get('description', ''))\n            \n                weights = editing['weights']\n            else:\n                template_name = st.text_input(\"Template Name *\")\n                template_description = st.text_area(\"Description\")\n                weights = TemplateManager.DEFAULT_WEIGHTS.copy()\n        \n            st.markdown(\"**Adjust Category Weights (must sum to 100%)**\")\n        \n            col1, col2 = st.columns(2)\n            with col1:\n                geology_weight = st.slider(\"â›°ï¸ Geology & Prospectivity\", 0, 100, int(weights.get('geology_prospectivity', 35)), key=\"geo_weight\")\n                resource_weight = st.slider(\"ðŸ’Ž Resource Potential\", 0, 100, int(weights.get('resource_potential', 20)), key=\"res_weight\")\n                economics_weight = st.slider(\"ðŸ’° Economics\", 0, 100, int(weights.get('economics', 15)), key=\"econ_weight\")\n            with col2:\n                legal_weight = st.slider(\"âš–ï¸ Legal & Title\", 0, 100, int(weights.get('legal_title', 10)), key=\"legal_weight\")\n                permitting_weight = st.slider(\"ðŸŒ¿ Permitting & ESG\", 0, 100, int(weights.get('permitting_esg', 10)), key=\"perm_weight\")\n                data_quality_weight = st.slider(\"ðŸ“Š Data Quality\", 0, 100, int(weights.get('data_quality', 10)), key=\"data_weight\")\n        \n            total_weight = geology_weight + resource_weight + economics_weight + legal_weight + permitting_weight + data_quality_weight\n        \n            if total_weight != 100:\n                st.error(f\"âš ï¸ Weights must sum to 100%. Current sum: {total_weight}%\")\n            else:\n                st.success(f\"âœ“ Weights sum to 100%\")\n        \n            is_default = st.checkbox(\"Set as default template\", value=editing.get('is_default', False) if editing else False)\n        \n            col_save, col_cancel = st.columns(2)\n            with col_save:\n                if st.button(\"ðŸ’¾ Save Template\", use_container_width=True, disabled=(total_weight != 100 or not template_name)):\n                    try:\n                        new_weights = {\n                            'geology_prospectivity': geology_weight,\n                            'resource_potential': resource_weight,\n                            'economics': economics_weight,\n                            'legal_title': legal_weight,\n                            'permitting_esg': permitting_weight,\n                            'data_quality': data_quality_weight\n                        }\n                    \n                        if editing:\n                            TemplateManager.update_template(\n                                editing['id'],\n                                name=template_name,\n                                description=template_description,\n                                weights=new_weights,\n                                is_default=is_default\n                            )\n                            st.success(\"Template updated successfully!\")\n                        else:\n                            TemplateManager.create_template(\n                                user_id=current_user['id'],\n                                name=template_name,\n                                description=template_description,\n                                weights=new_weights,\n                                is_default=is_default\n                            )\n                            st.success(\"Template created successfully!\")\n                    \n                        if 'editing_template' in st.session_state:\n                            del st.session_state.editing_template\n                        st.rerun()\n                    except ValueError as e:\n                        st.error(f\"Error: {e}\")\n        \n            with col_cancel:\n                if editing and st.button(\"âŒ Cancel\", use_container_width=True):\n                    del st.session_state.editing_template\n                    st.rerun()\n\n    elif st.session_state.ai_tier_mode == 'light_ai' and st.session_state.view_mode == 'comparables':\n        from page_modules.comparables_page import render_browse_comparables, render_benchmark_stats, render_about_section\n        \n        st.markdown(\"### ðŸŒ Global Comparables Database\")\n        st.markdown(\"Benchmark your mining projects against real-world analogues\")\n        st.markdown(\"---\")\n        \n        db = SessionLocal()\n        try:\n            tabs = st.tabs([\"ðŸ” Browse Projects\", \"ðŸ“Š Benchmark Statistics\", \"â„¹ï¸ About\"])\n            \n            with tabs[0]:\n                render_browse_comparables(db)\n            \n            with tabs[1]:\n                render_benchmark_stats(db)\n            \n            with tabs[2]:\n                render_about_section()\n        finally:\n            db.close()\n\n    if st.session_state.ai_tier_mode == 'light_ai':\n        st.markdown(\"---\")\n        st.markdown(\"### ðŸ’¬ Session History\")\n\n        if not st.session_state.history:\n            st.markdown('<div class=\"chat-message agent-message\">', unsafe_allow_html=True)\n            st.markdown(\"ðŸ‘‹ **AI Agent**: Welcome! Upload your mining project documents to begin the due diligence analysis.\")\n            st.markdown('</div>', unsafe_allow_html=True)\n        else:\n            for item in st.session_state.history:\n                if item['type'] == 'user':\n                    st.markdown('<div class=\"chat-message user-message\">', unsafe_allow_html=True)\n                    st.markdown(f\"**You**: {item['content']}\")\n                    if item.get('files'):\n                        st.markdown(\"Files: \" + \", \".join([f\"`{f}`\" for f in item['files']]))\n                    st.markdown(f\"<small>{item['timestamp'].strftime('%H:%M:%S')}</small>\", unsafe_allow_html=True)\n                    st.markdown('</div>', unsafe_allow_html=True)\n                else:\n                    st.markdown('<div class=\"chat-message agent-message\">', unsafe_allow_html=True)\n                    st.markdown(f\"**AI Agent**: {item['content']}\")\n                    st.markdown(f\"<small>{item['timestamp'].strftime('%H:%M:%S')}</small>\", unsafe_allow_html=True)\n                    st.markdown('</div>', unsafe_allow_html=True)\n\n    st.markdown(\"---\")\n    st.markdown(\"<small>Â©2025 Copyright Oreplot. All rights reserved.</small>\", unsafe_allow_html=True)\n","path":null,"size_bytes":51752,"size_tokens":null},"project_manager.py":{"content":"from database import get_db_session\nfrom models import Project, Analysis, Document\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\nclass ProjectManager:\n    \n    @staticmethod\n    def create_project(user_id: int, name: str, description: str = None, location: str = None, commodity: str = None):\n        \"\"\"Create a new mining project. Returns a dict representation.\"\"\"\n        with get_db_session() as session:\n            project = Project(\n                user_id=user_id,\n                name=name,\n                description=description,\n                location=location,\n                commodity=commodity,\n                created_at=datetime.utcnow(),\n                updated_at=datetime.utcnow()\n            )\n            session.add(project)\n            session.flush()\n            session.refresh(project)\n            return {\n                'id': project.id,\n                'user_id': project.user_id,\n                'name': project.name,\n                'description': project.description,\n                'location': project.location,\n                'commodity': project.commodity,\n                'created_at': project.created_at,\n                'updated_at': project.updated_at\n            }\n    \n    @staticmethod\n    def _format_findings(category_data: Dict[str, Any]) -> str:\n        \"\"\"Format category findings from AI analysis into readable text\"\"\"\n        if not category_data:\n            return ''\n        \n        findings_parts = []\n        \n        # Add rationale\n        if category_data.get('rationale'):\n            findings_parts.append(f\"**Rationale:** {category_data['rationale']}\")\n        \n        # Add facts found\n        facts = category_data.get('facts_found', [])\n        if facts:\n            findings_parts.append(\"\\n**Key Facts Found:**\")\n            for fact in facts:\n                findings_parts.append(f\"â€¢ {fact}\")\n        \n        # Add missing information\n        missing = category_data.get('missing_info', [])\n        if missing:\n            findings_parts.append(\"\\n**Missing Information:**\")\n            for item in missing:\n                findings_parts.append(f\"â€¢ {item}\")\n        \n        return '\\n'.join(findings_parts) if findings_parts else category_data.get('findings', '')\n    \n    @staticmethod\n    def save_analysis(project_id: int, analysis_data: Dict[str, Any], scoring_data: Dict[str, Any], \n                     recommendations: List[str], scoring_template_id: int = None, narrative_data: Dict[str, Any] = None,\n                     sustainability_data: Dict[str, Any] = None, sustainability_scoring: Dict[str, Any] = None,\n                     analysis_type: str = 'light_ai'):\n        \"\"\"Save an analysis result to the database. Returns a dict representation.\n        \n        Args:\n            analysis_type: 'light_ai' or 'advanced_ai' to identify the analysis source\n        \"\"\"\n        with get_db_session() as session:\n            categories = analysis_data.get('categories', {})\n            contributions = scoring_data.get('category_contributions', {})\n            \n            narrative_data = narrative_data or {}\n            sustainability_data = sustainability_data or {}\n            sustainability_scoring = sustainability_scoring or {}\n            \n            sust_categories = sustainability_data.get('sustainability_categories', {})\n            sust_contributions = sustainability_scoring.get('category_contributions', {})\n            \n            analysis = Analysis(\n                project_id=project_id,\n                scoring_template_id=scoring_template_id,\n                analysis_type=analysis_type,\n                total_score=scoring_data['total_score'],\n                risk_category=scoring_data.get('risk_band', scoring_data.get('risk_category', 'UNKNOWN')),\n                probability_of_success=scoring_data['probability_of_success'],\n                \n                geology_score=categories.get('geology_prospectivity', {}).get('score', 0),\n                geology_weight=contributions.get('geology_prospectivity', {}).get('weight', 0),\n                geology_contribution=contributions.get('geology_prospectivity', {}).get('contribution', 0),\n                geology_findings=ProjectManager._format_findings(categories.get('geology_prospectivity', {})),\n                \n                resource_score=categories.get('resource_potential', {}).get('score', 0),\n                resource_weight=contributions.get('resource_potential', {}).get('weight', 0),\n                resource_contribution=contributions.get('resource_potential', {}).get('contribution', 0),\n                resource_findings=ProjectManager._format_findings(categories.get('resource_potential', {})),\n                \n                economics_score=categories.get('economics', {}).get('score', 0),\n                economics_weight=contributions.get('economics', {}).get('weight', 0),\n                economics_contribution=contributions.get('economics', {}).get('contribution', 0),\n                economics_findings=ProjectManager._format_findings(categories.get('economics', {})),\n                \n                legal_score=categories.get('legal_title', {}).get('score', 0),\n                legal_weight=contributions.get('legal_title', {}).get('weight', 0),\n                legal_contribution=contributions.get('legal_title', {}).get('contribution', 0),\n                legal_findings=ProjectManager._format_findings(categories.get('legal_title', {})),\n                \n                permitting_score=categories.get('permitting_esg', {}).get('score', 0),\n                permitting_weight=contributions.get('permitting_esg', {}).get('weight', 0),\n                permitting_contribution=contributions.get('permitting_esg', {}).get('contribution', 0),\n                permitting_findings=ProjectManager._format_findings(categories.get('permitting_esg', {})),\n                \n                data_quality_score=categories.get('data_quality', {}).get('score', 0),\n                data_quality_weight=contributions.get('data_quality', {}).get('weight', 0),\n                data_quality_contribution=contributions.get('data_quality', {}).get('contribution', 0),\n                data_quality_findings=ProjectManager._format_findings(categories.get('data_quality', {})),\n                \n                recommendations=recommendations,\n                ai_analysis_raw=analysis_data,\n                \n                executive_summary=narrative_data.get('executive_summary'),\n                strategic_recommendations=narrative_data.get('strategic_recommendations'),\n                project_timeline=narrative_data.get('project_timeline'),\n                jurisdictional_context=narrative_data.get('jurisdictional_context'),\n                strategic_signals=narrative_data.get('strategic_signals'),\n                \n                sustainability_score=sustainability_scoring.get('sustainability_score'),\n                environmental_score=sust_categories.get('environmental', {}).get('score'),\n                environmental_weight=sust_contributions.get('environmental', {}).get('weight'),\n                environmental_contribution=sust_contributions.get('environmental', {}).get('contribution'),\n                environmental_findings=ProjectManager._format_findings(sust_categories.get('environmental', {})),\n                \n                social_score=sust_categories.get('social', {}).get('score'),\n                social_weight=sust_contributions.get('social', {}).get('weight'),\n                social_contribution=sust_contributions.get('social', {}).get('contribution'),\n                social_findings=ProjectManager._format_findings(sust_categories.get('social', {})),\n                \n                governance_score=sust_categories.get('governance', {}).get('score'),\n                governance_weight=sust_contributions.get('governance', {}).get('weight'),\n                governance_contribution=sust_contributions.get('governance', {}).get('contribution'),\n                governance_findings=ProjectManager._format_findings(sust_categories.get('governance', {})),\n                \n                climate_score=sust_categories.get('climate', {}).get('score'),\n                climate_weight=sust_contributions.get('climate', {}).get('weight'),\n                climate_contribution=sust_contributions.get('climate', {}).get('contribution'),\n                climate_findings=ProjectManager._format_findings(sust_categories.get('climate', {})),\n                \n                created_at=datetime.utcnow()\n            )\n            session.add(analysis)\n            session.flush()\n            session.refresh(analysis)\n            return {\n                'id': analysis.id,\n                'project_id': analysis.project_id,\n                'total_score': analysis.total_score,\n                'risk_category': analysis.risk_category,\n                'analysis_type': analysis.analysis_type,\n                'created_at': analysis.created_at\n            }\n    \n    @staticmethod\n    def save_documents(project_id: int, documents_data: List[Dict[str, Any]]):\n        \"\"\"Save uploaded documents metadata to the database.\"\"\"\n        with get_db_session() as session:\n            for doc_data in documents_data:\n                document = Document(\n                    project_id=project_id,\n                    file_name=doc_data.get('file_name', ''),\n                    file_type=doc_data.get('file_type', ''),\n                    file_size=doc_data.get('size', 0),\n                    extracted_text=doc_data.get('text', ''),\n                    extraction_success=doc_data.get('success', False),\n                    extraction_error=doc_data.get('error', ''),\n                    uploaded_at=datetime.utcnow()\n                )\n                session.add(document)\n            session.flush()\n    \n    @staticmethod\n    def get_user_projects(user_id: int, limit: int = 50):\n        \"\"\"Get all projects for a user.\"\"\"\n        with get_db_session() as session:\n            projects = session.query(Project).filter(\n                Project.user_id == user_id\n            ).order_by(Project.updated_at.desc()).limit(limit).all()\n            \n            result = []\n            for project in projects:\n                result.append({\n                    'id': project.id,\n                    'name': project.name,\n                    'description': project.description,\n                    'location': project.location,\n                    'commodity': project.commodity,\n                    'created_at': project.created_at,\n                    'updated_at': project.updated_at,\n                    'analysis_count': len(project.analyses)\n                })\n            return result\n    \n    @staticmethod\n    def get_project_analyses(project_id: int):\n        \"\"\"Get all analyses for a project with full details for reports.\"\"\"\n        with get_db_session() as session:\n            analyses = session.query(Analysis).filter(\n                Analysis.project_id == project_id\n            ).order_by(Analysis.created_at.desc()).all()\n            \n            result = []\n            for analysis in analyses:\n                result.append({\n                    'id': analysis.id,\n                    'total_score': analysis.total_score,\n                    'risk_category': analysis.risk_category,\n                    'probability_of_success': analysis.probability_of_success,\n                    'analysis_type': analysis.analysis_type or 'light_ai',\n                    'created_at': analysis.created_at,\n                    'recommendations': analysis.recommendations,\n                    # Include all category details for reports\n                    'geology_score': analysis.geology_score,\n                    'geology_weight': analysis.geology_weight,\n                    'geology_contribution': analysis.geology_contribution,\n                    'geology_findings': analysis.geology_findings,\n                    'resource_score': analysis.resource_score,\n                    'resource_weight': analysis.resource_weight,\n                    'resource_contribution': analysis.resource_contribution,\n                    'resource_findings': analysis.resource_findings,\n                    'economics_score': analysis.economics_score,\n                    'economics_weight': analysis.economics_weight,\n                    'economics_contribution': analysis.economics_contribution,\n                    'economics_findings': analysis.economics_findings,\n                    'legal_score': analysis.legal_score,\n                    'legal_weight': analysis.legal_weight,\n                    'legal_contribution': analysis.legal_contribution,\n                    'legal_findings': analysis.legal_findings,\n                    'permitting_score': analysis.permitting_score,\n                    'permitting_weight': analysis.permitting_weight,\n                    'permitting_contribution': analysis.permitting_contribution,\n                    'permitting_findings': analysis.permitting_findings,\n                    'data_quality_score': analysis.data_quality_score,\n                    'data_quality_weight': analysis.data_quality_weight,\n                    'data_quality_contribution': analysis.data_quality_contribution,\n                    'data_quality_findings': analysis.data_quality_findings,\n                    # Include raw AI analysis for structured category data\n                    'ai_analysis_raw': analysis.ai_analysis_raw,\n                    # Sustainability scores\n                    'sustainability_score': analysis.sustainability_score,\n                    'environmental_score': analysis.environmental_score,\n                    'environmental_weight': analysis.environmental_weight,\n                    'environmental_contribution': analysis.environmental_contribution,\n                    'social_score': analysis.social_score,\n                    'social_weight': analysis.social_weight,\n                    'social_contribution': analysis.social_contribution,\n                    'governance_score': analysis.governance_score,\n                    'governance_weight': analysis.governance_weight,\n                    'governance_contribution': analysis.governance_contribution,\n                    'climate_score': analysis.climate_score,\n                    'climate_weight': analysis.climate_weight,\n                    'climate_contribution': analysis.climate_contribution\n                })\n            return result\n    \n    @staticmethod\n    def get_analysis_details(analysis_id: int):\n        \"\"\"Get complete details of a specific analysis.\"\"\"\n        with get_db_session() as session:\n            analysis = session.query(Analysis).filter(Analysis.id == analysis_id).first()\n            \n            if not analysis:\n                return None\n            \n            # Build base categories with scores and weights\n            categories = {\n                'geology_prospectivity': {\n                    'score': analysis.geology_score,\n                    'weight': analysis.geology_weight,\n                    'contribution': analysis.geology_contribution,\n                    'findings': analysis.geology_findings\n                },\n                'resource_potential': {\n                    'score': analysis.resource_score,\n                    'weight': analysis.resource_weight,\n                    'contribution': analysis.resource_contribution,\n                    'findings': analysis.resource_findings\n                },\n                'economics': {\n                    'score': analysis.economics_score,\n                    'weight': analysis.economics_weight,\n                    'contribution': analysis.economics_contribution,\n                    'findings': analysis.economics_findings\n                },\n                'legal_title': {\n                    'score': analysis.legal_score,\n                    'weight': analysis.legal_weight,\n                    'contribution': analysis.legal_contribution,\n                    'findings': analysis.legal_findings\n                },\n                'permitting_esg': {\n                    'score': analysis.permitting_score,\n                    'weight': analysis.permitting_weight,\n                    'contribution': analysis.permitting_contribution,\n                    'findings': analysis.permitting_findings\n                },\n                'data_quality': {\n                    'score': analysis.data_quality_score,\n                    'weight': analysis.data_quality_weight,\n                    'contribution': analysis.data_quality_contribution,\n                    'findings': analysis.data_quality_findings\n                }\n            }\n            \n            # Merge detailed category data from ai_analysis_raw if available\n            if analysis.ai_analysis_raw and isinstance(analysis.ai_analysis_raw, dict):\n                raw_categories = analysis.ai_analysis_raw.get('categories', {})\n                for cat_key in categories.keys():\n                    if cat_key in raw_categories:\n                        raw_cat_data = raw_categories[cat_key]\n                        # Add rationale, facts_found, and missing_info from raw data\n                        if raw_cat_data.get('rationale'):\n                            categories[cat_key]['rationale'] = raw_cat_data['rationale']\n                        if raw_cat_data.get('facts_found'):\n                            categories[cat_key]['facts_found'] = raw_cat_data['facts_found']\n                        if raw_cat_data.get('missing_info'):\n                            categories[cat_key]['missing_info'] = raw_cat_data['missing_info']\n            \n            # Build sustainability categories if data exists\n            sustainability_scoring = None\n            sustainability_analysis = None\n            \n            if analysis.sustainability_score is not None:\n                sustainability_categories = {\n                    'environmental': {\n                        'score': analysis.environmental_score,\n                        'weight': analysis.environmental_weight,\n                        'contribution': analysis.environmental_contribution,\n                        'findings': analysis.environmental_findings\n                    },\n                    'social': {\n                        'score': analysis.social_score,\n                        'weight': analysis.social_weight,\n                        'contribution': analysis.social_contribution,\n                        'findings': analysis.social_findings\n                    },\n                    'governance': {\n                        'score': analysis.governance_score,\n                        'weight': analysis.governance_weight,\n                        'contribution': analysis.governance_contribution,\n                        'findings': analysis.governance_findings\n                    },\n                    'climate': {\n                        'score': analysis.climate_score,\n                        'weight': analysis.climate_weight,\n                        'contribution': analysis.climate_contribution,\n                        'findings': analysis.climate_findings\n                    }\n                }\n                \n                from scoring_engine import ScoringEngine\n                sustainability_scoring = ScoringEngine.calculate_sustainability_score(sustainability_categories)\n                sustainability_analysis = {'sustainability_categories': sustainability_categories}\n            \n            return {\n                'id': analysis.id,\n                'project_id': analysis.project_id,\n                'total_score': analysis.total_score,\n                'risk_category': analysis.risk_category,\n                'probability_of_success': analysis.probability_of_success,\n                'analysis_type': analysis.analysis_type or 'light_ai',\n                'categories': categories,\n                'recommendations': analysis.recommendations,\n                'ai_analysis_raw': analysis.ai_analysis_raw,\n                'sustainability_scoring': sustainability_scoring,\n                'sustainability_analysis': sustainability_analysis,\n                'created_at': analysis.created_at\n            }\n    \n    @staticmethod\n    def get_project_documents(project_id: int):\n        \"\"\"Get all documents for a project.\"\"\"\n        with get_db_session() as session:\n            documents = session.query(Document).filter(\n                Document.project_id == project_id\n            ).order_by(Document.uploaded_at.desc()).all()\n            \n            result = []\n            for doc in documents:\n                result.append({\n                    'id': doc.id,\n                    'file_name': doc.file_name,\n                    'file_type': doc.file_type,\n                    'file_size': doc.file_size,\n                    'extraction_success': doc.extraction_success,\n                    'uploaded_at': doc.uploaded_at\n                })\n            return result\n","path":null,"size_bytes":20999,"size_tokens":null},"models.py":{"content":"from datetime import datetime\nfrom sqlalchemy import Column, Integer, String, Float, DateTime, Text, ForeignKey, JSON, Boolean, Index, UniqueConstraint\nfrom sqlalchemy.orm import relationship\nfrom database import Base\n\nclass User(Base):\n    __tablename__ = 'users'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    email = Column(String, unique=True, index=True, nullable=False)\n    username = Column(String, unique=True, index=True, nullable=False)\n    password_hash = Column(String)\n    is_admin = Column(Boolean, default=False)\n    \n    full_name = Column(String)\n    company = Column(String)\n    role = Column(String)\n    phone = Column(String)\n    avatar_url = Column(String)\n    \n    api_key = Column(String, unique=True)\n    mfa_enabled = Column(Boolean, default=False)\n    mfa_secret = Column(String)\n    \n    theme = Column(String, default='light')\n    notifications_enabled = Column(Boolean, default=True)\n    ai_behavior_settings = Column(JSON)\n    \n    plan_type = Column(String, default='free')\n    usage_count = Column(Integer, default=0)\n    usage_limit = Column(Integer, default=10)\n    billing_status = Column(String, default='active')\n    \n    ai_tier_access = Column(String, default='light_ai')  # 'light_ai', 'advanced_ai', 'both'\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    last_login = Column(DateTime)\n    \n    projects = relationship(\"Project\", back_populates=\"user\", cascade=\"all, delete-orphan\")\n    scoring_templates = relationship(\"ScoringTemplate\", back_populates=\"user\", cascade=\"all, delete-orphan\")\n    team_memberships = relationship(\"TeamMember\", back_populates=\"user\", cascade=\"all, delete-orphan\")\n    owned_teams = relationship(\"Team\", back_populates=\"owner\", cascade=\"all, delete-orphan\")\n\nclass Project(Base):\n    __tablename__ = 'projects'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)\n    name = Column(String, nullable=False)\n    description = Column(Text)\n    location = Column(String)\n    commodity = Column(String)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    user = relationship(\"User\", back_populates=\"projects\")\n    analyses = relationship(\"Analysis\", back_populates=\"project\", cascade=\"all, delete-orphan\")\n    documents = relationship(\"Document\", back_populates=\"project\", cascade=\"all, delete-orphan\")\n    financial_models = relationship(\"FinancialModel\", back_populates=\"project\", cascade=\"all, delete-orphan\")\n\nclass Analysis(Base):\n    __tablename__ = 'analyses'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    project_id = Column(Integer, ForeignKey('projects.id'), nullable=False)\n    scoring_template_id = Column(Integer, ForeignKey('scoring_templates.id'), nullable=True)\n    \n    analysis_type = Column(String, default='light_ai')  # 'light_ai' or 'advanced_ai'\n    \n    total_score = Column(Float, nullable=False)\n    risk_category = Column(String, nullable=False)\n    probability_of_success = Column(Float, nullable=False)\n    \n    geology_score = Column(Float)\n    geology_weight = Column(Float)\n    geology_contribution = Column(Float)\n    geology_findings = Column(Text)\n    \n    resource_score = Column(Float)\n    resource_weight = Column(Float)\n    resource_contribution = Column(Float)\n    resource_findings = Column(Text)\n    \n    economics_score = Column(Float)\n    economics_weight = Column(Float)\n    economics_contribution = Column(Float)\n    economics_findings = Column(Text)\n    \n    legal_score = Column(Float)\n    legal_weight = Column(Float)\n    legal_contribution = Column(Float)\n    legal_findings = Column(Text)\n    \n    permitting_score = Column(Float)\n    permitting_weight = Column(Float)\n    permitting_contribution = Column(Float)\n    permitting_findings = Column(Text)\n    \n    data_quality_score = Column(Float)\n    data_quality_weight = Column(Float)\n    data_quality_contribution = Column(Float)\n    data_quality_findings = Column(Text)\n    \n    recommendations = Column(JSON)\n    ai_analysis_raw = Column(JSON)\n    \n    executive_summary = Column(Text)\n    strategic_recommendations = Column(JSON)\n    project_timeline = Column(Text)\n    jurisdictional_context = Column(Text)\n    strategic_signals = Column(JSON)\n    \n    sustainability_score = Column(Float)\n    environmental_score = Column(Float)\n    environmental_weight = Column(Float)\n    environmental_contribution = Column(Float)\n    environmental_findings = Column(Text)\n    \n    social_score = Column(Float)\n    social_weight = Column(Float)\n    social_contribution = Column(Float)\n    social_findings = Column(Text)\n    \n    governance_score = Column(Float)\n    governance_weight = Column(Float)\n    governance_contribution = Column(Float)\n    governance_findings = Column(Text)\n    \n    climate_score = Column(Float)\n    climate_weight = Column(Float)\n    climate_contribution = Column(Float)\n    climate_findings = Column(Text)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    project = relationship(\"Project\", back_populates=\"analyses\")\n    scoring_template = relationship(\"ScoringTemplate\")\n    comparable_matches = relationship(\"ComparableMatch\", back_populates=\"analysis\", cascade=\"all, delete-orphan\")\n\nclass Document(Base):\n    __tablename__ = 'documents'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    project_id = Column(Integer, ForeignKey('projects.id'), nullable=False)\n    \n    file_name = Column(String, nullable=False)\n    file_type = Column(String, nullable=False)\n    file_size = Column(Integer)\n    file_path = Column(String)\n    \n    extracted_text = Column(Text)\n    extraction_success = Column(Boolean, default=True)\n    extraction_error = Column(Text)\n    \n    uploaded_at = Column(DateTime, default=datetime.utcnow)\n    \n    project = relationship(\"Project\", back_populates=\"documents\")\n\nclass ScoringTemplate(Base):\n    __tablename__ = 'scoring_templates'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)\n    \n    name = Column(String, nullable=False)\n    description = Column(Text)\n    is_default = Column(Boolean, default=False)\n    \n    geology_weight = Column(Float, default=0.20)\n    resource_weight = Column(Float, default=0.25)\n    economics_weight = Column(Float, default=0.25)\n    legal_weight = Column(Float, default=0.10)\n    permitting_weight = Column(Float, default=0.10)\n    data_quality_weight = Column(Float, default=0.10)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    user = relationship(\"User\", back_populates=\"scoring_templates\")\n\nclass Team(Base):\n    __tablename__ = 'teams'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String, nullable=False)\n    description = Column(Text)\n    owner_id = Column(Integer, ForeignKey('users.id'), nullable=False)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    owner = relationship(\"User\", back_populates=\"owned_teams\")\n    members = relationship(\"TeamMember\", back_populates=\"team\", cascade=\"all, delete-orphan\")\n\nclass TeamMember(Base):\n    __tablename__ = 'team_members'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    team_id = Column(Integer, ForeignKey('teams.id'), nullable=False)\n    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)\n    role = Column(String, default='member')\n    \n    invited_at = Column(DateTime, default=datetime.utcnow)\n    joined_at = Column(DateTime)\n    status = Column(String, default='active')\n    \n    team = relationship(\"Team\", back_populates=\"members\")\n    user = relationship(\"User\", back_populates=\"team_memberships\")\n\nclass ComparableProject(Base):\n    \"\"\"Reference database of real mining projects for benchmarking\"\"\"\n    __tablename__ = 'comparable_projects'\n    __table_args__ = (\n        Index('idx_comparable_commodity_group', 'commodity_group'),\n        Index('idx_comparable_deposit_style', 'deposit_style'),\n        Index('idx_comparable_jurisdiction_risk', 'jurisdiction_risk_band'),\n        Index('idx_comparable_dev_stage', 'development_stage_detail'),\n    )\n    \n    id = Column(Integer, primary_key=True, index=True)\n    \n    # Basic Information\n    name = Column(String, nullable=False, index=True)\n    company = Column(String)\n    location = Column(String, index=True)\n    country = Column(String, index=True)\n    commodity = Column(String, index=True)\n    \n    # Project Status & Stage\n    project_stage = Column(String, index=True)  # exploration, development, production, closed\n    status = Column(String)  # active, suspended, completed\n    \n    # Geological Data\n    geology_type = Column(String)  # deposit type (e.g., porphyry copper, epithermal gold)\n    total_resource_mt = Column(Float)  # Total resource in million tonnes\n    grade = Column(Float)  # Average grade\n    grade_unit = Column(String)  # g/t, %, ppm\n    \n    # Economic Metrics\n    capex_millions_usd = Column(Float)  # Capital expenditure\n    opex_per_tonne_usd = Column(Float)  # Operating cost\n    npv_millions_usd = Column(Float)  # Net present value\n    irr_percent = Column(Float)  # Internal rate of return\n    payback_years = Column(Float)\n    \n    # Production Data\n    annual_production = Column(Float)\n    production_unit = Column(String)  # tonnes, ounces, etc.\n    mine_life_years = Column(Float)\n    \n    # Deal/Transaction Data\n    acquisition_value_millions_usd = Column(Float)\n    acquisition_date = Column(DateTime)\n    transaction_type = Column(String)  # acquisition, merger, JV, etc.\n    \n    # Risk Factors\n    permitting_duration_months = Column(Float)\n    esg_rating = Column(String)  # A, B, C, D rating\n    political_risk_score = Column(Float)  # 0-10\n    \n    # Benchmarking Scores (if available)\n    geology_score = Column(Float)\n    resource_score = Column(Float)\n    economics_score = Column(Float)\n    legal_score = Column(Float)\n    permitting_score = Column(Float)\n    data_quality_score = Column(Float)\n    overall_score = Column(Float)\n    \n    # Source & Metadata\n    data_source = Column(String)  # e.g., \"NI 43-101 Report\", \"SEDAR+\", \"Company Website\"\n    source_url = Column(String)\n    data_quality = Column(String)  # high, medium, low\n    last_updated = Column(DateTime, default=datetime.utcnow)\n    \n    # Additional Information\n    notes = Column(Text)\n    tags = Column(JSON)  # Searchable tags\n    \n    development_stage_detail = Column(String)\n    jurisdiction_risk_band = Column(String)\n    commodity_group = Column(String)\n    deposit_style = Column(String)\n    approved_for_display = Column(Boolean, default=True)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    ingestion_logs = relationship(\"ComparableIngestion\", back_populates=\"comparable\", cascade=\"all, delete-orphan\")\n\nclass FinancialModel(Base):\n    \"\"\"Financial model for NPV/IRR calculations and valuations\"\"\"\n    __tablename__ = 'financial_models'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    project_id = Column(Integer, ForeignKey('projects.id'), nullable=False)\n    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)\n    \n    name = Column(String, nullable=False)\n    description = Column(Text)\n    model_type = Column(String, default='dcf')\n    \n    base_commodity_price = Column(Float)\n    commodity_price_unit = Column(String)\n    production_profile = Column(JSON)\n    \n    initial_capex_millions = Column(Float)\n    sustaining_capex_millions = Column(Float)\n    opex_per_unit = Column(Float)\n    opex_unit = Column(String)\n    \n    discount_rate = Column(Float, default=10.0)\n    mine_life_years = Column(Integer)\n    ramp_up_years = Column(Integer, default=1)\n    \n    revenue_assumptions = Column(JSON)\n    cost_assumptions = Column(JSON)\n    tax_assumptions = Column(JSON)\n    \n    calculated_npv = Column(Float)\n    calculated_irr = Column(Float)\n    calculated_payback_years = Column(Float)\n    calculated_metrics = Column(JSON)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    project = relationship(\"Project\", back_populates=\"financial_models\")\n    scenarios = relationship(\"FinancialScenario\", back_populates=\"financial_model\", cascade=\"all, delete-orphan\")\n\nclass FinancialScenario(Base):\n    \"\"\"Different scenarios for sensitivity analysis\"\"\"\n    __tablename__ = 'financial_scenarios'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    financial_model_id = Column(Integer, ForeignKey('financial_models.id'), nullable=False)\n    \n    name = Column(String, nullable=False)\n    description = Column(Text)\n    scenario_type = Column(String)\n    \n    variable_name = Column(String)\n    variable_value = Column(Float)\n    variable_change_percent = Column(Float)\n    \n    assumptions_override = Column(JSON)\n    cashflow_data = Column(JSON)\n    \n    npv_result = Column(Float)\n    irr_result = Column(Float)\n    sensitivity_impact = Column(Float)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    financial_model = relationship(\"FinancialModel\", back_populates=\"scenarios\")\n\nclass CommodityPriceSnapshot(Base):\n    \"\"\"Cached commodity price data from external APIs\"\"\"\n    __tablename__ = 'commodity_prices'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    \n    commodity = Column(String, nullable=False, index=True)\n    price = Column(Float, nullable=False)\n    currency = Column(String, default='USD')\n    unit = Column(String)\n    \n    price_change_24h = Column(Float)\n    price_change_percent_24h = Column(Float)\n    \n    high_52w = Column(Float)\n    low_52w = Column(Float)\n    \n    source = Column(String)\n    source_url = Column(String)\n    \n    historical_data = Column(JSON)\n    \n    fetched_at = Column(DateTime, default=datetime.utcnow)\n    expires_at = Column(DateTime)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n\nclass ComparableMatch(Base):\n    \"\"\"Tracks matched comparable projects for analyses\"\"\"\n    __tablename__ = 'comparable_matches'\n    __table_args__ = (\n        UniqueConstraint('analysis_id', 'comparable_id', name='uq_analysis_comparable'),\n        Index('idx_comparable_match_analysis', 'analysis_id'),\n        Index('idx_comparable_match_comparable', 'comparable_id'),\n    )\n    \n    id = Column(Integer, primary_key=True, index=True)\n    analysis_id = Column(Integer, ForeignKey('analyses.id'), nullable=False)\n    comparable_id = Column(Integer, ForeignKey('comparable_projects.id'), nullable=False)\n    \n    similarity_score = Column(Float, nullable=False)\n    match_criteria = Column(JSON)\n    rank = Column(Integer)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    analysis = relationship(\"Analysis\", back_populates=\"comparable_matches\")\n    comparable = relationship(\"ComparableProject\")\n\nclass IngestionJob(Base):\n    \"\"\"Scheduled jobs for auto-updating comparables database\"\"\"\n    __tablename__ = 'ingestion_jobs'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    \n    status = Column(String, default='pending')\n    started_at = Column(DateTime, default=datetime.utcnow)\n    completed_at = Column(DateTime)\n    \n    total_records = Column(Integer)\n    successful_records = Column(Integer)\n    failed_records = Column(Integer)\n    error_log = Column(Text)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    ingestions = relationship(\"ComparableIngestion\", back_populates=\"job\", cascade=\"all, delete-orphan\")\n\nclass ComparableIngestion(Base):\n    \"\"\"Log of ingested comparable project data\"\"\"\n    __tablename__ = 'comparable_ingestions'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    job_id = Column(Integer, ForeignKey('ingestion_jobs.id'), nullable=False)\n    comparable_id = Column(Integer, ForeignKey('comparable_projects.id'), nullable=True)\n    \n    project_name = Column(String, nullable=False)\n    status = Column(String, nullable=False)\n    error_message = Column(Text)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    job = relationship(\"IngestionJob\", back_populates=\"ingestions\")\n    comparable = relationship(\"ComparableProject\", back_populates=\"ingestion_logs\")\n\n\nclass TrainingCollection(Base):\n    \"\"\"Collection of training documents for AI reference/few-shot learning\"\"\"\n    __tablename__ = 'training_collections'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String, nullable=False)\n    description = Column(Text)\n    category = Column(String, index=True)  # resource_statements, financial_tables, drill_data, etc.\n    commodity = Column(String, index=True)  # gold, copper, lithium, etc.\n    \n    total_documents = Column(Integer, default=0)\n    total_size_bytes = Column(Integer, default=0)\n    processing_status = Column(String, default='pending')  # pending, processing, completed, error\n    processing_progress = Column(Float, default=0.0)\n    processing_error = Column(Text)\n    \n    created_by = Column(Integer, ForeignKey('users.id'), nullable=False)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    documents = relationship(\"TrainingDocument\", back_populates=\"collection\", cascade=\"all, delete-orphan\")\n    examples = relationship(\"TrainingExample\", back_populates=\"collection\", cascade=\"all, delete-orphan\")\n\n\nclass TrainingDocument(Base):\n    \"\"\"Individual training document uploaded by admin\"\"\"\n    __tablename__ = 'training_documents'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    collection_id = Column(Integer, ForeignKey('training_collections.id'), nullable=False)\n    \n    file_name = Column(String, nullable=False)\n    file_type = Column(String, nullable=False)  # pdf, docx, xlsx\n    file_size = Column(Integer)\n    file_path = Column(String)  # Path in storage\n    \n    extracted_text = Column(Text)\n    extraction_status = Column(String, default='pending')  # pending, processing, completed, error\n    extraction_error = Column(Text)\n    page_count = Column(Integer)\n    char_count = Column(Integer)\n    \n    uploaded_at = Column(DateTime, default=datetime.utcnow)\n    processed_at = Column(DateTime)\n    \n    collection = relationship(\"TrainingCollection\", back_populates=\"documents\")\n    chunks = relationship(\"TrainingChunk\", back_populates=\"document\", cascade=\"all, delete-orphan\")\n\n\nclass TrainingChunk(Base):\n    \"\"\"Chunked segments from training documents for retrieval\"\"\"\n    __tablename__ = 'training_chunks'\n    __table_args__ = (\n        Index('idx_training_chunk_category', 'category'),\n        Index('idx_training_chunk_doc', 'document_id'),\n    )\n    \n    id = Column(Integer, primary_key=True, index=True)\n    document_id = Column(Integer, ForeignKey('training_documents.id'), nullable=False)\n    \n    chunk_index = Column(Integer, nullable=False)\n    chunk_text = Column(Text, nullable=False)\n    chunk_type = Column(String)  # text, table, header\n    category = Column(String, index=True)  # resource_statement, financial_table, drill_data, etc.\n    \n    start_page = Column(Integer)\n    end_page = Column(Integer)\n    token_count = Column(Integer)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    document = relationship(\"TrainingDocument\", back_populates=\"chunks\")\n\n\nclass TrainingExample(Base):\n    \"\"\"Curated gold-standard examples extracted from training documents\"\"\"\n    __tablename__ = 'training_examples'\n    __table_args__ = (\n        Index('idx_training_example_category', 'category'),\n        Index('idx_training_example_approved', 'is_approved'),\n    )\n    \n    id = Column(Integer, primary_key=True, index=True)\n    collection_id = Column(Integer, ForeignKey('training_collections.id'), nullable=False)\n    document_id = Column(Integer, ForeignKey('training_documents.id'), nullable=True)\n    \n    category = Column(String, nullable=False, index=True)  # resource_statement, financial_table, etc.\n    subcategory = Column(String)  # measured_indicated, proven_probable, npv_irr, etc.\n    \n    example_name = Column(String, nullable=False)\n    example_description = Column(Text)\n    \n    source_text = Column(Text, nullable=False)  # Original text from document\n    extracted_data = Column(JSON)  # Structured JSON extraction\n    extraction_notes = Column(Text)  # Notes on how this was extracted\n    \n    quality_score = Column(Float)  # 0-10 rating of example quality\n    is_approved = Column(Boolean, default=False)  # Admin approval for use\n    approved_by = Column(Integer, ForeignKey('users.id'), nullable=True)\n    approved_at = Column(DateTime)\n    \n    usage_count = Column(Integer, default=0)  # How many times used in prompts\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    collection = relationship(\"TrainingCollection\", back_populates=\"examples\")\n\n\nclass TrainingCategory(Base):\n    \"\"\"Categories/taxonomies for organizing training data\"\"\"\n    __tablename__ = 'training_categories'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String, nullable=False, unique=True)\n    display_name = Column(String, nullable=False)\n    description = Column(Text)\n    parent_category = Column(String)\n    \n    extraction_prompt = Column(Text)  # Custom prompt for this category\n    validation_rules = Column(JSON)  # Rules for validating extractions\n    \n    is_active = Column(Boolean, default=True)\n    priority = Column(Integer, default=0)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n\n\nclass AdvancedValuation(Base):\n    \"\"\"Advanced AI valuation results including market multiples, Kilburn method, and Monte Carlo analysis\"\"\"\n    __tablename__ = 'advanced_valuations'\n    __table_args__ = (\n        Index('idx_advanced_valuation_project', 'project_id'),\n        Index('idx_advanced_valuation_analysis', 'analysis_id'),\n    )\n    \n    id = Column(Integer, primary_key=True, index=True)\n    project_id = Column(Integer, ForeignKey('projects.id'), nullable=False)\n    analysis_id = Column(Integer, ForeignKey('analyses.id'), nullable=True)\n    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)\n    \n    valuation_type = Column(String, default='comprehensive')\n    \n    market_multiples_ev_per_oz = Column(Float)\n    market_multiples_peer_median = Column(Float)\n    market_multiples_implied_value = Column(Float)\n    market_multiples_upside_percent = Column(Float)\n    market_multiples_comparable_count = Column(Integer)\n    market_multiples_data = Column(JSON)\n    \n    kilburn_base_acquisition_cost = Column(Float)\n    kilburn_regional_prospectivity_score = Column(Float)\n    kilburn_project_maturity_score = Column(Float)\n    kilburn_local_geology_score = Column(Float)\n    kilburn_analytical_data_score = Column(Float)\n    kilburn_total_rating = Column(Float)\n    kilburn_pem_multiplier = Column(Float)\n    kilburn_appraised_value = Column(Float)\n    kilburn_data = Column(JSON)\n    \n    historical_exploration_spend = Column(Float)\n    mee_multiplier = Column(Float)\n    mee_appraised_value = Column(Float)\n    \n    monte_carlo_simulations = Column(Integer, default=10000)\n    monte_carlo_commodity_volatility = Column(Float)\n    monte_carlo_npv_p10 = Column(Float)\n    monte_carlo_npv_p50 = Column(Float)\n    monte_carlo_npv_p90 = Column(Float)\n    monte_carlo_probability_profit = Column(Float)\n    monte_carlo_var_5_percent = Column(Float)\n    monte_carlo_data = Column(JSON)\n    \n    resource_total_oz = Column(Float)\n    resource_category = Column(String)\n    commodity = Column(String)\n    project_stage = Column(String)\n    \n    valuation_summary = Column(Text)\n    valuation_recommendation = Column(String)\n    confidence_level = Column(String)\n    \n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    project = relationship(\"Project\")\n    analysis = relationship(\"Analysis\")\n\n\nclass TrainingEmbedding(Base):\n    \"\"\"Vector embeddings for enhanced RAG training system\"\"\"\n    __tablename__ = 'training_embeddings'\n    __table_args__ = (\n        Index('idx_training_embedding_category', 'category'),\n        Index('idx_training_embedding_commodity', 'commodity'),\n    )\n    \n    id = Column(Integer, primary_key=True, index=True)\n    \n    file_name = Column(String, nullable=False)\n    file_type = Column(String, nullable=False)\n    file_size = Column(Integer)\n    \n    chunk_index = Column(Integer, nullable=False)\n    chunk_text = Column(Text, nullable=False)\n    chunk_tokens = Column(Integer)\n    \n    category = Column(String, index=True)\n    commodity = Column(String, index=True)\n    \n    embedding = Column(JSON)\n    embedding_model = Column(String, default='text-embedding-3-small')\n    \n    chunk_metadata = Column(JSON)\n    \n    uploaded_by = Column(Integer, ForeignKey('users.id'), nullable=False)\n    created_at = Column(DateTime, default=datetime.utcnow)\n\n\nclass TrainingStats(Base):\n    \"\"\"Statistics and settings for the training system\"\"\"\n    __tablename__ = 'training_stats'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    \n    total_documents = Column(Integer, default=0)\n    total_chunks = Column(Integer, default=0)\n    total_tokens = Column(Integer, default=0)\n    total_size_bytes = Column(Integer, default=0)\n    \n    categories_count = Column(JSON)\n    commodities_count = Column(JSON)\n    \n    last_upload_at = Column(DateTime)\n    last_training_used_at = Column(DateTime)\n    \n    settings = Column(JSON)\n    \n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n","path":null,"size_bytes":26315,"size_tokens":null},"init_db.py":{"content":"from database import init_db\n\nif __name__ == \"__main__\":\n    print(\"Initializing database...\")\n    init_db()\n    print(\"Database initialized successfully!\")\n","path":null,"size_bytes":157,"size_tokens":null},"scoring_engine.py":{"content":"from typing import Dict, Any, List\n\n\nclass ScoringEngine:\n    \n    CATEGORY_WEIGHTS = {\n        \"geology_prospectivity\": 35,\n        \"resource_potential\": 20,\n        \"economics\": 15,\n        \"legal_title\": 10,\n        \"permitting_esg\": 10,\n        \"data_quality\": 10\n    }\n    \n    SUSTAINABILITY_CATEGORY_WEIGHTS = {\n        \"environmental\": 35,\n        \"social\": 30,\n        \"governance\": 20,\n        \"climate\": 15\n    }\n    \n    # Severity-based penalty system: distinguish CRITICAL gaps vs MINOR refinements\n    # CRITICAL items (missing resource estimates, no grades, no drill data) get harsh penalties\n    # MINOR items (no consolidated table, no explicit 3D dims when implied, formatting) get light penalties\n    CRITICAL_MISSING_KEYWORDS = [\n        'resource estimate', 'tonnage', 'grade', 'ounces', 'mineral resource',\n        'capex', 'opex', 'cash flow', 'npv', 'irr', 'economic',\n        'title', 'ownership', 'royalty', 'permit', 'license',\n        'drilling', 'intercepts', 'assay', 'qaqc'\n    ]\n    \n    SCORE_CAP_RULES_BY_SEVERITY = {\n        'critical': {7: 5, 6: 6, 5: 6, 4: 7, 3: 7, 2: 8, 1: 9},  # Harsh: 7+ critical â†’ max 5\n        'minor': {7: 9, 6: 9, 5: 8, 4: 8, 3: 8, 2: 8, 1: 9}      # Lenient: minor gaps don't reduce much\n    }\n    \n    MIN_EVIDENCE_REQUIREMENTS = {\n        10: 8,\n        9: 7,\n        8: 6,\n        7: 5,\n        6: 4\n    }\n    \n    @staticmethod\n    def classify_missing_item_severity(item: str) -> str:\n        \"\"\"Classify missing information as CRITICAL or MINOR based on content.\"\"\"\n        item_lower = item.lower()\n        for critical_keyword in ScoringEngine.CRITICAL_MISSING_KEYWORDS:\n            if critical_keyword in item_lower:\n                return 'critical'\n        return 'minor'\n    \n    @staticmethod\n    def validate_and_adjust_score(raw_score: float, missing_info_count: int, evidence_count: int, missing_items: List[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Validate and adjust scores using SEVERITY-BASED penalties:\n        1. Classify missing items as CRITICAL vs MINOR\n        2. Apply appropriate caps based on severity\n        3. Enforce minimum evidence requirements\n        \n        Args:\n            raw_score: The AI-generated raw score (0-10)\n            missing_info_count: Number of missing items (kept for backward compatibility)\n            evidence_count: Number of evidence items found\n            missing_items: List of missing item descriptions for severity classification\n        \n        Returns:\n            Dict with adjusted_score, penalty_applied, and reasoning\n        \"\"\"\n        adjusted_score = raw_score\n        penalties = []\n        \n        # Classify missing items by severity if provided\n        critical_count = 0\n        minor_count = 0\n        \n        if missing_items:\n            for item in missing_items:\n                severity = ScoringEngine.classify_missing_item_severity(item)\n                if severity == 'critical':\n                    critical_count += 1\n                else:\n                    minor_count += 1\n        else:\n            # Fallback: assume all missing items are critical if not specified\n            critical_count = missing_info_count\n        \n        # Apply severity-based penalty\n        if critical_count >= 7:\n            cap = 5\n            if adjusted_score > cap:\n                penalties.append(f\"Capped to {cap}/10 due to {critical_count} critical missing items\")\n                adjusted_score = cap\n        elif critical_count >= 6:\n            cap = 6\n            if adjusted_score > cap:\n                penalties.append(f\"Capped to {cap}/10 due to {critical_count} critical missing items\")\n                adjusted_score = cap\n        elif critical_count >= 5:\n            cap = 6\n            if adjusted_score > cap:\n                penalties.append(f\"Capped to {cap}/10 due to {critical_count} critical missing items\")\n                adjusted_score = cap\n        elif critical_count >= 4:\n            cap = 7\n            if adjusted_score > cap:\n                penalties.append(f\"Capped to {cap}/10 due to {critical_count} critical missing items\")\n                adjusted_score = cap\n        # For 3 or fewer critical items, minor gaps alone don't reduce score\n        \n        # Enforce minimum evidence requirements (slightly relaxed)\n        max_iterations = 10\n        iteration = 0\n        while iteration < max_iterations:\n            iteration += 1\n            score_int = int(adjusted_score)\n            \n            if score_int < 6:\n                break\n            \n            min_evidence = ScoringEngine.MIN_EVIDENCE_REQUIREMENTS.get(score_int, 0)\n            \n            if evidence_count >= min_evidence:\n                break\n            \n            new_score = max(5, adjusted_score - 1)\n            if iteration == 1:\n                penalties.append(f\"Insufficient evidence: need {min_evidence} items for score {score_int}/10, have only {evidence_count}\")\n            adjusted_score = new_score\n            \n            if adjusted_score <= 5:\n                break\n        \n        return {\n            'adjusted_score': round(adjusted_score, 1),\n            'original_score': raw_score,\n            'penalties_applied': penalties,\n            'penalty_applied': len(penalties) > 0,\n            'critical_missing': critical_count,\n            'minor_missing': minor_count\n        }\n    \n    @staticmethod\n    def calculate_investment_score(categories: Dict[str, Any], custom_weights: Dict[str, float] = None) -> Dict[str, Any]:\n        \"\"\"\n        Calculate investment score using category scores and weights with SEVERITY-BASED penalties.\n        \n        Args:\n            categories: Dictionary of category data with scores\n            custom_weights: Optional custom weights dict. If None, uses CATEGORY_WEIGHTS\n        \n        Returns:\n            Dictionary with total_score, risk_band, recommendation, and category_contributions\n        \"\"\"\n        weights = custom_weights if custom_weights is not None else ScoringEngine.CATEGORY_WEIGHTS\n        \n        total_score = 0.0\n        category_contributions = {}\n        \n        for category_key, weight in weights.items():\n            category_data = categories.get(category_key, {})\n            raw_score = category_data.get('score', 0)\n            \n            missing_info = category_data.get('missing_info', [])\n            evidence = category_data.get('facts_found', [])\n            missing_count = len(missing_info) if isinstance(missing_info, list) else 0\n            evidence_count = len(evidence) if isinstance(evidence, list) else 0\n            \n            # Pass missing items for severity classification\n            validation_result = ScoringEngine.validate_and_adjust_score(\n                raw_score, \n                missing_count, \n                evidence_count,\n                missing_items=missing_info if isinstance(missing_info, list) else []\n            )\n            \n            adjusted_score = validation_result['adjusted_score']\n            \n            contribution = (adjusted_score / 10.0) * weight\n            category_contributions[category_key] = {\n                'raw_score': raw_score,\n                'adjusted_score': adjusted_score,\n                'weight': weight,\n                'contribution': round(contribution, 2),\n                'penalty_applied': validation_result['penalty_applied'],\n                'penalties': validation_result['penalties_applied'],\n                'missing_info_count': missing_count,\n                'evidence_count': evidence_count,\n                'critical_missing': validation_result.get('critical_missing', 0),\n                'minor_missing': validation_result.get('minor_missing', 0)\n            }\n            total_score += contribution\n        \n        total_score = round(total_score, 2)\n        \n        if total_score >= 70:\n            risk_band = \"LOW RISK\"\n            recommendation = \"Favourable - Fast-track or Term Sheet\"\n        elif total_score >= 50:\n            risk_band = \"MODERATE RISK\"\n            recommendation = \"Proceed to Deeper DD (drill program, PEA, more testwork)\"\n        else:\n            risk_band = \"HIGH RISK\"\n            recommendation = \"Reject or Restructure (farm-out, lower price, more data required)\"\n        \n        probability_of_success = total_score / 100.0\n        \n        return {\n            'total_score': total_score,\n            'risk_band': risk_band,\n            'recommendation': recommendation,\n            'probability_of_success': round(probability_of_success, 4),\n            'category_contributions': category_contributions\n        }\n    \n    @staticmethod\n    def calculate_sustainability_score(sustainability_categories: Dict[str, Any], custom_weights: Dict[str, float] = None) -> Dict[str, Any]:\n        \"\"\"\n        Calculate sustainability score using category scores and weights.\n        \n        Args:\n            sustainability_categories: Dictionary of sustainability category data with scores\n            custom_weights: Optional custom weights dict. If None, uses SUSTAINABILITY_CATEGORY_WEIGHTS\n        \n        Returns:\n            Dictionary with sustainability_score, rating, and category_contributions\n        \"\"\"\n        weights = custom_weights if custom_weights is not None else ScoringEngine.SUSTAINABILITY_CATEGORY_WEIGHTS\n        \n        sustainability_score = 0.0\n        category_contributions = {}\n        \n        for category_key, weight in weights.items():\n            category_data = sustainability_categories.get(category_key, {})\n            raw_score = category_data.get('score', 0)\n            \n            contribution = (raw_score / 10.0) * weight\n            category_contributions[category_key] = {\n                'raw_score': raw_score,\n                'weight': weight,\n                'contribution': round(contribution, 2)\n            }\n            sustainability_score += contribution\n        \n        sustainability_score = round(sustainability_score, 2)\n        \n        if sustainability_score >= 80:\n            rating = \"EXCELLENT\"\n            description = \"Industry-leading sustainability practices - ESG excellence\"\n        elif sustainability_score >= 65:\n            rating = \"GOOD\"\n            description = \"Strong sustainability performance - above industry standards\"\n        elif sustainability_score >= 50:\n            rating = \"MODERATE\"\n            description = \"Acceptable sustainability performance - meets basic standards\"\n        else:\n            rating = \"NEEDS IMPROVEMENT\"\n            description = \"Sustainability concerns - requires significant improvements\"\n        \n        return {\n            'sustainability_score': sustainability_score,\n            'rating': rating,\n            'description': description,\n            'category_contributions': category_contributions\n        }\n    \n    @staticmethod\n    def calculate_risk_adjusted_npv(total_score: float, unrisked_npv: float) -> Dict[str, Any]:\n        probability_of_success = total_score / 100.0\n        risk_adjusted_npv = unrisked_npv * probability_of_success\n        \n        return {\n            'unrisked_npv': unrisked_npv,\n            'probability_of_success': probability_of_success,\n            'risk_adjusted_npv': round(risk_adjusted_npv, 2)\n        }\n","path":null,"size_bytes":11298,"size_tokens":null},"auth.py":{"content":"import streamlit as st\nimport streamlit.components.v1 as components\nfrom models import User\nfrom database import get_db_session\nfrom datetime import datetime\n\ndef get_replit_auth_user():\n    \"\"\"\n    Get authenticated user information from Replit Auth headers or environment.\n    Returns user info dict if authenticated, None otherwise.\n    \"\"\"\n    import os\n    \n    try:\n        user_id = st.context.headers.get('X-Replit-User-Id')\n        username = st.context.headers.get('X-Replit-User-Name')\n        email = st.context.headers.get('X-Replit-User-Email')\n    except:\n        user_id = None\n        username = None\n        email = None\n    \n    if not user_id or not username:\n        user_id = os.environ.get('REPL_OWNER_ID', 'dev_user_1')\n        username = os.environ.get('REPL_OWNER', 'dev_user')\n        email = f\"{username}@mining-dd.local\"\n    \n    if user_id and username:\n        return {\n            'id': user_id,\n            'username': username,\n            'email': email or f\"{username}@replit.user\"\n        }\n    \n    return None\n\ndef get_or_create_user(auth_info):\n    \"\"\"\n    Get existing user or create new user in database based on Replit Auth info.\n    Returns a dict representation of the user.\n    \"\"\"\n    from sqlalchemy import or_\n    from sqlalchemy.exc import OperationalError\n    import time\n    \n    max_retries = 3\n    retry_delay = 2\n    \n    for attempt in range(max_retries):\n        try:\n            with get_db_session() as session:\n                user = session.query(User).filter(\n                    or_(User.email == auth_info['email'], User.username == auth_info['username'])\n                ).first()\n                \n                if not user:\n                    user = User(\n                        email=auth_info['email'],\n                        username=auth_info['username'],\n                        created_at=datetime.utcnow(),\n                        last_login=datetime.utcnow()\n                    )\n                    session.add(user)\n                    session.flush()\n                else:\n                    user.last_login = datetime.utcnow()\n                    session.flush()\n                \n                # Return full user data as dictionary\n                return {\n                    'id': user.id,\n                    'email': user.email,\n                    'username': user.username,\n                    'created_at': user.created_at,\n                    'last_login': user.last_login,\n                    'password_hash': user.password_hash,\n                    'is_admin': user.is_admin or False,\n                    'full_name': user.full_name,\n                    'company': user.company,\n                    'role': user.role,\n                    'phone': user.phone,\n                    'avatar_url': user.avatar_url,\n                    'api_key': user.api_key,\n                    'mfa_enabled': user.mfa_enabled,\n                    'mfa_secret': user.mfa_secret,\n                    'theme': user.theme,\n                    'notifications_enabled': user.notifications_enabled,\n                    'ai_behavior_settings': user.ai_behavior_settings,\n                    'plan_type': user.plan_type,\n                    'usage_count': user.usage_count,\n                    'usage_limit': user.usage_limit,\n                    'billing_status': user.billing_status\n                }\n        except OperationalError as e:\n            if attempt < max_retries - 1:\n                time.sleep(retry_delay * (attempt + 1))\n            else:\n                raise\n\ndef render_login_button():\n    \"\"\"\n    Render the Replit Auth login button as a Streamlit component.\n    \"\"\"\n    components.html(\"\"\"\n        <div style=\"text-align: center; padding: 2rem;\">\n            <h2>ðŸ” Please Log In</h2>\n            <p>This application requires authentication to access.</p>\n            <script authed=\"location.reload()\" src=\"https://auth.util.repl.co/script.js\"></script>\n        </div>\n    \"\"\", height=200)\n\ndef require_auth():\n    \"\"\"\n    Require authentication for the app. Returns authenticated user or shows login screen.\n    \"\"\"\n    # Check if user is already authenticated in session\n    if 'authenticated' in st.session_state and st.session_state.authenticated:\n        return st.session_state.current_user\n    \n    # Otherwise, they need to go through login page\n    # This will be handled by app.py showing the login page\n    return None\n\ndef render_user_info():\n    \"\"\"\n    Render user information in the sidebar.\n    \"\"\"\n    if 'username' in st.session_state:\n        st.sidebar.markdown(f\"ðŸ‘¤ **{st.session_state.username}**\")\n        st.sidebar.markdown(f\"ðŸ“§ {st.session_state.user_email}\")\n        st.sidebar.markdown(\"---\")\n","path":null,"size_bytes":4720,"size_tokens":null},"template_manager.py":{"content":"from typing import Dict, List, Optional, Any\nfrom sqlalchemy.orm import Session\nfrom database import get_db_session\nfrom models import ScoringTemplate\n\n\nclass TemplateManager:\n    \"\"\"Manages CRUD operations for scoring templates\"\"\"\n    \n    DEFAULT_WEIGHTS = {\n        'geology_prospectivity': 35,\n        'resource_potential': 20,\n        'economics': 15,\n        'legal_title': 10,\n        'permitting_esg': 10,\n        'data_quality': 10\n    }\n    \n    @staticmethod\n    def create_template(\n        user_id: int,\n        name: str,\n        description: str = \"\",\n        weights: Optional[Dict[str, float]] = None,\n        is_default: bool = False\n    ) -> Dict[str, Any]:\n        \"\"\"Create a new scoring template\"\"\"\n        if weights is None:\n            weights = TemplateManager.DEFAULT_WEIGHTS.copy()\n        \n        # Validate weights sum to 100\n        total_weight = sum(weights.values())\n        if abs(total_weight - 100) > 0.01:\n            raise ValueError(f\"Weights must sum to 100. Current sum: {total_weight}\")\n        \n        with get_db_session() as db:\n            # If setting as default, unset other defaults for this user\n            if is_default:\n                db.query(ScoringTemplate).filter(\n                    ScoringTemplate.user_id == user_id,\n                    ScoringTemplate.is_default == True\n                ).update({'is_default': False})\n            \n            template = ScoringTemplate(\n                user_id=user_id,\n                name=name,\n                description=description,\n                is_default=is_default,\n                geology_weight=weights.get('geology_prospectivity', 35) / 100,\n                resource_weight=weights.get('resource_potential', 20) / 100,\n                economics_weight=weights.get('economics', 15) / 100,\n                legal_weight=weights.get('legal_title', 10) / 100,\n                permitting_weight=weights.get('permitting_esg', 10) / 100,\n                data_quality_weight=weights.get('data_quality', 10) / 100\n            )\n            \n            db.add(template)\n            db.commit()\n            db.refresh(template)\n            \n            return TemplateManager._to_dict(template)\n    \n    @staticmethod\n    def get_template(template_id: int) -> Optional[Dict[str, Any]]:\n        \"\"\"Get a template by ID\"\"\"\n        with get_db_session() as db:\n            template = db.query(ScoringTemplate).filter(\n                ScoringTemplate.id == template_id\n            ).first()\n            \n            if template:\n                return TemplateManager._to_dict(template)\n            return None\n    \n    @staticmethod\n    def get_user_templates(user_id: int) -> List[Dict[str, Any]]:\n        \"\"\"Get all templates for a user\"\"\"\n        with get_db_session() as db:\n            templates = db.query(ScoringTemplate).filter(\n                ScoringTemplate.user_id == user_id\n            ).order_by(ScoringTemplate.is_default.desc(), ScoringTemplate.created_at.desc()).all()\n            \n            return [TemplateManager._to_dict(t) for t in templates]\n    \n    @staticmethod\n    def get_default_template(user_id: int) -> Optional[Dict[str, Any]]:\n        \"\"\"Get the user's default template\"\"\"\n        with get_db_session() as db:\n            template = db.query(ScoringTemplate).filter(\n                ScoringTemplate.user_id == user_id,\n                ScoringTemplate.is_default == True\n            ).first()\n            \n            if template:\n                return TemplateManager._to_dict(template)\n            return None\n    \n    @staticmethod\n    def update_template(\n        template_id: int,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        weights: Optional[Dict[str, float]] = None,\n        is_default: Optional[bool] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Update an existing template\"\"\"\n        with get_db_session() as db:\n            template = db.query(ScoringTemplate).filter(\n                ScoringTemplate.id == template_id\n            ).first()\n            \n            if not template:\n                raise ValueError(f\"Template {template_id} not found\")\n            \n            if name is not None:\n                template.name = name\n            \n            if description is not None:\n                template.description = description\n            \n            if weights is not None:\n                # Validate weights sum to 100\n                total_weight = sum(weights.values())\n                if abs(total_weight - 100) > 0.01:\n                    raise ValueError(f\"Weights must sum to 100. Current sum: {total_weight}\")\n                \n                template.geology_weight = weights.get('geology_prospectivity', 35) / 100\n                template.resource_weight = weights.get('resource_potential', 20) / 100\n                template.economics_weight = weights.get('economics', 15) / 100\n                template.legal_weight = weights.get('legal_title', 10) / 100\n                template.permitting_weight = weights.get('permitting_esg', 10) / 100\n                template.data_quality_weight = weights.get('data_quality', 10) / 100\n            \n            if is_default is not None and is_default:\n                # Unset other defaults for this user\n                db.query(ScoringTemplate).filter(\n                    ScoringTemplate.user_id == template.user_id,\n                    ScoringTemplate.id != template_id,\n                    ScoringTemplate.is_default == True\n                ).update({'is_default': False})\n                template.is_default = True\n            elif is_default is not None and not is_default:\n                template.is_default = False\n            \n            db.commit()\n            db.refresh(template)\n            \n            return TemplateManager._to_dict(template)\n    \n    @staticmethod\n    def delete_template(template_id: int) -> Dict[str, Any]:\n        \"\"\"\n        Delete a template. Returns dict with 'success' and 'message' keys.\n        Prevents deletion if template is in use by analyses.\n        \"\"\"\n        from models import Analysis\n        \n        with get_db_session() as db:\n            template = db.query(ScoringTemplate).filter(\n                ScoringTemplate.id == template_id\n            ).first()\n            \n            if not template:\n                return {'success': False, 'message': 'Template not found'}\n            \n            # Check if template is in use\n            analyses_count = db.query(Analysis).filter(\n                Analysis.scoring_template_id == template_id\n            ).count()\n            \n            if analyses_count > 0:\n                return {\n                    'success': False, \n                    'message': f'Cannot delete template in use by {analyses_count} analysis(es). Remove references first.'\n                }\n            \n            db.delete(template)\n            db.commit()\n            return {'success': True, 'message': 'Template deleted successfully'}\n    \n    @staticmethod\n    def _to_dict(template: ScoringTemplate) -> Dict[str, Any]:\n        \"\"\"Convert template to dictionary with weights as percentages\"\"\"\n        return {\n            'id': template.id,\n            'user_id': template.user_id,\n            'name': template.name,\n            'description': template.description,\n            'is_default': template.is_default,\n            'weights': {\n                'geology_prospectivity': round(template.geology_weight * 100, 1),\n                'resource_potential': round(template.resource_weight * 100, 1),\n                'economics': round(template.economics_weight * 100, 1),\n                'legal_title': round(template.legal_weight * 100, 1),\n                'permitting_esg': round(template.permitting_weight * 100, 1),\n                'data_quality': round(template.data_quality_weight * 100, 1)\n            },\n            'created_at': template.created_at,\n            'updated_at': template.updated_at\n        }\n    \n    @staticmethod\n    def get_weights_dict(template: Optional[Dict[str, Any]] = None) -> Dict[str, float]:\n        \"\"\"Get weights dictionary from template or return defaults\"\"\"\n        if template and 'weights' in template:\n            return template['weights']\n        return TemplateManager.DEFAULT_WEIGHTS.copy()\n","path":null,"size_bytes":8288,"size_tokens":null},"drill_qaqc_analyzer.py":{"content":"import pandas as pd\nimport numpy as np\nfrom typing import Dict, List, Any, Optional, Tuple\nimport io\nfrom datetime import datetime\nimport json\n\n\nclass DrillQAQCAnalyzer:\n    \n    REQUIRED_COLLAR_FIELDS = ['hole_id', 'x', 'y', 'z']\n    REQUIRED_SURVEY_FIELDS = ['hole_id', 'depth', 'azimuth', 'dip']\n    REQUIRED_ASSAY_FIELDS = ['hole_id', 'from_depth', 'to_depth']\n    \n    QC_SAMPLE_TYPES = ['standard', 'blank', 'duplicate', 'field_duplicate', 'check_lab']\n    \n    @staticmethod\n    def parse_drill_database(file_bytes: bytes, file_name: str) -> Dict[str, Any]:\n        \"\"\"\n        Parse drill database from CSV or XLSX file.\n        Returns dict with dataframes for collar, survey, assay, and lithology tables.\n        \"\"\"\n        try:\n            file_ext = file_name.lower().split('.')[-1]\n            \n            if file_ext == 'csv':\n                df = pd.read_csv(io.BytesIO(file_bytes))\n                return {'main': df, 'file_type': 'csv'}\n            \n            elif file_ext in ['xlsx', 'xls']:\n                excel_file = pd.ExcelFile(io.BytesIO(file_bytes))\n                sheets = {}\n                \n                for sheet_name in excel_file.sheet_names:\n                    sheets[sheet_name.lower()] = pd.read_excel(excel_file, sheet_name=sheet_name)\n                \n                return {'sheets': sheets, 'file_type': 'excel'}\n            \n            else:\n                return {'error': f'Unsupported file type: {file_ext}'}\n        \n        except Exception as e:\n            return {'error': f'Error parsing drill database: {str(e)}'}\n    \n    @staticmethod\n    def validate_collar_data(df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Validate collar survey data\"\"\"\n        issues = []\n        stats = {}\n        \n        df.columns = [col.lower().strip() for col in df.columns]\n        \n        for field in DrillQAQCAnalyzer.REQUIRED_COLLAR_FIELDS:\n            if field not in df.columns:\n                issues.append(f\"Missing required field: {field}\")\n        \n        if not issues:\n            missing_coords = df[['x', 'y', 'z']].isnull().any(axis=1).sum()\n            if missing_coords > 0:\n                issues.append(f\"{missing_coords} holes with missing coordinates\")\n            \n            duplicate_holes = df['hole_id'].duplicated().sum()\n            if duplicate_holes > 0:\n                issues.append(f\"{duplicate_holes} duplicate hole IDs found\")\n            \n            stats['total_holes'] = len(df)\n            stats['unique_holes'] = df['hole_id'].nunique()\n            stats['missing_coords'] = missing_coords\n            stats['duplicate_holes'] = duplicate_holes\n            \n            if 'x' in df.columns and 'y' in df.columns:\n                stats['spatial_extent'] = {\n                    'x_min': float(df['x'].min()),\n                    'x_max': float(df['x'].max()),\n                    'y_min': float(df['y'].min()),\n                    'y_max': float(df['y'].max()),\n                    'z_min': float(df['z'].min()) if 'z' in df.columns else None,\n                    'z_max': float(df['z'].max()) if 'z' in df.columns else None\n                }\n        \n        return {\n            'valid': len(issues) == 0,\n            'issues': issues,\n            'stats': stats\n        }\n    \n    @staticmethod\n    def validate_assay_intervals(df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Validate assay interval data for gaps, overlaps, and missing values\"\"\"\n        issues = []\n        stats = {}\n        \n        df.columns = [col.lower().strip() for col in df.columns]\n        \n        for field in DrillQAQCAnalyzer.REQUIRED_ASSAY_FIELDS:\n            if field not in df.columns:\n                issues.append(f\"Missing required field: {field}\")\n                return {'valid': False, 'issues': issues, 'stats': {}}\n        \n        df = df.sort_values(['hole_id', 'from_depth'])\n        \n        gaps_found = 0\n        overlaps_found = 0\n        negative_intervals = 0\n        \n        for hole_id, group in df.groupby('hole_id'):\n            group = group.sort_values('from_depth')\n            \n            for i in range(len(group) - 1):\n                current_to = group.iloc[i]['to_depth']\n                next_from = group.iloc[i + 1]['from_depth']\n                \n                if pd.notna(current_to) and pd.notna(next_from):\n                    gap = next_from - current_to\n                    if gap > 0.1:\n                        gaps_found += 1\n                    elif gap < -0.1:\n                        overlaps_found += 1\n            \n            interval_lengths = group['to_depth'] - group['from_depth']\n            negative_intervals += (interval_lengths < 0).sum()\n        \n        if gaps_found > 0:\n            issues.append(f\"{gaps_found} gaps found in sample intervals\")\n        if overlaps_found > 0:\n            issues.append(f\"{overlaps_found} overlaps found in sample intervals\")\n        if negative_intervals > 0:\n            issues.append(f\"{negative_intervals} negative interval lengths (from > to)\")\n        \n        missing_values = df[['from_depth', 'to_depth']].isnull().sum()\n        if missing_values.any():\n            issues.append(f\"Missing depth values: FROM={missing_values['from_depth']}, TO={missing_values['to_depth']}\")\n        \n        stats['total_samples'] = len(df)\n        stats['total_holes'] = df['hole_id'].nunique()\n        stats['gaps'] = gaps_found\n        stats['overlaps'] = overlaps_found\n        stats['negative_intervals'] = negative_intervals\n        stats['avg_sample_length'] = float((df['to_depth'] - df['from_depth']).mean())\n        \n        return {\n            'valid': len(issues) == 0,\n            'issues': issues,\n            'stats': stats\n        }\n    \n    @staticmethod\n    def analyze_grade_distribution(df: pd.DataFrame, element_columns: List[str]) -> Dict[str, Any]:\n        \"\"\"Analyze grade distributions and identify outliers\"\"\"\n        results = {}\n        \n        for element in element_columns:\n            if element not in df.columns:\n                continue\n            \n            values = pd.to_numeric(df[element], errors='coerce').dropna()\n            \n            if len(values) == 0:\n                continue\n            \n            q1 = values.quantile(0.25)\n            q3 = values.quantile(0.75)\n            iqr = q3 - q1\n            lower_fence = q1 - 1.5 * iqr\n            upper_fence = q3 + 1.5 * iqr\n            \n            outliers = values[(values < lower_fence) | (values > upper_fence)]\n            \n            results[element] = {\n                'count': len(values),\n                'mean': float(values.mean()),\n                'median': float(values.median()),\n                'std': float(values.std()),\n                'min': float(values.min()),\n                'max': float(values.max()),\n                'q1': float(q1),\n                'q3': float(q3),\n                'outlier_count': len(outliers),\n                'outlier_percent': round(len(outliers) / len(values) * 100, 2),\n                'upper_fence': float(upper_fence),\n                'lower_fence': float(lower_fence)\n            }\n        \n        return results\n    \n    @staticmethod\n    def analyze_qc_samples(df: pd.DataFrame, sample_type_column: str = 'sample_type') -> Dict[str, Any]:\n        \"\"\"Analyze QC sample performance (standards, blanks, duplicates)\"\"\"\n        results = {\n            'qc_summary': {},\n            'issues': []\n        }\n        \n        if sample_type_column not in df.columns:\n            results['issues'].append(f\"Sample type column '{sample_type_column}' not found\")\n            return results\n        \n        df['sample_type_clean'] = df[sample_type_column].str.lower().str.strip()\n        \n        total_samples = len(df)\n        qc_samples = df[df['sample_type_clean'].isin(DrillQAQCAnalyzer.QC_SAMPLE_TYPES)]\n        qc_count = len(qc_samples)\n        qc_percentage = round(qc_count / total_samples * 100, 2) if total_samples > 0 else 0\n        \n        results['qc_summary']['total_samples'] = total_samples\n        results['qc_summary']['qc_samples'] = qc_count\n        results['qc_summary']['qc_percentage'] = qc_percentage\n        \n        if qc_percentage < 5:\n            results['issues'].append(f\"QC samples only {qc_percentage}% of total (recommended minimum: 5-10%)\")\n        \n        by_type = qc_samples['sample_type_clean'].value_counts().to_dict()\n        results['qc_summary']['by_type'] = by_type\n        \n        standards = qc_samples[qc_samples['sample_type_clean'] == 'standard']\n        blanks = qc_samples[qc_samples['sample_type_clean'] == 'blank']\n        duplicates = qc_samples[qc_samples['sample_type_clean'].str.contains('duplicate', na=False)]\n        \n        results['qc_summary']['standards_count'] = len(standards)\n        results['qc_summary']['blanks_count'] = len(blanks)\n        results['qc_summary']['duplicates_count'] = len(duplicates)\n        \n        if len(standards) == 0:\n            results['issues'].append(\"No standard samples found (required for accuracy monitoring)\")\n        if len(blanks) == 0:\n            results['issues'].append(\"No blank samples found (required for contamination detection)\")\n        if len(duplicates) == 0:\n            results['issues'].append(\"No duplicate samples found (required for precision monitoring)\")\n        \n        return results\n    \n    @staticmethod\n    def generate_qaqc_report(validation_results: Dict[str, Any]) -> str:\n        \"\"\"Generate a comprehensive QAQC report in text format\"\"\"\n        report_lines = []\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"DRILL DATABASE QAQC REPORT\")\n        report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"\")\n        \n        if 'collar_validation' in validation_results:\n            report_lines.append(\"COLLAR DATA VALIDATION\")\n            report_lines.append(\"-\" * 80)\n            collar = validation_results['collar_validation']\n            report_lines.append(f\"Status: {'PASS' if collar['valid'] else 'FAIL'}\")\n            \n            if collar.get('stats'):\n                stats = collar['stats']\n                report_lines.append(f\"Total Holes: {stats.get('total_holes', 'N/A')}\")\n                report_lines.append(f\"Unique Holes: {stats.get('unique_holes', 'N/A')}\")\n                report_lines.append(f\"Missing Coordinates: {stats.get('missing_coords', 'N/A')}\")\n                report_lines.append(f\"Duplicate Hole IDs: {stats.get('duplicate_holes', 'N/A')}\")\n            \n            if collar.get('issues'):\n                report_lines.append(\"\\nIssues Found:\")\n                for issue in collar['issues']:\n                    report_lines.append(f\"  - {issue}\")\n            report_lines.append(\"\")\n        \n        if 'interval_validation' in validation_results:\n            report_lines.append(\"ASSAY INTERVAL VALIDATION\")\n            report_lines.append(\"-\" * 80)\n            intervals = validation_results['interval_validation']\n            report_lines.append(f\"Status: {'PASS' if intervals['valid'] else 'FAIL'}\")\n            \n            if intervals.get('stats'):\n                stats = intervals['stats']\n                report_lines.append(f\"Total Samples: {stats.get('total_samples', 'N/A')}\")\n                report_lines.append(f\"Total Holes: {stats.get('total_holes', 'N/A')}\")\n                report_lines.append(f\"Gaps: {stats.get('gaps', 'N/A')}\")\n                report_lines.append(f\"Overlaps: {stats.get('overlaps', 'N/A')}\")\n                report_lines.append(f\"Negative Intervals: {stats.get('negative_intervals', 'N/A')}\")\n                report_lines.append(f\"Avg Sample Length: {stats.get('avg_sample_length', 0):.2f}m\")\n            \n            if intervals.get('issues'):\n                report_lines.append(\"\\nIssues Found:\")\n                for issue in intervals['issues']:\n                    report_lines.append(f\"  - {issue}\")\n            report_lines.append(\"\")\n        \n        if 'grade_analysis' in validation_results:\n            report_lines.append(\"GRADE DISTRIBUTION ANALYSIS\")\n            report_lines.append(\"-\" * 80)\n            for element, stats in validation_results['grade_analysis'].items():\n                report_lines.append(f\"\\n{element.upper()}:\")\n                report_lines.append(f\"  Count: {stats['count']}\")\n                report_lines.append(f\"  Mean: {stats['mean']:.4f}\")\n                report_lines.append(f\"  Median: {stats['median']:.4f}\")\n                report_lines.append(f\"  Std Dev: {stats['std']:.4f}\")\n                report_lines.append(f\"  Min: {stats['min']:.4f} | Max: {stats['max']:.4f}\")\n                report_lines.append(f\"  Q1: {stats['q1']:.4f} | Q3: {stats['q3']:.4f}\")\n                report_lines.append(f\"  Outliers: {stats['outlier_count']} ({stats['outlier_percent']}%)\")\n            report_lines.append(\"\")\n        \n        if 'qc_analysis' in validation_results:\n            report_lines.append(\"QC SAMPLE ANALYSIS\")\n            report_lines.append(\"-\" * 80)\n            qc = validation_results['qc_analysis']\n            \n            if qc.get('qc_summary'):\n                summary = qc['qc_summary']\n                report_lines.append(f\"Total Samples: {summary.get('total_samples', 'N/A')}\")\n                report_lines.append(f\"QC Samples: {summary.get('qc_samples', 'N/A')} ({summary.get('qc_percentage', 0)}%)\")\n                report_lines.append(f\"  - Standards: {summary.get('standards_count', 0)}\")\n                report_lines.append(f\"  - Blanks: {summary.get('blanks_count', 0)}\")\n                report_lines.append(f\"  - Duplicates: {summary.get('duplicates_count', 0)}\")\n            \n            if qc.get('issues'):\n                report_lines.append(\"\\nIssues Found:\")\n                for issue in qc['issues']:\n                    report_lines.append(f\"  - {issue}\")\n            report_lines.append(\"\")\n        \n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"END OF QAQC REPORT\")\n        report_lines.append(\"=\" * 80)\n        \n        return \"\\n\".join(report_lines)\n    \n    @staticmethod\n    def calculate_qaqc_score(validation_results: Dict[str, Any]) -> Tuple[int, str]:\n        \"\"\"\n        Calculate a QAQC score (0-10) based on validation results.\n        Returns (score, rationale)\n        \"\"\"\n        score = 10\n        deductions = []\n        \n        if 'collar_validation' in validation_results:\n            collar = validation_results['collar_validation']\n            if not collar['valid']:\n                score -= 2\n                deductions.append(\"Collar data has critical issues\")\n            \n            stats = collar.get('stats', {})\n            if stats.get('duplicate_holes', 0) > 0:\n                score -= 1\n                deductions.append(\"Duplicate hole IDs found\")\n            if stats.get('missing_coords', 0) > 0:\n                score -= 1\n                deductions.append(\"Missing coordinate data\")\n        \n        if 'interval_validation' in validation_results:\n            intervals = validation_results['interval_validation']\n            if not intervals['valid']:\n                score -= 2\n                deductions.append(\"Sample intervals have gaps/overlaps\")\n            \n            stats = intervals.get('stats', {})\n            if stats.get('negative_intervals', 0) > 0:\n                score -= 1\n                deductions.append(\"Negative sample intervals detected\")\n        \n        if 'qc_analysis' in validation_results:\n            qc = validation_results['qc_analysis']\n            qc_pct = qc.get('qc_summary', {}).get('qc_percentage', 0)\n            \n            if qc_pct < 5:\n                score -= 2\n                deductions.append(f\"Insufficient QC samples ({qc_pct}%, recommended: 5-10%)\")\n            \n            summary = qc.get('qc_summary', {})\n            if summary.get('standards_count', 0) == 0:\n                score -= 1\n                deductions.append(\"No standard samples for accuracy monitoring\")\n            if summary.get('blanks_count', 0) == 0:\n                score -= 1\n                deductions.append(\"No blank samples for contamination detection\")\n        \n        if 'grade_analysis' in validation_results:\n            grade = validation_results['grade_analysis']\n            high_outlier_elements = [\n                elem for elem, stats in grade.items()\n                if stats.get('outlier_percent', 0) > 10\n            ]\n            if high_outlier_elements:\n                score -= 1\n                deductions.append(f\"High outlier percentage in: {', '.join(high_outlier_elements)}\")\n        \n        score = max(0, score)\n        \n        if score >= 9:\n            rationale = \"Excellent QAQC - data quality is high with minimal issues. \"\n        elif score >= 7:\n            rationale = \"Good QAQC - minor issues present but data is generally reliable. \"\n        elif score >= 5:\n            rationale = \"Moderate QAQC - several issues identified requiring attention. \"\n        else:\n            rationale = \"Poor QAQC - significant data quality issues present. \"\n        \n        if deductions:\n            rationale += \"Issues: \" + \"; \".join(deductions)\n        \n        return score, rationale\n    \n    @staticmethod\n    def perform_full_analysis(parsed_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Perform comprehensive QAQC analysis on drill database.\n        Returns validation results, scores, and report.\n        \"\"\"\n        results = {}\n        \n        if 'error' in parsed_data:\n            return {'error': parsed_data['error']}\n        \n        if parsed_data.get('file_type') == 'csv':\n            df = parsed_data['main']\n            \n            if all(field in df.columns for field in ['hole_id', 'x', 'y']):\n                results['collar_validation'] = DrillQAQCAnalyzer.validate_collar_data(df)\n            \n            if all(field in df.columns for field in ['hole_id', 'from_depth', 'to_depth']):\n                results['interval_validation'] = DrillQAQCAnalyzer.validate_assay_intervals(df)\n                \n                element_cols = [col for col in df.columns if any(x in col.lower() for x in ['au', 'ag', 'cu', 'pb', 'zn', 'grade', 'ppm', 'pct'])]\n                if element_cols:\n                    results['grade_analysis'] = DrillQAQCAnalyzer.analyze_grade_distribution(df, element_cols)\n                \n                if 'sample_type' in df.columns:\n                    results['qc_analysis'] = DrillQAQCAnalyzer.analyze_qc_samples(df)\n        \n        elif parsed_data.get('file_type') == 'excel':\n            sheets = parsed_data['sheets']\n            \n            if 'collar' in sheets:\n                results['collar_validation'] = DrillQAQCAnalyzer.validate_collar_data(sheets['collar'])\n            \n            assay_sheet = None\n            for sheet_name in ['assay', 'assays', 'geochemistry', 'samples']:\n                if sheet_name in sheets:\n                    assay_sheet = sheets[sheet_name]\n                    break\n            \n            if assay_sheet is not None:\n                results['interval_validation'] = DrillQAQCAnalyzer.validate_assay_intervals(assay_sheet)\n                \n                element_cols = [col for col in assay_sheet.columns if any(x in col.lower() for x in ['au', 'ag', 'cu', 'pb', 'zn', 'grade', 'ppm', 'pct'])]\n                if element_cols:\n                    results['grade_analysis'] = DrillQAQCAnalyzer.analyze_grade_distribution(assay_sheet, element_cols)\n                \n                if 'sample_type' in assay_sheet.columns:\n                    results['qc_analysis'] = DrillQAQCAnalyzer.analyze_qc_samples(assay_sheet)\n        \n        if results:\n            qaqc_score, qaqc_rationale = DrillQAQCAnalyzer.calculate_qaqc_score(results)\n            results['qaqc_score'] = qaqc_score\n            results['qaqc_rationale'] = qaqc_rationale\n            results['qaqc_report'] = DrillQAQCAnalyzer.generate_qaqc_report(results)\n        \n        return results\n","path":null,"size_bytes":20023,"size_tokens":null},"landing.py":{"content":"import streamlit as st\n\nst.set_page_config(\n    page_title=\"Oreplot - AI Mining Due Diligence\",\n    page_icon=\"â›ï¸\",\n    layout=\"wide\",\n    initial_sidebar_state=\"collapsed\"\n)\n\nst.markdown(\"\"\"\n<style>\n    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&display=swap');\n    \n    * {\n        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;\n    }\n    \n    .main {\n        background-color: #FFFFFF;\n    }\n    .stApp {\n        background-color: #FFFFFF;\n    }\n    \n    [data-testid=\"stSidebar\"] {\n        display: none;\n    }\n    \n    .landing-header {\n        display: flex;\n        justify-content: space-between;\n        align-items: center;\n        padding: 1.25rem 3rem;\n        background-color: #FFFFFF;\n        border-bottom: 1px solid #E5E7EB;\n        position: sticky;\n        top: 0;\n        z-index: 1000;\n    }\n    \n    .logo-section {\n        display: flex;\n        align-items: center;\n        gap: 0.75rem;\n    }\n    \n    .logo-text {\n        font-size: 1.75rem;\n        font-weight: 900;\n        background: linear-gradient(135deg, #3B82F6, #8B5CF6);\n        -webkit-background-clip: text;\n        -webkit-text-fill-color: transparent;\n        background-clip: text;\n        letter-spacing: -0.05em;\n    }\n    \n    .nav-links {\n        display: flex;\n        gap: 2.5rem;\n        align-items: center;\n    }\n    \n    .nav-link {\n        color: #334155;\n        font-weight: 500;\n        font-size: 0.95rem;\n        text-decoration: none;\n        transition: color 0.2s;\n        cursor: pointer;\n    }\n    \n    .nav-link:hover {\n        color: #3B82F6;\n    }\n    \n    .auth-buttons {\n        display: flex;\n        gap: 1rem;\n        align-items: center;\n    }\n    \n    .btn-signin {\n        color: #334155;\n        font-weight: 600;\n        font-size: 0.95rem;\n        padding: 0.5rem 1.25rem;\n        border-radius: 8px;\n        transition: background-color 0.2s;\n        cursor: pointer;\n        background-color: transparent;\n        border: none;\n    }\n    \n    .btn-signin:hover {\n        background-color: #F1F5F9;\n    }\n    \n    .btn-signup {\n        background: linear-gradient(135deg, #3B82F6, #8B5CF6);\n        color: #FFFFFF;\n        font-weight: 600;\n        font-size: 0.95rem;\n        padding: 0.5rem 1.5rem;\n        border-radius: 8px;\n        border: none;\n        cursor: pointer;\n        transition: transform 0.2s, box-shadow 0.2s;\n        box-shadow: 0 4px 6px -1px rgba(59, 130, 246, 0.2);\n    }\n    \n    .btn-signup:hover {\n        transform: translateY(-1px);\n        box-shadow: 0 8px 12px -1px rgba(59, 130, 246, 0.3);\n    }\n    \n    .hero-section {\n        text-align: center;\n        padding: 5rem 2rem;\n        background: linear-gradient(180deg, #FFFFFF 0%, #F8FAFC 100%);\n    }\n    \n    .hero-title {\n        font-size: 4rem;\n        font-weight: 900;\n        color: #0F172A;\n        line-height: 1.1;\n        letter-spacing: -0.03em;\n        margin-bottom: 1.5rem;\n    }\n    \n    .hero-gradient {\n        background: linear-gradient(135deg, #3B82F6, #8B5CF6);\n        -webkit-background-clip: text;\n        -webkit-text-fill-color: transparent;\n        background-clip: text;\n    }\n    \n    .hero-subtitle {\n        font-size: 1.35rem;\n        color: #64748B;\n        font-weight: 400;\n        margin-bottom: 3rem;\n        line-height: 1.6;\n        max-width: 700px;\n        margin-left: auto;\n        margin-right: auto;\n    }\n    \n    .hero-cta-buttons {\n        display: flex;\n        gap: 1.25rem;\n        justify-content: center;\n        align-items: center;\n    }\n    \n    .btn-primary {\n        background: linear-gradient(135deg, #3B82F6, #8B5CF6);\n        color: #FFFFFF;\n        font-weight: 600;\n        font-size: 1.05rem;\n        padding: 0.875rem 2.25rem;\n        border-radius: 10px;\n        border: none;\n        cursor: pointer;\n        transition: transform 0.2s, box-shadow 0.2s;\n        box-shadow: 0 8px 16px -4px rgba(59, 130, 246, 0.3);\n    }\n    \n    .btn-primary:hover {\n        transform: translateY(-2px);\n        box-shadow: 0 12px 20px -4px rgba(59, 130, 246, 0.4);\n    }\n    \n    .btn-secondary {\n        background: #FFFFFF;\n        color: #334155;\n        font-weight: 600;\n        font-size: 1.05rem;\n        padding: 0.875rem 2.25rem;\n        border-radius: 10px;\n        border: 2px solid #CBD5E1;\n        cursor: pointer;\n        transition: all 0.2s;\n    }\n    \n    .btn-secondary:hover {\n        border-color: #3B82F6;\n        background-color: #F0F9FF;\n    }\n    \n    .features-section {\n        padding: 5rem 3rem;\n        background-color: #FFFFFF;\n    }\n    \n    .section-title {\n        font-size: 2.5rem;\n        font-weight: 800;\n        color: #0F172A;\n        text-align: center;\n        margin-bottom: 1rem;\n        letter-spacing: -0.02em;\n    }\n    \n    .section-subtitle {\n        font-size: 1.15rem;\n        color: #64748B;\n        text-align: center;\n        margin-bottom: 4rem;\n        max-width: 600px;\n        margin-left: auto;\n        margin-right: auto;\n    }\n    \n    .feature-card {\n        background: #FFFFFF;\n        border: 1px solid #E5E7EB;\n        border-radius: 16px;\n        padding: 2rem;\n        transition: transform 0.2s, box-shadow 0.2s;\n        box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.05);\n    }\n    \n    .feature-card:hover {\n        transform: translateY(-4px);\n        box-shadow: 0 12px 24px -8px rgba(0, 0, 0, 0.12);\n    }\n    \n    .feature-icon {\n        font-size: 2.5rem;\n        margin-bottom: 1rem;\n    }\n    \n    .feature-title {\n        font-size: 1.4rem;\n        font-weight: 700;\n        color: #0F172A;\n        margin-bottom: 0.75rem;\n    }\n    \n    .feature-description {\n        color: #64748B;\n        line-height: 1.7;\n        font-size: 1rem;\n    }\n    \n    .cta-section {\n        padding: 5rem 3rem;\n        background: linear-gradient(135deg, #3B82F6, #8B5CF6);\n        text-align: center;\n    }\n    \n    .cta-title {\n        font-size: 2.75rem;\n        font-weight: 800;\n        color: #FFFFFF;\n        margin-bottom: 1.25rem;\n        letter-spacing: -0.02em;\n    }\n    \n    .cta-subtitle {\n        font-size: 1.2rem;\n        color: rgba(255, 255, 255, 0.9);\n        margin-bottom: 2.5rem;\n    }\n    \n    .btn-white {\n        background: #FFFFFF;\n        color: #3B82F6;\n        font-weight: 700;\n        font-size: 1.1rem;\n        padding: 1rem 2.5rem;\n        border-radius: 10px;\n        border: none;\n        cursor: pointer;\n        transition: transform 0.2s, box-shadow 0.2s;\n        box-shadow: 0 8px 16px -4px rgba(0, 0, 0, 0.2);\n    }\n    \n    .btn-white:hover {\n        transform: translateY(-2px);\n        box-shadow: 0 12px 20px -4px rgba(0, 0, 0, 0.3);\n    }\n    \n    .footer {\n        padding: 3rem;\n        background-color: #F8F9FB;\n        border-top: 1px solid #E5E7EB;\n    }\n    \n    .footer-text {\n        text-align: center;\n        color: #64748B;\n        font-size: 0.9rem;\n    }\n    \n    .stButton>button {\n        width: auto !important;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)\n\ncol_header1, col_header2, col_header3 = st.columns([2, 3, 2])\nwith col_header1:\n    st.image(\"attached_assets/plot_1761767561805.png\", width=150)\nwith col_header2:\n    st.markdown(\"\"\"\n    <div style=\"display: flex; gap: 2rem; align-items: center; justify-content: center; margin-top: 1.5rem;\">\n        <a class=\"nav-link\" href=\"#platform\">Platform</a>\n        <a class=\"nav-link\" href=\"#solutions\">Solutions</a>\n        <a class=\"nav-link\" href=\"#blog\">Blog</a>\n        <a class=\"nav-link\" href=\"#contact\">Contact</a>\n    </div>\n    \"\"\", unsafe_allow_html=True)\nwith col_header3:\n    col_space, col_signin, col_signup = st.columns([6, 1, 1])\n    with col_signin:\n        if st.button(\"Sign In\", key=\"header_signin\", use_container_width=True):\n            st.session_state.show_app = True\n            st.rerun()\n    with col_signup:\n        if st.button(\"Get Started\", key=\"header_signup\", use_container_width=True, type=\"primary\"):\n            st.session_state.show_app = True\n            st.rerun()\nst.markdown(\"<hr style='margin: 1rem 0; border: none; border-top: 1px solid #E5E7EB;'>\", unsafe_allow_html=True)\n\nst.markdown(\"\"\"\n<div class=\"hero-section\">\n    <div class=\"hero-title\">\n        Transform Mining Due Diligence<br>\n        with <span class=\"hero-gradient\">AI Intelligence</span>\n    </div>\n    <div class=\"hero-subtitle\">\n        Oreplot analyzes mining projects, validates drill databases, and generates comprehensive \n        investment risk scores in minutesâ€”powered by advanced AI and industry-standard QAQC protocols.\n    </div>\n</div>\n\"\"\", unsafe_allow_html=True)\n\ncol_cta1, col_cta2, col_cta3, col_cta4, col_cta5 = st.columns([2, 1, 1, 1, 2])\nwith col_cta2:\n    if st.button(\"ðŸ“§ Contact Sales\", key=\"hero_contact\", use_container_width=True):\n        st.info(\"ðŸ“§ Contact us at: cokhaligzada@gmail.com\")\nwith col_cta3:\n    if st.button(\"ðŸ“… Book Demo\", key=\"hero_demo\", use_container_width=True):\n        st.info(\"ðŸ“… Contact us to schedule a demo: cokhaligzada@gmail.com\")\nwith col_cta4:\n    if st.button(\"ðŸš€ Get Started\", key=\"hero_start\", use_container_width=True, type=\"primary\"):\n        st.session_state.show_app = True\n        st.rerun()\n\nst.markdown(\"\")\n\ncol1, col2, col3 = st.columns(3)\n\nwith col1:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ“„</div>\n        <div class=\"feature-title\">Multi-Format Document Processing</div>\n        <div class=\"feature-description\">\n            Upload PDF, DOCX, XLSX, and drill databases. Our AI extracts technical facts \n            from geological reports, assay certificates, and regulatory filings.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nwith col2:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ”</div>\n        <div class=\"feature-title\">Industry-Standard QAQC</div>\n        <div class=\"feature-description\">\n            Automated validation of drill databases including collar surveys, sample intervals, \n            duplicate analysis, outlier detection, and statistical quality control.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nwith col3:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ“Š</div>\n        <div class=\"feature-title\">Customizable Scoring Templates</div>\n        <div class=\"feature-description\">\n            Create custom scoring templates with adjustable weights for geology, economics, \n            legal, ESG, and data quality to match your investment strategy.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nst.markdown('<a id=\"platform\"></a>', unsafe_allow_html=True)\nst.markdown(\"\"\"\n<div class=\"features-section\">\n    <div class=\"section-title\">Platform Features</div>\n    <div class=\"section-subtitle\">\n        Everything you need for comprehensive mining due diligence in one platform\n    </div>\n</div>\n\"\"\", unsafe_allow_html=True)\n\ncol4, col5, col6 = st.columns(3)\n\nwith col4:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ¤–</div>\n        <div class=\"feature-title\">AI-Powered Analysis</div>\n        <div class=\"feature-description\">\n            Advanced natural language processing analyzes your documents against NI-43-101 \n            and JORC standards to provide transparent, repeatable assessments.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nwith col5:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ“ˆ</div>\n        <div class=\"feature-title\">Investment Risk Scoring</div>\n        <div class=\"feature-description\">\n            Compute weighted scores (0-100) across six categories with clear risk banding: \n            Fast-track (â‰¥70), Deeper DD (50-69), or Reject/Restructure (<50).\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nwith col6:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ“‘</div>\n        <div class=\"feature-title\">Professional PDF Reports</div>\n        <div class=\"feature-description\">\n            Generate stakeholder-ready due diligence reports with detailed analysis, \n            scores, recommendations, and QAQC findings in portable PDF format.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nst.markdown('<a id=\"solutions\"></a>', unsafe_allow_html=True)\nst.markdown(\"\"\"\n<div class=\"features-section\">\n    <div class=\"section-title\">Solutions for Every Stage</div>\n    <div class=\"section-subtitle\">\n        From early-stage exploration to near-production assets\n    </div>\n</div>\n\"\"\", unsafe_allow_html=True)\n\nsol_col1, sol_col2, sol_col3 = st.columns(3)\n\nwith sol_col1:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ”¬</div>\n        <div class=\"feature-title\">Early-Stage Exploration</div>\n        <div class=\"feature-description\">\n            Emphasize geology and prospectivity scoring with custom templates \n            optimized for greenfield projects and conceptual resources.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nwith sol_col2:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">âš’ï¸</div>\n        <div class=\"feature-title\">Resource Development</div>\n        <div class=\"feature-description\">\n            Balance resource potential, data quality, and permitting considerations \n            for advancing projects through feasibility studies.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nwith sol_col3:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ­</div>\n        <div class=\"feature-title\">Near-Production Assets</div>\n        <div class=\"feature-description\">\n            Focus on economics, legal compliance, and operational readiness \n            for projects approaching construction or production.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nst.markdown('<a id=\"blog\"></a>', unsafe_allow_html=True)\nst.markdown(\"\"\"\n<div class=\"features-section\">\n    <div class=\"section-title\">Latest Insights</div>\n    <div class=\"section-subtitle\">\n        Industry trends, best practices, and mining due diligence expertise\n    </div>\n</div>\n\"\"\", unsafe_allow_html=True)\n\nblog_col1, blog_col2, blog_col3 = st.columns(3)\n\nwith blog_col1:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ“</div>\n        <div class=\"feature-title\">The Future of Mining DD</div>\n        <div class=\"feature-description\">\n            How AI and automation are transforming traditional due diligence \n            processes in the mining sector.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nwith blog_col2:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸŽ¯</div>\n        <div class=\"feature-title\">QAQC Best Practices</div>\n        <div class=\"feature-description\">\n            Industry-standard quality control protocols for drill databases \n            and assay data validation.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nwith blog_col3:\n    st.markdown(\"\"\"\n    <div class=\"feature-card\">\n        <div class=\"feature-icon\">ðŸ’¡</div>\n        <div class=\"feature-title\">Risk Scoring Methodology</div>\n        <div class=\"feature-description\">\n            Understanding weighted category scoring and how to customize \n            templates for your investment strategy.\n        </div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n\nst.markdown('<a id=\"demo\"></a><a id=\"contact\"></a>', unsafe_allow_html=True)\nst.markdown(\"\"\"\n<div class=\"cta-section\">\n    <div class=\"cta-title\">Ready to Transform Your Mining Investment Process?</div>\n    <div class=\"cta-subtitle\">\n        Join leading mining investors using AI-powered due diligence to make faster, \n        more informed investment decisions.\n    </div>\n</div>\n\"\"\", unsafe_allow_html=True)\n\ncol_final1, col_final2, col_final3 = st.columns([2, 1, 2])\nwith col_final2:\n    if st.button(\"ðŸš€ Get Started Free\", key=\"cta_final\", use_container_width=True, type=\"primary\"):\n        st.session_state.show_app = True\n        st.rerun()\n\nst.markdown(\"\")\n\nst.markdown(\"\"\"\n<div class=\"footer\">\n    <div class=\"footer-text\">\n        Â© 2025 Oreplot. All rights reserved. | Privacy Policy | Terms of Service\n    </div>\n</div>\n\"\"\", unsafe_allow_html=True)\n","path":null,"size_bytes":16360,"size_tokens":null},"app_backup.py":{"content":"import streamlit as st\nfrom datetime import datetime\nfrom document_extractor import DocumentExtractor\nfrom ai_analyzer import MiningProjectAnalyzer\nfrom scoring_engine import ScoringEngine\nfrom report_generator import ReportGenerator\nfrom auth import require_auth, render_user_info\nfrom project_manager import ProjectManager\nfrom template_manager import TemplateManager\n\nst.set_page_config(\n    page_title=\"Oreplot - AI Mining Due Diligence\",\n    page_icon=\"â›ï¸\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\ncurrent_user = require_auth()\nrender_user_info()\n\nst.sidebar.markdown(\"## ðŸ“‚ Navigation\")\n\ncol_a, col_b = st.sidebar.columns(2)\nwith col_a:\n    if st.button(\"âž• New Analysis\", use_container_width=True):\n        st.session_state.view_mode = 'new_analysis'\n        st.session_state.current_project = None\n        st.session_state.analysis_result = None\n        st.rerun()\nwith col_b:\n    if st.button(\"âš™ï¸ Templates\", use_container_width=True):\n        st.session_state.view_mode = 'template_manager'\n        st.rerun()\n\nst.sidebar.markdown(\"## ðŸ“‚ Projects\")\nuser_projects = ProjectManager.get_user_projects(current_user['id'])\n\nif user_projects:\n    selected_project_id = st.sidebar.selectbox(\n        \"Select a project to view\",\n        options=[None] + [p['id'] for p in user_projects],\n        format_func=lambda x: \"-- Select --\" if x is None else next((p['name'] for p in user_projects if p['id'] == x), \"Unknown\")\n    )\n    \n    if selected_project_id:\n        project = next((p for p in user_projects if p['id'] == selected_project_id), None)\n        if project:\n            st.sidebar.markdown(f\"**{project['name']}**\")\n            st.sidebar.markdown(f\"ðŸ“ {project['location'] or 'N/A'}\")\n            st.sidebar.markdown(f\"âš’ï¸ {project['commodity'] or 'N/A'}\")\n            st.sidebar.markdown(f\"ðŸ“Š {project['analysis_count']} analysis(es)\")\n            \n            if st.sidebar.button(\"View Project\", use_container_width=True):\n                st.session_state.view_mode = 'view_project'\n                st.session_state.current_project = project\n                st.rerun()\nelse:\n    st.sidebar.info(\"No projects yet. Create your first analysis!\")\n\nst.markdown(\"\"\"\n<style>\n    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&display=swap');\n    \n    * {\n        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;\n    }\n    \n    .main {\n        background-color: #FFFFFF;\n    }\n    .stApp {\n        background-color: #FFFFFF;\n    }\n    \n    [data-testid=\"stSidebar\"] {\n        background-color: #F8F9FB;\n        border-right: 1px solid #E5E7EB;\n    }\n    \n    h1 {\n        color: #0F172A;\n        font-weight: 800;\n        letter-spacing: -0.8px;\n        font-size: 2.5rem;\n    }\n    h2, h3 {\n        color: #1E293B;\n        font-weight: 700;\n    }\n    h4 {\n        color: #334155;\n        font-weight: 600;\n    }\n    \n    .chat-message {\n        padding: 1.25rem;\n        border-radius: 12px;\n        margin-bottom: 1rem;\n        border: 1px solid #E5E7EB;\n        background-color: #FFFFFF;\n        box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.05);\n    }\n    .user-message {\n        border-left: 3px solid #7C3AED;\n        background-color: #F5F3FF;\n    }\n    .agent-message {\n        border-left: 3px solid #3B82F6;\n        background-color: #EFF6FF;\n    }\n    \n    .score-display {\n        font-size: 3.5rem;\n        font-weight: 900;\n        text-align: center;\n        padding: 2.5rem;\n        border-radius: 16px;\n        background: linear-gradient(135deg, #F0F9FF 0%, #EFF6FF 100%);\n        border: 2px solid #BFDBFE;\n        margin: 1.5rem 0;\n    }\n    .category-card {\n        background-color: #FFFFFF;\n        padding: 1.25rem;\n        border-radius: 12px;\n        margin: 0.75rem 0;\n        border: 1px solid #E5E7EB;\n        box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.04);\n        border-left: 4px solid #3B82F6;\n    }\n    \n    .stButton>button {\n        background: linear-gradient(135deg, #3B82F6, #8B5CF6);\n        color: #FFFFFF;\n        font-weight: 600;\n        border-radius: 10px;\n        border: none;\n        padding: 0.625rem 1.5rem;\n        font-size: 0.95rem;\n        transition: all 0.2s ease;\n        box-shadow: 0 4px 6px -1px rgba(59, 130, 246, 0.2);\n    }\n    .stButton>button:hover {\n        transform: translateY(-1px);\n        box-shadow: 0 8px 12px -1px rgba(59, 130, 246, 0.3);\n    }\n    \n    .file-upload-section {\n        background-color: #F8F9FB;\n        padding: 2rem;\n        border-radius: 16px;\n        border: 2px dashed #CBD5E1;\n    }\n    \n    .stMarkdown p, .stMarkdown li {\n        color: #475569;\n        line-height: 1.7;\n    }\n    \n    .stSelectbox label, .stTextInput label, .stTextArea label {\n        color: #334155 !important;\n        font-weight: 500 !important;\n    }\n    \n    div[data-testid=\"stExpander\"] {\n        background-color: #FFFFFF;\n        border: 1px solid #E5E7EB;\n        border-radius: 12px;\n    }\n    \n    .oreplot-logo {\n        font-size: 2.2rem;\n        font-weight: 900;\n        background: linear-gradient(135deg, #3B82F6, #8B5CF6);\n        -webkit-background-clip: text;\n        -webkit-text-fill-color: transparent;\n        background-clip: text;\n        letter-spacing: -0.05em;\n        margin-bottom: 0.5rem;\n    }\n    \n    .oreplot-subtitle {\n        color: #64748B;\n        font-size: 1.05rem;\n        font-weight: 400;\n        margin-bottom: 2rem;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)\n\nif 'history' not in st.session_state:\n    st.session_state.history = []\nif 'uploaded_files_data' not in st.session_state:\n    st.session_state.uploaded_files_data = []\nif 'analysis_result' not in st.session_state:\n    st.session_state.analysis_result = None\nif 'current_project' not in st.session_state:\n    st.session_state.current_project = None\nif 'view_mode' not in st.session_state:\n    st.session_state.view_mode = 'new_analysis'\n\ncol_logo, col_title = st.columns([1, 5])\nwith col_logo:\n    st.image(\"attached_assets/plot_1761767561805.png\", width=200)\nwith col_title:\n    st.markdown('<div class=\"oreplot-subtitle\" style=\"margin-top: 2rem;\">AI-Powered Mining Due Diligence Platform</div>', unsafe_allow_html=True)\nst.markdown(\"---\")\n\nif st.session_state.view_mode == 'view_project' and st.session_state.current_project:\n    project = st.session_state.current_project\n    st.markdown(f\"### ðŸ”ï¸ {project['name']}\")\n    \n    col_info1, col_info2 = st.columns(2)\n    with col_info1:\n        st.markdown(f\"**ðŸ“ Location:** {project['location'] or 'N/A'}\")\n        st.markdown(f\"**âš’ï¸ Commodity:** {project['commodity'] or 'N/A'}\")\n    with col_info2:\n        st.markdown(f\"**ðŸ“… Created:** {project['created_at'].strftime('%Y-%m-%d %H:%M')}\")\n        st.markdown(f\"**ðŸ“Š Analyses:** {project['analysis_count']}\")\n    \n    if project.get('description'):\n        st.markdown(f\"**Description:** {project['description']}\")\n    \n    st.markdown(\"---\")\n    st.markdown(\"### ðŸ“‹ Analysis History\")\n    \n    analyses = ProjectManager.get_project_analyses(project['id'])\n    \n    if not analyses:\n        st.info(\"No analyses yet for this project.\")\n    else:\n        for analysis_summary in analyses:\n            with st.expander(f\"Analysis from {analysis_summary['created_at'].strftime('%Y-%m-%d %H:%M')} - Score: {analysis_summary['total_score']}/100\"):\n                st.markdown(f\"**Risk Category:** {analysis_summary['risk_category']}\")\n                st.markdown(f\"**Probability of Success:** {analysis_summary['probability_of_success']*100:.1f}%\")\n                \n                if st.button(f\"View Full Analysis\", key=f\"view_analysis_{analysis_summary['id']}\"):\n                    full_analysis = ProjectManager.get_analysis_details(analysis_summary['id'])\n                    \n                    category_contributions = {}\n                    for cat_key, cat_data in full_analysis['categories'].items():\n                        category_contributions[cat_key] = {\n                            'raw_score': cat_data['score'],\n                            'weight': cat_data['weight'],\n                            'contribution': cat_data['contribution']\n                        }\n                    \n                    st.session_state.analysis_result = {\n                        'analysis': {\n                            'categories': full_analysis['categories'],\n                            'overall_observations': full_analysis.get('ai_analysis_raw', {}).get('overall_observations', '')\n                        },\n                        'scoring': {\n                            'total_score': full_analysis['total_score'],\n                            'risk_category': full_analysis['risk_category'],\n                            'risk_band': full_analysis['risk_category'],\n                            'probability_of_success': full_analysis['probability_of_success'],\n                            'recommendation': f\"Historical analysis from {full_analysis['created_at'].strftime('%Y-%m-%d')}\",\n                            'categories': full_analysis['categories'],\n                            'category_contributions': category_contributions\n                        },\n                        'recommendations': full_analysis['recommendations'],\n                        'project_id': full_analysis['project_id'],\n                        'analysis_id': full_analysis['id']\n                    }\n                    st.rerun()\n\nelif st.session_state.view_mode == 'new_analysis':\n    st.markdown(\"### ðŸ“ New Project Analysis\")\n    \n    with st.expander(\"ðŸ”ï¸ Project Information\", expanded=True):\n        project_name = st.text_input(\"Project Name *\", value=st.session_state.get('project_name', ''), key=\"project_name_input\")\n        project_description = st.text_area(\"Description\", value=st.session_state.get('project_description', ''), placeholder=\"To make proper analysis, please provide as much detail about the project as possible.\", key=\"project_description_input\")\n\nif st.session_state.view_mode == 'new_analysis':\n    col1, col2 = st.columns([2, 1])\n\n    with col1:\n        st.markdown('<div class=\"file-upload-section\">', unsafe_allow_html=True)\n        st.markdown(\"#### ðŸ“ Upload Project Documents\")\n        st.markdown(\"Accepted formats: PDF, DOCX, XLSX, CSV, TXT, JPEG, PNG\")\n        \n        uploaded_files = st.file_uploader(\n            \"Drag and drop files here\",\n            accept_multiple_files=True,\n            type=['pdf', 'docx', 'doc', 'xlsx', 'xls', 'csv', 'txt', 'jpg', 'jpeg', 'png'],\n            label_visibility=\"collapsed\"\n        )\n        st.markdown('</div>', unsafe_allow_html=True)\n        \n        if uploaded_files:\n            st.markdown(f\"**{len(uploaded_files)} file(s) uploaded:**\")\n            for file in uploaded_files:\n                st.markdown(f\"- `{file.name}` ({file.size / 1024:.1f} KB)\")\n\n    with col2:\n        st.markdown(\"#### ðŸŽ¯ Quick Actions\")\n        \n        # Template selection\n        user_templates = TemplateManager.get_user_templates(current_user['id'])\n        default_template = TemplateManager.get_default_template(current_user['id'])\n        \n        if user_templates:\n            template_options = [{'id': None, 'name': 'Standard Weights'}] + user_templates\n            default_index = 0\n            if default_template:\n                default_index = next((i for i, t in enumerate(template_options) if t.get('id') == default_template['id']), 0)\n            \n            selected_template_idx = st.selectbox(\n                \"Scoring Template\",\n                range(len(template_options)),\n                index=default_index,\n                format_func=lambda i: template_options[i]['name'] + (' â­' if template_options[i].get('is_default') else ''),\n                key=\"selected_template\"\n            )\n            selected_template = template_options[selected_template_idx]\n        else:\n            st.info(\"Using standard weights. Create custom templates in Template Manager.\")\n            selected_template = None\n        \n        can_generate = uploaded_files and st.session_state.get('project_name_input', '').strip()\n        if not st.session_state.get('project_name_input', '').strip():\n            st.warning(\"âš ï¸ Project name required\")\n    \n    if st.button(\"ðŸš€ Generate Analysis\", use_container_width=True, disabled=not can_generate):\n        with st.spinner(\"ðŸ” Processing documents...\"):\n            extracted_docs = []\n            failed_files = []\n            for file in uploaded_files:\n                file_bytes = file.read()\n                result = DocumentExtractor.extract_text(file.name, file_bytes)\n                extracted_docs.append(result)\n                if not result.get('success', False):\n                    failed_files.append(file.name)\n                file.seek(0)\n            \n            st.session_state.uploaded_files_data = extracted_docs\n            \n            drill_databases = [doc for doc in extracted_docs if doc.get('is_drill_database')]\n            if drill_databases:\n                st.success(f\"ðŸ”¬ Detected {len(drill_databases)} drill database(s) - QAQC analysis performed\")\n                for db in drill_databases:\n                    with st.expander(f\"ðŸ“Š QAQC Report: {db['file_name']}\", expanded=True):\n                        st.markdown(f\"**QAQC Score: {db.get('qaqc_score', 0)}/10**\")\n                        st.markdown(f\"_{db.get('qaqc_rationale', '')}_\")\n                        st.code(db.get('text', ''), language='text')\n            \n            if failed_files:\n                st.warning(f\"âš ï¸ Could not extract text from {len(failed_files)} file(s): {', '.join(failed_files)}\")\n            \n            st.session_state.history.append({\n                'type': 'user',\n                'content': f\"Uploaded {len(uploaded_files)} documents for analysis\",\n                'files': [f.name for f in uploaded_files],\n                'failed_files': failed_files,\n                'drill_databases': len(drill_databases),\n                'timestamp': datetime.now()\n            })\n        \n        with st.spinner(\"ðŸ¤– AI analyzing project data...\"):\n            analysis = MiningProjectAnalyzer.analyze_documents(extracted_docs)\n            \n            if analysis.get('error'):\n                st.error(f\"âŒ {analysis['error']}\")\n                if analysis.get('extraction_errors'):\n                    st.error(f\"Failed files: {', '.join(analysis['extraction_errors'])}\")\n                st.session_state.analysis_result = None\n            else:\n                # Get weights from selected template or use defaults\n                custom_weights = None\n                template_id = None\n                if selected_template and selected_template.get('id'):\n                    custom_weights = TemplateManager.get_weights_dict(selected_template)\n                    template_id = selected_template['id']\n                \n                scoring = ScoringEngine.calculate_investment_score(\n                    analysis.get('categories', {}),\n                    custom_weights=custom_weights\n                )\n                recommendations = MiningProjectAnalyzer.generate_recommendations(analysis, scoring['total_score'])\n                \n                project = ProjectManager.create_project(\n                    user_id=current_user['id'],\n                    name=st.session_state.get('project_name_input', 'Untitled Project'),\n                    description=st.session_state.get('project_description_input', ''),\n                    location=None,\n                    commodity=None\n                )\n                \n                saved_analysis = ProjectManager.save_analysis(\n                    project_id=project['id'],\n                    analysis_data=analysis,\n                    scoring_data=scoring,\n                    recommendations=recommendations,\n                    scoring_template_id=template_id\n                )\n                \n                ProjectManager.save_documents(project['id'], extracted_docs)\n                \n                st.session_state.analysis_result = {\n                    'analysis': analysis,\n                    'scoring': scoring,\n                    'recommendations': recommendations,\n                    'project_id': project['id'],\n                    'analysis_id': saved_analysis['id']\n                }\n                \n                project_name = project['name']\n                st.session_state.history.append({\n                    'type': 'agent',\n                    'content': f'Analysis complete and saved to project \"{project_name}\"',\n                    'timestamp': datetime.now()\n                })\n                \n                st.success(f\"âœ… Project '{project_name}' created and analysis saved!\")\n        \n        st.rerun()\n    \n    if st.session_state.analysis_result:\n        if st.button(\"ðŸ“„ Download PDF Report\", use_container_width=True):\n            result = st.session_state.analysis_result\n            project_name = result['analysis'].get('project_name', 'Mining Project')\n            \n            pdf_bytes = ReportGenerator.generate_pdf_report(\n                project_name=project_name,\n                analysis=result['analysis'],\n                scoring_result=result['scoring'],\n                uploaded_files=[f.name for f in uploaded_files] if uploaded_files else [],\n                recommendations=result['recommendations']\n            )\n            \n            st.download_button(\n                label=\"â¬‡ï¸ Download Report\",\n                data=pdf_bytes,\n                file_name=f\"mining_dd_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\",\n                mime=\"application/pdf\",\n                use_container_width=True\n            )\n    \n    if st.button(\"ðŸ”„ Clear Session\", use_container_width=True):\n        st.session_state.history = []\n        st.session_state.uploaded_files_data = []\n        st.session_state.analysis_result = None\n        st.rerun()\n\nst.markdown(\"---\")\n\nif st.session_state.analysis_result:\n    result = st.session_state.analysis_result\n    scoring = result['scoring']\n    analysis = result['analysis']\n    \n    st.markdown(\"### ðŸ“Š Investment Analysis Results\")\n    \n    score_color = \"#00FF88\" if scoring['total_score'] >= 70 else \"#FFB800\" if scoring['total_score'] >= 50 else \"#FF4444\"\n    \n    st.markdown(f\"\"\"\n    <div class=\"score-display\">\n        <div style=\"color: {score_color};\">{scoring['total_score']}/100</div>\n        <div style=\"font-size: 1.2rem; color: #888; margin-top: 0.5rem;\">{scoring['risk_band']}</div>\n        <div style=\"font-size: 0.9rem; color: #666; margin-top: 0.5rem;\">Probability of Success: {scoring['probability_of_success']*100:.1f}%</div>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    st.markdown(f\"**Recommendation:** {scoring['recommendation']}\")\n    \n    st.markdown(\"---\")\n    st.markdown(\"### ðŸ“ˆ Category Breakdown\")\n    \n    category_names = {\n        \"geology_prospectivity\": \"â›°ï¸ Geology / Prospectivity\",\n        \"resource_potential\": \"ðŸ’Ž Resource Potential\",\n        \"economics\": \"ðŸ’° Economics\",\n        \"legal_title\": \"âš–ï¸ Legal & Title\",\n        \"permitting_esg\": \"ðŸŒ¿ Permitting & ESG\",\n        \"data_quality\": \"ðŸ“Š Data Quality\"\n    }\n    \n    for cat_key, cat_contrib in scoring['category_contributions'].items():\n        cat_name = category_names.get(cat_key, cat_key)\n        cat_data = analysis.get('categories', {}).get(cat_key, {})\n        \n        with st.expander(f\"{cat_name} - Score: {cat_contrib['raw_score']}/10 (Contribution: {cat_contrib['contribution']})\", expanded=False):\n            st.markdown(f\"**Weight:** {cat_contrib['weight']}%\")\n            \n            if cat_data.get('rationale'):\n                st.markdown(f\"**Rationale:** {cat_data['rationale']}\")\n            \n            if cat_data.get('facts_found'):\n                st.markdown(\"**âœ“ Evidence Found:**\")\n                for fact in cat_data['facts_found']:\n                    st.markdown(f\"- {fact}\")\n            \n            if cat_data.get('missing_info'):\n                st.markdown(\"**âš ï¸ Missing Information:**\")\n                for missing in cat_data['missing_info']:\n                    st.markdown(f\"- {missing}\")\n    \n    st.markdown(\"---\")\n    st.markdown(\"### ðŸ’¡ Recommendations\")\n    for rec in result['recommendations']:\n        st.markdown(f\"- {rec}\")\n    \n    if analysis.get('overall_observations'):\n        st.markdown(\"---\")\n        st.markdown(\"### ðŸ“ Overall Observations\")\n        st.markdown(analysis['overall_observations'])\n\nelif st.session_state.view_mode == 'template_manager':\n    st.markdown(\"### âš™ï¸ Scoring Template Manager\")\n    st.markdown(\"Create and manage custom scoring templates with different category weights.\")\n    st.markdown(\"---\")\n    \n    # Display existing templates\n    user_templates = TemplateManager.get_user_templates(current_user['id'])\n    \n    tab1, tab2 = st.tabs([\"ðŸ“‹ My Templates\", \"âž• Create New Template\"])\n    \n    with tab1:\n        if not user_templates:\n            st.info(\"No custom templates yet. Create your first template in the 'Create New Template' tab!\")\n        else:\n            for template in user_templates:\n                with st.expander(f\"{'â­ ' if template['is_default'] else ''}{template['name']}\", expanded=False):\n                    st.markdown(f\"**Description:** {template.get('description', 'No description')}\")\n                    st.markdown(f\"**Created:** {template['created_at'].strftime('%Y-%m-%d %H:%M')}\")\n                    \n                    st.markdown(\"**Category Weights:**\")\n                    weights = template['weights']\n                    col1, col2 = st.columns(2)\n                    with col1:\n                        st.markdown(f\"- â›°ï¸ Geology: **{weights['geology_prospectivity']}%**\")\n                        st.markdown(f\"- ðŸ’Ž Resource: **{weights['resource_potential']}%**\")\n                        st.markdown(f\"- ðŸ’° Economics: **{weights['economics']}%**\")\n                    with col2:\n                        st.markdown(f\"- âš–ï¸ Legal: **{weights['legal_title']}%**\")\n                        st.markdown(f\"- ðŸŒ¿ Permitting: **{weights['permitting_esg']}%**\")\n                        st.markdown(f\"- ðŸ“Š Data Quality: **{weights['data_quality']}%**\")\n                    \n                    st.markdown(\"---\")\n                    \n                    col_a, col_b, col_c = st.columns(3)\n                    with col_a:\n                        if not template['is_default']:\n                            if st.button(f\"â­ Set as Default\", key=f\"default_{template['id']}\"):\n                                TemplateManager.update_template(template['id'], is_default=True)\n                                st.success(\"Template set as default!\")\n                                st.rerun()\n                    with col_b:\n                        if st.button(f\"âœï¸ Edit\", key=f\"edit_{template['id']}\"):\n                            st.session_state.editing_template = template\n                            st.rerun()\n                    with col_c:\n                        if st.button(f\"ðŸ—‘ï¸ Delete\", key=f\"delete_{template['id']}\"):\n                            result = TemplateManager.delete_template(template['id'])\n                            if result['success']:\n                                st.success(result['message'])\n                                st.rerun()\n                            else:\n                                st.error(result['message'])\n    \n    with tab2:\n        # Check if we're editing an existing template\n        editing = st.session_state.get('editing_template')\n        \n        if editing:\n            st.info(f\"Editing template: {editing['name']}\")\n            template_name = st.text_input(\"Template Name *\", value=editing['name'])\n            template_description = st.text_area(\"Description\", value=editing.get('description', ''))\n            \n            weights = editing['weights']\n        else:\n            template_name = st.text_input(\"Template Name *\")\n            template_description = st.text_area(\"Description\")\n            weights = TemplateManager.DEFAULT_WEIGHTS.copy()\n        \n        st.markdown(\"**Adjust Category Weights (must sum to 100%)**\")\n        \n        col1, col2 = st.columns(2)\n        with col1:\n            geology_weight = st.slider(\"â›°ï¸ Geology & Prospectivity\", 0, 100, int(weights.get('geology_prospectivity', 35)), key=\"geo_weight\")\n            resource_weight = st.slider(\"ðŸ’Ž Resource Potential\", 0, 100, int(weights.get('resource_potential', 20)), key=\"res_weight\")\n            economics_weight = st.slider(\"ðŸ’° Economics\", 0, 100, int(weights.get('economics', 15)), key=\"econ_weight\")\n        with col2:\n            legal_weight = st.slider(\"âš–ï¸ Legal & Title\", 0, 100, int(weights.get('legal_title', 10)), key=\"legal_weight\")\n            permitting_weight = st.slider(\"ðŸŒ¿ Permitting & ESG\", 0, 100, int(weights.get('permitting_esg', 10)), key=\"perm_weight\")\n            data_quality_weight = st.slider(\"ðŸ“Š Data Quality\", 0, 100, int(weights.get('data_quality', 10)), key=\"data_weight\")\n        \n        total_weight = geology_weight + resource_weight + economics_weight + legal_weight + permitting_weight + data_quality_weight\n        \n        if total_weight != 100:\n            st.error(f\"âš ï¸ Weights must sum to 100%. Current sum: {total_weight}%\")\n        else:\n            st.success(f\"âœ“ Weights sum to 100%\")\n        \n        is_default = st.checkbox(\"Set as default template\", value=editing.get('is_default', False) if editing else False)\n        \n        col_save, col_cancel = st.columns(2)\n        with col_save:\n            if st.button(\"ðŸ’¾ Save Template\", use_container_width=True, disabled=(total_weight != 100 or not template_name)):\n                try:\n                    new_weights = {\n                        'geology_prospectivity': geology_weight,\n                        'resource_potential': resource_weight,\n                        'economics': economics_weight,\n                        'legal_title': legal_weight,\n                        'permitting_esg': permitting_weight,\n                        'data_quality': data_quality_weight\n                    }\n                    \n                    if editing:\n                        TemplateManager.update_template(\n                            editing['id'],\n                            name=template_name,\n                            description=template_description,\n                            weights=new_weights,\n                            is_default=is_default\n                        )\n                        st.success(\"Template updated successfully!\")\n                    else:\n                        TemplateManager.create_template(\n                            user_id=current_user['id'],\n                            name=template_name,\n                            description=template_description,\n                            weights=new_weights,\n                            is_default=is_default\n                        )\n                        st.success(\"Template created successfully!\")\n                    \n                    if 'editing_template' in st.session_state:\n                        del st.session_state.editing_template\n                    st.rerun()\n                except ValueError as e:\n                    st.error(f\"Error: {e}\")\n        \n        with col_cancel:\n            if editing and st.button(\"âŒ Cancel\", use_container_width=True):\n                del st.session_state.editing_template\n                st.rerun()\n\nst.markdown(\"---\")\nst.markdown(\"### ðŸ’¬ Session History\")\n\nif not st.session_state.history:\n    st.markdown('<div class=\"chat-message agent-message\">', unsafe_allow_html=True)\n    st.markdown(\"ðŸ‘‹ **AI Agent**: Welcome! Upload your mining project documents to begin the due diligence analysis.\")\n    st.markdown('</div>', unsafe_allow_html=True)\nelse:\n    for item in st.session_state.history:\n        if item['type'] == 'user':\n            st.markdown('<div class=\"chat-message user-message\">', unsafe_allow_html=True)\n            st.markdown(f\"**You**: {item['content']}\")\n            if item.get('files'):\n                st.markdown(\"Files: \" + \", \".join([f\"`{f}`\" for f in item['files']]))\n            st.markdown(f\"<small>{item['timestamp'].strftime('%H:%M:%S')}</small>\", unsafe_allow_html=True)\n            st.markdown('</div>', unsafe_allow_html=True)\n        else:\n            st.markdown('<div class=\"chat-message agent-message\">', unsafe_allow_html=True)\n            st.markdown(f\"**AI Agent**: {item['content']}\")\n            st.markdown(f\"<small>{item['timestamp'].strftime('%H:%M:%S')}</small>\", unsafe_allow_html=True)\n            st.markdown('</div>', unsafe_allow_html=True)\n\nst.markdown(\"---\")\nst.markdown(\"<small>Powered by Replit AI Integrations â€¢ GPT-5 Analysis Engine</small>\", unsafe_allow_html=True)\n","path":null,"size_bytes":28926,"size_tokens":null},"page_modules/dashboard_page.py":{"content":"import streamlit as st\nfrom datetime import datetime, timedelta\nfrom project_manager import ProjectManager\n\ndef render_dashboard(current_user):\n    \"\"\"Render the main dashboard with project overview, summaries, and alerts\"\"\"\n    \n    st.title(\"ðŸ“Š Dashboard\")\n    st.markdown(\"### Overview of Your Mining Projects\")\n    \n    # Get user projects and analyses\n    user_projects = ProjectManager.get_user_projects(current_user['id'])\n    \n    # Calculate statistics\n    total_projects = len(user_projects)\n    total_analyses = sum(p.get('analysis_count', 0) for p in user_projects)\n    \n    # Get recent analyses\n    recent_analyses = []\n    for project in user_projects:\n        analyses = ProjectManager.get_project_analyses(project['id'])\n        for analysis in analyses:\n            recent_analyses.append({\n                'project_name': project['name'],\n                'analysis_id': analysis['id'],\n                'score': analysis['total_score'],\n                'risk': analysis['risk_category'],\n                'date': analysis['created_at']\n            })\n    \n    recent_analyses.sort(key=lambda x: x['date'], reverse=True)\n    recent_analyses = recent_analyses[:5]\n    \n    # Key Metrics Row\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.markdown(\"\"\"\n        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 1.5rem; border-radius: 12px; color: white;\">\n            <div style=\"font-size: 2rem; font-weight: 800;\">{}</div>\n            <div style=\"font-size: 0.875rem; opacity: 0.9;\">Total Projects</div>\n        </div>\n        \"\"\".format(total_projects), unsafe_allow_html=True)\n    \n    with col2:\n        st.markdown(\"\"\"\n        <div style=\"background: linear-gradient(135deg, #3B82F6 0%, #2563EB 100%); padding: 1.5rem; border-radius: 12px; color: white;\">\n            <div style=\"font-size: 2rem; font-weight: 800;\">{}</div>\n            <div style=\"font-size: 0.875rem; opacity: 0.9;\">Analyses Run</div>\n        </div>\n        \"\"\".format(total_analyses), unsafe_allow_html=True)\n    \n    # Calculate average score\n    avg_score = sum(a['score'] for a in recent_analyses) / len(recent_analyses) if recent_analyses else 0\n    \n    with col3:\n        st.markdown(\"\"\"\n        <div style=\"background: linear-gradient(135deg, #10B981 0%, #059669 100%); padding: 1.5rem; border-radius: 12px; color: white;\">\n            <div style=\"font-size: 2rem; font-weight: 800;\">{:.1f}</div>\n            <div style=\"font-size: 0.875rem; opacity: 0.9;\">Avg Score</div>\n        </div>\n        \"\"\".format(avg_score), unsafe_allow_html=True)\n    \n    # Count high-risk projects\n    high_risk_count = sum(1 for a in recent_analyses if a['score'] < 50)\n    \n    with col4:\n        st.markdown(\"\"\"\n        <div style=\"background: linear-gradient(135deg, #EF4444 0%, #DC2626 100%); padding: 1.5rem; border-radius: 12px; color: white;\">\n            <div style=\"font-size: 2rem; font-weight: 800;\">{}</div>\n            <div style=\"font-size: 0.875rem; opacity: 0.9;\">High Risk</div>\n        </div>\n        \"\"\".format(high_risk_count), unsafe_allow_html=True)\n    \n    st.markdown(\"<br>\", unsafe_allow_html=True)\n    \n    # Recent Analyses Section\n    col_left, col_right = st.columns([2, 1])\n    \n    with col_left:\n        st.markdown(\"### ðŸ“ˆ Recent Analyses\")\n        \n        if recent_analyses:\n            for analysis in recent_analyses:\n                # Determine color based on score\n                if analysis['score'] >= 70:\n                    color = \"#10B981\"\n                    icon = \"âœ…\"\n                elif analysis['score'] >= 50:\n                    color = \"#F59E0B\"\n                    icon = \"âš ï¸\"\n                else:\n                    color = \"#EF4444\"\n                    icon = \"âŒ\"\n                \n                st.markdown(f\"\"\"\n                <div style=\"background: white; padding: 1rem; border-radius: 8px; border-left: 4px solid {color}; margin-bottom: 1rem; box-shadow: 0 1px 3px rgba(0,0,0,0.1);\">\n                    <div style=\"display: flex; justify-content: space-between; align-items: center;\">\n                        <div>\n                            <div style=\"font-weight: 600; color: #0F172A;\">{icon} {analysis['project_name']}</div>\n                            <div style=\"font-size: 0.875rem; color: #64748B; margin-top: 0.25rem;\">\n                                {analysis['risk']} â€¢ Score: {analysis['score']:.1f}/100\n                            </div>\n                        </div>\n                        <div style=\"font-size: 0.75rem; color: #94A3B8;\">\n                            {analysis['date'].strftime('%b %d, %Y') if hasattr(analysis['date'], 'strftime') else 'Recent'}\n                        </div>\n                    </div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n        else:\n            st.info(\"No analyses yet. Create your first project to get started!\")\n    \n    with col_right:\n        st.markdown(\"### ðŸ”” Alerts & Notifications\")\n        \n        # Generate smart alerts based on data\n        alerts = []\n        \n        # Alert for high-risk projects\n        if high_risk_count > 0:\n            alerts.append({\n                'type': 'warning',\n                'icon': 'âš ï¸',\n                'title': 'High Risk Projects',\n                'message': f'{high_risk_count} project(s) require attention'\n            })\n        \n        # Alert for usage limit\n        usage_percent = (current_user.get('usage_count', 0) / current_user.get('usage_limit', 10)) * 100\n        if usage_percent > 80:\n            alerts.append({\n                'type': 'info',\n                'icon': 'ðŸ“Š',\n                'title': 'Usage Limit',\n                'message': f'{usage_percent:.0f}% of monthly limit used'\n            })\n        \n        # Welcome message if new user\n        if total_analyses == 0:\n            alerts.append({\n                'type': 'success',\n                'icon': 'ðŸ‘‹',\n                'title': 'Welcome to Oreplot!',\n                'message': 'Start your first analysis to unlock AI-powered insights'\n            })\n        \n        if alerts:\n            for alert in alerts:\n                bg_color = {\n                    'warning': '#FEF3C7',\n                    'info': '#DBEAFE',\n                    'success': '#D1FAE5'\n                }.get(alert['type'], '#F3F4F6')\n                \n                text_color = {\n                    'warning': '#92400E',\n                    'info': '#1E40AF',\n                    'success': '#065F46'\n                }.get(alert['type'], '#1F2937')\n                \n                st.markdown(f\"\"\"\n                <div style=\"background: {bg_color}; padding: 1rem; border-radius: 8px; margin-bottom: 0.75rem;\">\n                    <div style=\"font-weight: 600; color: {text_color};\">{alert['icon']} {alert['title']}</div>\n                    <div style=\"font-size: 0.875rem; color: {text_color}; margin-top: 0.25rem; opacity: 0.9;\">\n                        {alert['message']}\n                    </div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n        else:\n            st.success(\"All systems normal! No alerts at this time.\")\n    \n    # Quick Actions\n    st.markdown(\"<br>\", unsafe_allow_html=True)\n    st.markdown(\"### âš¡ Quick Actions\")\n    \n    col_a, col_b, col_c = st.columns(3)\n    \n    with col_a:\n        if st.button(\"âž• New Analysis\", use_container_width=True, type=\"primary\"):\n            st.session_state.current_page = 'ai_agent'\n            st.rerun()\n    \n    with col_b:\n        if st.button(\"ðŸ“ View All Projects\", use_container_width=True):\n            st.session_state.current_page = 'projects'\n            st.rerun()\n    \n    with col_c:\n        if st.button(\"ðŸ“„ Generate Reports\", use_container_width=True):\n            st.session_state.current_page = 'reports'\n            st.rerun()\n","path":null,"size_bytes":7887,"size_tokens":null},"page_modules/app_settings_page.py":{"content":"import streamlit as st\nfrom database import get_db_session\nfrom models import User\nimport json\n\ndef render_app_settings_page(current_user):\n    \"\"\"Render app settings page for AI behavior, notifications, theme configuration\"\"\"\n    \n    st.title(\"ðŸŽ¨ App Settings\")\n    st.markdown(\"### Customize Your Experience\")\n    \n    # current_user is already a dictionary with all user data\n    # Appearance Settings\n    st.markdown(\"#### ðŸŽ¨ Appearance\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        theme = st.selectbox(\n            \"Theme\",\n            options=[\"light\", \"dark\", \"auto\"],\n            index=[\"light\", \"dark\", \"auto\"].index(current_user.get('theme') or \"light\"),\n            format_func=lambda x: {\"light\": \"â˜€ï¸ Light\", \"dark\": \"ðŸŒ™ Dark\", \"auto\": \"ðŸ”„ Auto\"}.get(x, x)\n        )\n        \n        if theme != current_user.get('theme'):\n            with get_db_session() as db:\n                user_to_update = db.query(User).filter(User.id == current_user['id']).first()\n                user_to_update.theme = theme\n                db.commit()\n            st.success(f\"Theme changed to {theme}\")\n            st.rerun()\n    \n    with col2:\n        st.info(\"Theme settings will be applied on next login\")\n    \n    st.markdown(\"---\")\n    \n    # Notification Settings\n    st.markdown(\"#### ðŸ”” Notifications\")\n    \n    notifications_enabled = st.checkbox(\n        \"Enable Notifications\",\n        value=current_user.get('notifications_enabled') if current_user.get('notifications_enabled') is not None else True\n    )\n    \n    if notifications_enabled != current_user.get('notifications_enabled'):\n        with get_db_session() as db:\n            user_to_update = db.query(User).filter(User.id == current_user['id']).first()\n            user_to_update.notifications_enabled = notifications_enabled\n            db.commit()\n        st.success(f\"Notifications {'enabled' if notifications_enabled else 'disabled'}\")\n        st.rerun()\n    \n    if notifications_enabled:\n        st.markdown(\"**Notify me when:**\")\n        notify_col1, notify_col2 = st.columns(2)\n        \n        with notify_col1:\n            st.checkbox(\"Analysis is complete\", value=True)\n            st.checkbox(\"High-risk project detected\", value=True)\n        \n        with notify_col2:\n            st.checkbox(\"Usage limit reached\", value=True)\n            st.checkbox(\"New team member joins\", value=False)\n    \n    st.markdown(\"---\")\n    \n    # AI Behavior Settings\n    st.markdown(\"#### ðŸ¤– AI Analysis Settings\")\n    st.caption(\"Customize how the AI analyzes your mining projects\")\n    \n    # Load current AI settings\n    ai_settings = current_user.get('ai_behavior_settings') or {}\n    if isinstance(ai_settings, str):\n        try:\n            ai_settings = json.loads(ai_settings)\n        except:\n            ai_settings = {}\n    \n    analysis_depth = st.select_slider(\n        \"Analysis Depth\",\n        options=[\"Quick\", \"Standard\", \"Detailed\", \"Comprehensive\"],\n        value=ai_settings.get('analysis_depth', 'Standard')\n    )\n    \n    risk_sensitivity = st.slider(\n        \"Risk Sensitivity\",\n        min_value=1,\n        max_value=10,\n        value=ai_settings.get('risk_sensitivity', 5),\n        help=\"Higher values make the AI more conservative in risk assessment\"\n    )\n    \n    focus_areas = st.multiselect(\n        \"Primary Focus Areas\",\n        options=[\"Geology\", \"Resources\", \"Economics\", \"Legal\", \"Environment\", \"Data Quality\"],\n        default=ai_settings.get('focus_areas', [\"Geology\", \"Resources\", \"Economics\"])\n    )\n    \n    include_recommendations = st.checkbox(\n        \"Include Detailed Recommendations\",\n        value=ai_settings.get('include_recommendations', True)\n    )\n    \n    # Save AI settings button\n    if st.button(\"ðŸ’¾ Save AI Settings\", type=\"primary\"):\n        new_ai_settings = {\n            'analysis_depth': analysis_depth,\n            'risk_sensitivity': risk_sensitivity,\n            'focus_areas': focus_areas,\n            'include_recommendations': include_recommendations\n        }\n        \n        with get_db_session() as db:\n            user_to_update = db.query(User).filter(User.id == current_user['id']).first()\n            user_to_update.ai_behavior_settings = json.dumps(new_ai_settings)\n            db.commit()\n        \n        st.success(\"âœ… AI settings saved successfully!\")\n        st.rerun()\n    \n    st.markdown(\"---\")\n    \n    # Data & Privacy\n    st.markdown(\"#### ðŸ”’ Data & Privacy\")\n    \n    col_privacy1, col_privacy2 = st.columns(2)\n    \n    with col_privacy1:\n        st.checkbox(\"Allow analytics for product improvement\", value=True)\n        st.checkbox(\"Share anonymous usage data\", value=False)\n    \n    with col_privacy2:\n        if st.button(\"ðŸ“¥ Export My Data\", use_container_width=True):\n            st.info(\"Data export functionality coming soon!\")\n        \n        if st.button(\"ðŸ—‘ï¸ Clear Analysis History\", use_container_width=True):\n            st.warning(\"This will delete all your analysis history. This action cannot be undone!\")\n","path":null,"size_bytes":5022,"size_tokens":null},"page_modules/projects_page.py":{"content":"import streamlit as st\nfrom datetime import datetime\nfrom project_manager import ProjectManager\n\ndef render_projects_page(current_user):\n    \"\"\"Render the projects page with list view and management\"\"\"\n    \n    st.title(\"ðŸ“ Projects\")\n    st.markdown(\"### Manage Your Mining Assets\")\n    \n    # Get user projects\n    user_projects = ProjectManager.get_user_projects(current_user['id'])\n    \n    # Search and filter bar\n    col_search, col_filter, col_sort = st.columns([3, 1, 1])\n    \n    with col_search:\n        search_query = st.text_input(\"ðŸ” Search projects\", placeholder=\"Enter project name or commodity...\")\n    \n    with col_filter:\n        filter_option = st.selectbox(\"Filter by\", [\"All Projects\", \"High Risk\", \"Moderate Risk\", \"Low Risk\"])\n    \n    with col_sort:\n        sort_option = st.selectbox(\"Sort by\", [\"Recent\", \"Name A-Z\", \"Score High-Low\"])\n    \n    # Filter projects based on search\n    filtered_projects = user_projects\n    if search_query:\n        filtered_projects = [\n            p for p in user_projects \n            if search_query.lower() in p['name'].lower() or \n            (p.get('commodity') and search_query.lower() in p['commodity'].lower())\n        ]\n    \n    # Sort projects\n    if sort_option == \"Name A-Z\":\n        filtered_projects.sort(key=lambda x: x['name'])\n    elif sort_option == \"Recent\":\n        filtered_projects.sort(key=lambda x: x.get('updated_at', x.get('created_at')), reverse=True)\n    \n    st.markdown(f\"**{len(filtered_projects)} projects** found\")\n    st.markdown(\"---\")\n    \n    # Display projects in grid layout\n    if filtered_projects:\n        for i in range(0, len(filtered_projects), 2):\n            cols = st.columns(2)\n            \n            for j, col in enumerate(cols):\n                if i + j < len(filtered_projects):\n                    project = filtered_projects[i + j]\n                    \n                    with col:\n                        # Get latest analysis for this project\n                        analyses = ProjectManager.get_project_analyses(project['id'])\n                        latest_analysis = analyses[0] if analyses else None\n                        \n                        # Determine status color\n                        if latest_analysis:\n                            score = latest_analysis['total_score']\n                            if score >= 70:\n                                status_color = \"#10B981\"\n                                status_text = \"LOW RISK\"\n                            elif score >= 50:\n                                status_color = \"#F59E0B\"\n                                status_text = \"MODERATE\"\n                            else:\n                                status_color = \"#EF4444\"\n                                status_text = \"HIGH RISK\"\n                            \n                            # Determine analysis type\n                            analysis_type = latest_analysis.get('analysis_type', 'light_ai')\n                            if analysis_type == 'advanced_ai':\n                                ai_type_badge = '<span style=\"background: #8B5CF6; color: white; padding: 2px 8px; border-radius: 999px; font-size: 0.7rem; margin-left: 8px;\">Oreplot Advanced</span>'\n                            else:\n                                ai_type_badge = '<span style=\"background: #3B82F6; color: white; padding: 2px 8px; border-radius: 999px; font-size: 0.7rem; margin-left: 8px;\">Oreplot Light</span>'\n                        else:\n                            status_color = \"#94A3B8\"\n                            status_text = \"NO ANALYSIS\"\n                            ai_type_badge = \"\"\n                        \n                        # Safely format project data\n                        project_name = str(project.get('name', 'Unnamed')).replace('\"', '&quot;').replace(\"'\", '&#39;')\n                        description = str(project.get('description') or 'No description').replace('\"', '&quot;').replace(\"'\", '&#39;')[:100]\n                        location = str(project.get('location') or 'N/A').replace('\"', '&quot;').replace(\"'\", '&#39;')\n                        commodity = str(project.get('commodity') or 'N/A').replace('\"', '&quot;').replace(\"'\", '&#39;')\n                        analysis_count = project.get('analysis_count', 0)\n                        \n                        updated_at = project.get('updated_at', project.get('created_at'))\n                        if hasattr(updated_at, 'strftime'):\n                            updated_str = updated_at.strftime('%b %d')\n                        else:\n                            updated_str = 'Recent'\n                        \n                        card_html = f\"\"\"\n                        <div style=\"background: white; padding: 1.5rem; border-radius: 12px; border: 1px solid #E5E7EB; margin-bottom: 1rem; box-shadow: 0 1px 3px rgba(0,0,0,0.1);\">\n                            <div style=\"display: flex; justify-content: space-between; align-items: start; margin-bottom: 1rem;\">\n                                <div style=\"font-weight: 700; font-size: 1.125rem; color: #0F172A;\">{project_name}{ai_type_badge}</div>\n                                <div style=\"background: {status_color}; color: white; padding: 0.25rem 0.75rem; border-radius: 999px; font-size: 0.75rem; font-weight: 600;\">{status_text}</div>\n                            </div>\n                            <div style=\"color: #64748B; font-size: 0.875rem; margin-bottom: 1rem; min-height: 2.5rem;\">{description}</div>\n                            <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 0.5rem; margin-bottom: 1rem; font-size: 0.875rem;\">\n                                <div><span style=\"color: #94A3B8;\">ðŸ“ Location:</span> {location}</div>\n                                <div><span style=\"color: #94A3B8;\">âš’ï¸ Commodity:</span> {commodity}</div>\n                                <div><span style=\"color: #94A3B8;\">ðŸ“Š Analyses:</span> {analysis_count}</div>\n                                <div><span style=\"color: #94A3B8;\">ðŸ“… Updated:</span> {updated_str}</div>\n                            </div>\n                        </div>\n                        \"\"\"\n                        st.markdown(card_html, unsafe_allow_html=True)\n                        \n                        col_a, col_b, col_c = st.columns(3)\n                        with col_a:\n                            if st.button(\"ðŸ“Š View\", key=f\"view_{project['id']}\", use_container_width=True):\n                                st.session_state.current_project = project\n                                st.session_state.view_mode = 'view_project'\n                                st.session_state.current_page = 'ai_agent'\n                                st.rerun()\n                        \n                        with col_b:\n                            if st.button(\"âž• Analyze\", key=f\"analyze_{project['id']}\", use_container_width=True):\n                                st.session_state.current_page = 'ai_agent'\n                                st.session_state.view_mode = 'new_analysis'\n                                st.rerun()\n                        \n                        with col_c:\n                            if st.button(\"ðŸ—‘ï¸ Delete\", key=f\"delete_{project['id']}\", use_container_width=True, type=\"secondary\"):\n                                st.session_state[f'confirm_delete_{project[\"id\"]}'] = True\n                                st.rerun()\n                        \n                        # Confirmation dialog\n                        if st.session_state.get(f'confirm_delete_{project[\"id\"]}', False):\n                            st.warning(f\"âš ï¸ Are you sure you want to delete '{project['name']}'? This will also delete all associated analyses and documents.\")\n                            col_yes, col_no = st.columns(2)\n                            with col_yes:\n                                if st.button(\"Yes, Delete\", key=f\"confirm_yes_{project['id']}\", type=\"primary\"):\n                                    from database import get_db_session\n                                    from models import Project\n                                    with get_db_session() as db:\n                                        proj_to_delete = db.query(Project).filter(Project.id == project['id']).first()\n                                        if proj_to_delete:\n                                            db.delete(proj_to_delete)\n                                            db.commit()\n                                    st.session_state[f'confirm_delete_{project[\"id\"]}'] = False\n                                    st.success(\"Project deleted successfully!\")\n                                    st.rerun()\n                            with col_no:\n                                if st.button(\"Cancel\", key=f\"confirm_no_{project['id']}\"):\n                                    st.session_state[f'confirm_delete_{project[\"id\"]}'] = False\n                                    st.rerun()\n    else:\n        st.info(\"No projects found. Create your first project to get started!\")\n        \n        if st.button(\"âž• Create New Project\", type=\"primary\"):\n            st.session_state.current_page = 'ai_agent'\n            st.session_state.view_mode = 'new_analysis'\n            st.rerun()\n","path":null,"size_bytes":9280,"size_tokens":null},"page_modules/billing_page.py":{"content":"import streamlit as st\nfrom database import get_db_session\nfrom models import Analysis\nfrom datetime import datetime\nfrom sqlalchemy import func, extract\n\ndef render_billing_page(current_user):\n    \"\"\"Render usage page showing monthly AI analysis usage metrics\"\"\"\n    \n    st.title(\"ðŸ“Š Usage\")\n    st.markdown(\"### Monthly AI Analysis Usage Metrics\")\n    \n    # Get current month and year\n    current_date = datetime.now()\n    current_month = current_date.month\n    current_year = current_date.year\n    \n    # Calculate usage statistics from database\n    with get_db_session() as session:\n        # Get analyses for current month\n        month_analyses_orm = session.query(Analysis).join(\n            Analysis.project\n        ).filter(\n            Analysis.project.has(user_id=current_user['id']),\n            extract('month', Analysis.created_at) == current_month,\n            extract('year', Analysis.created_at) == current_year\n        ).all()\n        \n        # Materialize to dictionaries to avoid DetachedInstanceError\n        month_analyses = []\n        for analysis in month_analyses_orm:\n            month_analyses.append({\n                'id': analysis.id,\n                'project_name': analysis.project.name if analysis.project else 'Unknown Project',\n                'total_score': analysis.total_score,\n                'created_at': analysis.created_at\n            })\n        \n        # Get total analyses count\n        total_analyses = session.query(func.count(Analysis.id)).join(\n            Analysis.project\n        ).filter(\n            Analysis.project.has(user_id=current_user['id'])\n        ).scalar()\n        \n        # Get analyses by risk category this month\n        low_risk = len([a for a in month_analyses if a['total_score'] >= 70])\n        moderate_risk = len([a for a in month_analyses if 50 <= a['total_score'] < 70])\n        high_risk = len([a for a in month_analyses if a['total_score'] < 50])\n    \n    # Display current month usage\n    st.markdown(f\"### ðŸ“… {current_date.strftime('%B %Y')} Usage\")\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.markdown(f\"\"\"\n        <div style=\"background: white; padding: 1.5rem; border-radius: 12px; border: 1px solid #E5E7EB; text-align: center; box-shadow: 0 1px 3px rgba(0,0,0,0.1);\">\n            <div style=\"font-size: 2.5rem; font-weight: 800; color: #3B82F6; margin-bottom: 0.5rem;\">\n                {len(month_analyses)}\n            </div>\n            <div style=\"color: #64748B; font-size: 0.875rem; font-weight: 600;\">\n                ANALYSES THIS MONTH\n            </div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    with col2:\n        st.markdown(f\"\"\"\n        <div style=\"background: white; padding: 1.5rem; border-radius: 12px; border: 1px solid #E5E7EB; text-align: center; box-shadow: 0 1px 3px rgba(0,0,0,0.1);\">\n            <div style=\"font-size: 2.5rem; font-weight: 800; color: #10B981; margin-bottom: 0.5rem;\">\n                {low_risk}\n            </div>\n            <div style=\"color: #64748B; font-size: 0.875rem; font-weight: 600;\">\n                LOW RISK\n            </div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    with col3:\n        st.markdown(f\"\"\"\n        <div style=\"background: white; padding: 1.5rem; border-radius: 12px; border: 1px solid #E5E7EB; text-align: center; box-shadow: 0 1px 3px rgba(0,0,0,0.1);\">\n            <div style=\"font-size: 2.5rem; font-weight: 800; color: #F59E0B; margin-bottom: 0.5rem;\">\n                {moderate_risk}\n            </div>\n            <div style=\"color: #64748B; font-size: 0.875rem; font-weight: 600;\">\n                MODERATE RISK\n            </div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    with col4:\n        st.markdown(f\"\"\"\n        <div style=\"background: white; padding: 1.5rem; border-radius: 12px; border: 1px solid #E5E7EB; text-align: center; box-shadow: 0 1px 3px rgba(0,0,0,0.1);\">\n            <div style=\"font-size: 2.5rem; font-weight: 800; color: #EF4444; margin-bottom: 0.5rem;\">\n                {high_risk}\n            </div>\n            <div style=\"color: #64748B; font-size: 0.875rem; font-weight: 600;\">\n                HIGH RISK\n            </div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    st.markdown(\"---\")\n    \n    # All-time usage statistics\n    st.markdown(\"### ðŸ“Š All-Time Statistics\")\n    \n    col_all1, col_all2 = st.columns(2)\n    \n    with col_all1:\n        st.markdown(f\"\"\"\n        <div style=\"background: linear-gradient(135deg, #3B82F6 0%, #8B5CF6 100%); padding: 2rem; border-radius: 12px; color: white; text-align: center; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\">\n            <div style=\"font-size: 3.5rem; font-weight: 800; margin-bottom: 0.5rem;\">\n                {total_analyses}\n            </div>\n            <div style=\"font-size: 1.25rem; opacity: 0.9;\">\n                Total Analyses Completed\n            </div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    with col_all2:\n        avg_score = (sum(a['total_score'] for a in month_analyses) / len(month_analyses)) if month_analyses else 0\n        \n        st.markdown(f\"\"\"\n        <div style=\"background: white; padding: 2rem; border-radius: 12px; border: 1px solid #E5E7EB; text-align: center; box-shadow: 0 1px 3px rgba(0,0,0,0.1);\">\n            <div style=\"font-size: 3.5rem; font-weight: 800; color: #3B82F6; margin-bottom: 0.5rem;\">\n                {avg_score:.1f}\n            </div>\n            <div style=\"font-size: 1.25rem; color: #64748B;\">\n                Average Score This Month\n            </div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    st.markdown(\"---\")\n    \n    # Monthly breakdown\n    st.markdown(\"### ðŸ“ˆ Recent Activity\")\n    \n    if month_analyses:\n        st.markdown(\"**Recent analyses this month:**\")\n        \n        for analysis in sorted(month_analyses, key=lambda x: x['created_at'], reverse=True)[:10]:\n            # Determine risk color\n            if analysis['total_score'] >= 70:\n                risk_color = \"#10B981\"\n                risk_label = \"LOW RISK\"\n            elif analysis['total_score'] >= 50:\n                risk_color = \"#F59E0B\"\n                risk_label = \"MODERATE\"\n            else:\n                risk_color = \"#EF4444\"\n                risk_label = \"HIGH RISK\"\n            \n            st.markdown(f\"\"\"\n            <div style=\"background: white; padding: 1rem; border-radius: 8px; border-left: 4px solid {risk_color}; margin-bottom: 0.5rem; box-shadow: 0 1px 3px rgba(0,0,0,0.1);\">\n                <div style=\"display: flex; justify-content: space-between; align-items: center;\">\n                    <div>\n                        <div style=\"font-weight: 600; color: #0F172A;\">{analysis['project_name']}</div>\n                        <div style=\"font-size: 0.875rem; color: #64748B;\">{analysis['created_at'].strftime('%b %d, %Y at %I:%M %p')}</div>\n                    </div>\n                    <div style=\"text-align: right;\">\n                        <div style=\"font-size: 1.5rem; font-weight: 700; color: {risk_color};\">{analysis['total_score']:.1f}</div>\n                        <div style=\"background: {risk_color}; color: white; padding: 0.25rem 0.75rem; border-radius: 999px; font-size: 0.75rem; font-weight: 600;\">\n                            {risk_label}\n                        </div>\n                    </div>\n                </div>\n            </div>\n            \"\"\", unsafe_allow_html=True)\n    else:\n        st.info(\"No analyses completed this month yet. Start a new analysis to see your usage statistics!\")\n","path":null,"size_bytes":7572,"size_tokens":null},"page_modules/team_page.py":{"content":"import streamlit as st\nfrom database import get_db_session\nfrom models import Team, TeamMember, User\nfrom datetime import datetime\n\ndef render_team_page(current_user):\n    \"\"\"Render team/members page for managing access and roles\"\"\"\n    \n    st.title(\"ðŸ‘¥ Team & Members\")\n    st.markdown(\"### Collaborate with Your Team\")\n    \n    # Get user's teams (owned and member of) - eagerly load members\n    with get_db_session() as db:\n        owned_teams_query = db.query(Team).filter(Team.owner_id == current_user['id']).all()\n        member_teams_query = db.query(Team).join(TeamMember).filter(TeamMember.user_id == current_user['id']).all()\n        \n        # Convert to dicts with all needed data while session is open\n        owned_teams = []\n        for team in owned_teams_query:\n            team_dict = {\n                'id': team.id,\n                'name': team.name,\n                'description': team.description,\n                'owner_id': team.owner_id,\n                'created_at': team.created_at,\n                'members': []\n            }\n            for member in team.members:\n                member_user = db.query(User).filter(User.id == member.user_id).first()\n                team_dict['members'].append({\n                    'id': member.id,\n                    'user_id': member.user_id,\n                    'username': member_user.username if member_user else 'Unknown',\n                    'email': member_user.email if member_user else '',\n                    'role': member.role,\n                    'status': member.status\n                })\n            owned_teams.append(team_dict)\n        \n        member_teams = []\n        for team in member_teams_query:\n            if team.id not in [t['id'] for t in owned_teams]:  # Avoid duplicates\n                team_dict = {\n                    'id': team.id,\n                    'name': team.name,\n                    'description': team.description,\n                    'owner_id': team.owner_id,\n                    'created_at': team.created_at,\n                    'members': []\n                }\n                for member in team.members:\n                    member_user = db.query(User).filter(User.id == member.user_id).first()\n                    team_dict['members'].append({\n                        'id': member.id,\n                        'user_id': member.user_id,\n                        'username': member_user.username if member_user else 'Unknown',\n                        'email': member_user.email if member_user else '',\n                        'role': member.role,\n                        'status': member.status\n                    })\n                member_teams.append(team_dict)\n    \n    # Tabs for different views\n    tab1, tab2, tab3 = st.tabs([\"My Teams\", \"Create Team\", \"Invitations\"])\n    \n    with tab1:\n        st.markdown(\"#### Your Teams\")\n        \n        all_teams = owned_teams + member_teams\n        \n        if all_teams:\n            for team in all_teams:\n                is_owner = team['owner_id'] == current_user['id']\n                \n                with st.expander(f\"{'ðŸ‘‘' if is_owner else 'ðŸ‘¤'} {team['name']} ({len(team['members'])} members)\"):\n                    st.markdown(f\"**Description:** {team['description'] or 'No description'}\")\n                    st.markdown(f\"**Created:** {team['created_at'].strftime('%B %d, %Y') if hasattr(team['created_at'], 'strftime') else 'Recently'}\")\n                    \n                    if is_owner:\n                        st.success(\"âœ“ You are the owner of this team\")\n                    \n                    st.markdown(\"---\")\n                    st.markdown(\"**Team Members:**\")\n                    \n                    # Display team members\n                    for member in team['members']:\n                        col_member, col_role, col_actions = st.columns([3, 1, 1])\n                        \n                        with col_member:\n                            owner_badge = \" ðŸ‘‘\" if member['user_id'] == team['owner_id'] else \"\"\n                            st.markdown(f\"**{member['username']}**{owner_badge}\")\n                            st.caption(member['email'])\n                        \n                        with col_role:\n                            st.markdown(f\"**{member['role'].upper()}**\")\n                        \n                        with col_actions:\n                            if is_owner and member['user_id'] != team['owner_id']:\n                                if st.button(\"Remove\", key=f\"remove_{member['id']}\", use_container_width=True):\n                                    with get_db_session() as db:\n                                        member_to_remove = db.query(TeamMember).filter(TeamMember.id == member['id']).first()\n                                        db.delete(member_to_remove)\n                                        db.commit()\n                                    st.success(f\"Removed {member['username']} from team\")\n                                    st.rerun()\n                    \n                    if is_owner:\n                        st.markdown(\"---\")\n                        st.markdown(\"**Invite New Member:**\")\n                        \n                        col_email, col_role, col_invite = st.columns([3, 1, 1])\n                        \n                        with col_email:\n                            invite_email = st.text_input(\"Email\", key=f\"invite_email_{team['id']}\", placeholder=\"colleague@example.com\")\n                        \n                        with col_role:\n                            invite_role = st.selectbox(\"Role\", options=[\"member\", \"admin\"], key=f\"invite_role_{team['id']}\")\n                        \n                        with col_invite:\n                            st.markdown(\"<br>\", unsafe_allow_html=True)\n                            if st.button(\"ðŸ“§ Send Invite\", key=f\"send_invite_{team['id']}\", use_container_width=True):\n                                if invite_email:\n                                    with get_db_session() as db:\n                                        invited_user = db.query(User).filter(User.email == invite_email).first()\n                                        \n                                        if invited_user:\n                                            existing_member = db.query(TeamMember).filter(\n                                                TeamMember.team_id == team['id'],\n                                                TeamMember.user_id == invited_user.id\n                                            ).first()\n                                            \n                                            if not existing_member:\n                                                new_member = TeamMember(\n                                                    team_id=team['id'],\n                                                    user_id=invited_user.id,\n                                                    role=invite_role,\n                                                    status='invited'\n                                                )\n                                                db.add(new_member)\n                                                db.commit()\n                                                st.success(f\"âœ… Invitation sent to {invite_email}!\")\n                                                st.rerun()\n                                            else:\n                                                st.warning(\"User is already a team member\")\n                                        else:\n                                            st.error(\"User not found. They need to create an account first.\")\n                                else:\n                                    st.error(\"Please enter an email address\")\n        else:\n            st.info(\"You're not part of any teams yet. Create your first team to collaborate!\")\n    \n    with tab2:\n        st.markdown(\"#### Create a New Team\")\n        \n        with st.form(\"create_team_form\"):\n            team_name = st.text_input(\"Team Name\", placeholder=\"Exploration Team Alpha\")\n            team_description = st.text_area(\"Description\", placeholder=\"Describe the purpose of this team...\")\n            \n            submitted = st.form_submit_button(\"âž• Create Team\", type=\"primary\")\n            \n            if submitted:\n                if team_name:\n                    with get_db_session() as db:\n                        new_team = Team(\n                            name=team_name,\n                            description=team_description,\n                            owner_id=current_user['id']\n                        )\n                        db.add(new_team)\n                        db.flush()\n                        \n                        # Add owner as first member\n                        owner_member = TeamMember(\n                            team_id=new_team.id,\n                            user_id=current_user['id'],\n                            role='owner',\n                            status='active',\n                            joined_at=datetime.utcnow()\n                        )\n                        db.add(owner_member)\n                        db.commit()\n                    \n                    st.success(f\"âœ… Team '{team_name}' created successfully!\")\n                    st.rerun()\n                else:\n                    st.error(\"Please enter a team name\")\n    \n    with tab3:\n        st.markdown(\"#### Pending Invitations\")\n        \n        # Get pending invitations for current user - convert to dicts\n        with get_db_session() as db:\n            pending_invites_query = db.query(TeamMember).filter(\n                TeamMember.user_id == current_user['id'],\n                TeamMember.status == 'invited'\n            ).all()\n            \n            pending_invites = []\n            for invite in pending_invites_query:\n                team = db.query(Team).filter(Team.id == invite.team_id).first()\n                owner = db.query(User).filter(User.id == team.owner_id).first() if team else None\n                \n                pending_invites.append({\n                    'id': invite.id,\n                    'team_id': invite.team_id,\n                    'role': invite.role,\n                    'team_name': team.name if team else 'Unknown',\n                    'owner_username': owner.username if owner else 'Unknown',\n                    'owner_email': owner.email if owner else ''\n                })\n        \n        if pending_invites:\n            for invite in pending_invites:\n                col_info, col_actions = st.columns([3, 1])\n                \n                with col_info:\n                    st.markdown(f\"\"\"\n                    <div style=\"background: white; padding: 1rem; border-radius: 8px; border: 1px solid #E5E7EB;\">\n                        <div style=\"font-weight: 600; font-size: 1.125rem; margin-bottom: 0.5rem;\">{invite['team_name']}</div>\n                        <div style=\"font-size: 0.875rem; color: #64748B;\">\n                            Invited by: {invite['owner_username']} ({invite['owner_email']})<br>\n                            Role: {invite['role'].upper()}\n                        </div>\n                    </div>\n                    \"\"\", unsafe_allow_html=True)\n                \n                with col_actions:\n                    col_accept, col_decline = st.columns(2)\n                    \n                    with col_accept:\n                        if st.button(\"âœ… Accept\", key=f\"accept_{invite['id']}\", use_container_width=True):\n                            with get_db_session() as db:\n                                member_to_update = db.query(TeamMember).filter(TeamMember.id == invite['id']).first()\n                                member_to_update.status = 'active'\n                                member_to_update.joined_at = datetime.utcnow()\n                                db.commit()\n                            st.success(\"Invitation accepted!\")\n                            st.rerun()\n                    \n                    with col_decline:\n                        if st.button(\"âŒ Decline\", key=f\"decline_{invite['id']}\", use_container_width=True):\n                            with get_db_session() as db:\n                                member_to_delete = db.query(TeamMember).filter(TeamMember.id == invite['id']).first()\n                                db.delete(member_to_delete)\n                                db.commit()\n                            st.info(\"Invitation declined\")\n                            st.rerun()\n                \n                st.markdown(\"<br>\", unsafe_allow_html=True)\n        else:\n            st.info(\"No pending invitations\")\n    \n    st.markdown(\"---\")\n    \n    # Team Benefits\n    st.markdown(\"### âœ¨ Team Features\")\n    \n    col_feat1, col_feat2, col_feat3 = st.columns(3)\n    \n    with col_feat1:\n        st.markdown(\"\"\"\n        <div style=\"text-align: center; padding: 1.5rem;\">\n            <div style=\"font-size: 3rem;\">ðŸ¤</div>\n            <div style=\"font-weight: 600; margin-top: 0.5rem;\">Collaborate</div>\n            <div style=\"font-size: 0.875rem; color: #64748B; margin-top: 0.25rem;\">Work together on projects</div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    with col_feat2:\n        st.markdown(\"\"\"\n        <div style=\"text-align: center; padding: 1.5rem;\">\n            <div style=\"font-size: 3rem;\">ðŸ”’</div>\n            <div style=\"font-weight: 600; margin-top: 0.5rem;\">Role-Based Access</div>\n            <div style=\"font-size: 0.875rem; color: #64748B; margin-top: 0.25rem;\">Control permissions</div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    with col_feat3:\n        st.markdown(\"\"\"\n        <div style=\"text-align: center; padding: 1.5rem;\">\n            <div style=\"font-size: 3rem;\">ðŸ“Š</div>\n            <div style=\"font-weight: 600; margin-top: 0.5rem;\">Shared Insights</div>\n            <div style=\"font-size: 0.875rem; color: #64748B; margin-top: 0.25rem;\">Share analyses</div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n","path":null,"size_bytes":14081,"size_tokens":null},"page_modules/reports_page.py":{"content":"import streamlit as st\nfrom datetime import datetime\nfrom project_manager import ProjectManager\nfrom report_generator import ReportGenerator\n\ndef render_reports_page(current_user):\n    \"\"\"Render the reports page with auto-generated reports and exports\"\"\"\n    \n    st.title(\"ðŸ“„ Reports\")\n    st.markdown(\"### Download and Export Due Diligence Reports\")\n    \n    # Get all user projects and analyses\n    user_projects = ProjectManager.get_user_projects(current_user['id'])\n    \n    # Collect all analyses across projects\n    all_reports = []\n    for project in user_projects:\n        analyses = ProjectManager.get_project_analyses(project['id'])\n        for analysis in analyses:\n            all_reports.append({\n                'project': project,\n                'analysis': analysis\n            })\n    \n    # Sort by date\n    all_reports.sort(key=lambda x: x['analysis']['created_at'], reverse=True)\n    \n    # Summary statistics\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.metric(\"Total Reports\", len(all_reports))\n    \n    with col2:\n        this_month = sum(1 for r in all_reports if (datetime.now() - r['analysis']['created_at']).days < 30)\n        st.metric(\"This Month\", this_month)\n    \n    with col3:\n        avg_score = sum(r['analysis']['total_score'] for r in all_reports) / len(all_reports) if all_reports else 0\n        st.metric(\"Avg Score\", f\"{avg_score:.1f}\")\n    \n    st.markdown(\"---\")\n    \n    # Filter options\n    col_filter, col_export = st.columns([3, 1])\n    \n    with col_filter:\n        filter_option = st.selectbox(\n            \"Filter reports by risk category\",\n            [\"All Reports\", \"Low Risk\", \"Moderate Risk\", \"High Risk\"]\n        )\n    \n    with col_export:\n        if st.button(\"ðŸ“¦ Batch Export\", use_container_width=True):\n            st.info(\"Batch export feature coming soon!\")\n    \n    # Filter reports\n    filtered_reports = all_reports\n    if filter_option == \"Low Risk\":\n        filtered_reports = [r for r in all_reports if r['analysis']['total_score'] >= 70]\n    elif filter_option == \"Moderate Risk\":\n        filtered_reports = [r for r in all_reports if 50 <= r['analysis']['total_score'] < 70]\n    elif filter_option == \"High Risk\":\n        filtered_reports = [r for r in all_reports if r['analysis']['total_score'] < 50]\n    \n    st.markdown(f\"**{len(filtered_reports)} reports** available\")\n    st.markdown(\"---\")\n    \n    # Display reports\n    if filtered_reports:\n        for report in filtered_reports:\n            project = report['project']\n            analysis = report['analysis']\n            \n            # Determine status color\n            score = analysis['total_score']\n            if score >= 70:\n                status_color = \"#10B981\"\n                risk_badge = \"LOW RISK\"\n            elif score >= 50:\n                status_color = \"#F59E0B\"\n                risk_badge = \"MODERATE\"\n            else:\n                status_color = \"#EF4444\"\n                risk_badge = \"HIGH RISK\"\n            \n            # Determine analysis type badge\n            analysis_type = analysis.get('analysis_type', 'light_ai')\n            if analysis_type == 'advanced_ai':\n                ai_badge = \"ðŸŸ£ Oreplot Advanced\"\n                ai_badge_color = \"#8B5CF6\"\n            else:\n                ai_badge = \"ðŸ”µ Oreplot Light\"\n                ai_badge_color = \"#3B82F6\"\n            \n            col_info, col_actions = st.columns([3, 1])\n            \n            with col_info:\n                # Create expandable section for detailed view\n                with st.expander(f\"ðŸ“‹ {project['name']} ({ai_badge}) - {risk_badge} (Score: {score:.1f}/100)\", expanded=False):\n                    st.markdown(f\"\"\"\n                    <div style=\"background: white; padding: 1rem; border-radius: 8px; border-left: 4px solid {status_color};\">\n                        <div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 1rem; font-size: 0.875rem; color: #64748B; margin-bottom: 1rem;\">\n                            <div><strong>Score:</strong> {score:.1f}/100</div>\n                            <div><strong>Success Rate:</strong> {analysis['probability_of_success']*100:.1f}%</div>\n                            <div><strong>Analysis ID:</strong> #{analysis['id']}</div>\n                        </div>\n                    </div>\n                    \"\"\", unsafe_allow_html=True)\n                    \n                    # Display detailed category findings\n                    st.markdown(\"### ðŸ“Š Category Analysis\")\n                    \n                    # Get the structured category data from ai_analysis_raw\n                    ai_raw_data = analysis.get('ai_analysis_raw', {})\n                    categories_data = ai_raw_data.get('categories', {})\n                    \n                    category_info = [\n                        ('geology_prospectivity', 'ðŸŒ Geology/Prospectivity', analysis.get('geology_score', 0), analysis.get('geology_weight', 0)),\n                        ('resource_potential', 'ðŸ’Ž Resource Potential', analysis.get('resource_score', 0), analysis.get('resource_weight', 0)),\n                        ('economics', 'ðŸ’° Economics', analysis.get('economics_score', 0), analysis.get('economics_weight', 0)),\n                        ('legal_title', 'âš–ï¸ Legal/Title', analysis.get('legal_score', 0), analysis.get('legal_weight', 0)),\n                        ('permitting_esg', 'ðŸŒ± Permitting/ESG', analysis.get('permitting_score', 0), analysis.get('permitting_weight', 0)),\n                        ('data_quality', 'ðŸ“ˆ Data Quality', analysis.get('data_quality_score', 0), analysis.get('data_quality_weight', 0))\n                    ]\n                    \n                    for cat_key, cat_name, score, weight in category_info:\n                        with st.container():\n                            st.markdown(f\"**{cat_name}** (Score: {score:.1f}/10, Weight: {weight*100:.0f}%)\")\n                            \n                            cat_data = categories_data.get(cat_key, {})\n                            \n                            # Display rationale\n                            if cat_data.get('rationale'):\n                                st.markdown(f\"**Rationale:** {cat_data['rationale']}\")\n                            \n                            # Display facts found\n                            if cat_data.get('facts_found'):\n                                st.markdown(\"**âœ“ Evidence Found:**\")\n                                for fact in cat_data['facts_found']:\n                                    st.markdown(f\"- {fact}\")\n                            \n                            # Display missing information\n                            if cat_data.get('missing_info'):\n                                st.markdown(\"**âš ï¸ Missing Information:**\")\n                                for missing in cat_data['missing_info']:\n                                    st.markdown(f\"- {missing}\")\n                            \n                            # Fallback to formatted findings if structured data not available\n                            if not cat_data and analysis.get(f'{cat_key.split(\"_\")[0]}_findings'):\n                                st.markdown(analysis.get(f'{cat_key.split(\"_\")[0]}_findings', 'No findings available'))\n                            elif not cat_data:\n                                st.markdown('No findings available')\n                            \n                            st.markdown(\"---\")\n                    \n                    # Recommendations\n                    if analysis.get('recommendations'):\n                        st.markdown(\"### ðŸ’¡ Recommendations\")\n                        recommendations = analysis.get('recommendations')\n                        if isinstance(recommendations, list):\n                            for rec in recommendations:\n                                st.markdown(f\"â€¢ {rec}\")\n                        else:\n                            st.markdown(str(recommendations))\n            \n            with col_actions:\n                # Get the full analysis data from ai_analysis_raw for PDF generation\n                ai_raw_data = analysis.get('ai_analysis_raw', {})\n                analysis_data = {\n                    'project_name': project['name'],\n                    'categories': ai_raw_data.get('categories', {}),\n                    'overall_observations': ai_raw_data.get('overall_observations', '')\n                }\n                \n                scoring_result = {\n                    'total_score': analysis['total_score'],\n                    'risk_band': analysis['risk_category'],\n                    'probability_of_success': analysis['probability_of_success'],\n                    'category_contributions': {\n                        'geology_prospectivity': {\n                            'raw_score': analysis.get('geology_score', 0),\n                            'weight': analysis.get('geology_weight', 0),\n                            'contribution': analysis.get('geology_contribution', 0)\n                        },\n                        'resource_potential': {\n                            'raw_score': analysis.get('resource_score', 0),\n                            'weight': analysis.get('resource_weight', 0),\n                            'contribution': analysis.get('resource_contribution', 0)\n                        },\n                        'economics': {\n                            'raw_score': analysis.get('economics_score', 0),\n                            'weight': analysis.get('economics_weight', 0),\n                            'contribution': analysis.get('economics_contribution', 0)\n                        },\n                        'legal_title': {\n                            'raw_score': analysis.get('legal_score', 0),\n                            'weight': analysis.get('legal_weight', 0),\n                            'contribution': analysis.get('legal_contribution', 0)\n                        },\n                        'permitting_esg': {\n                            'raw_score': analysis.get('permitting_score', 0),\n                            'weight': analysis.get('permitting_weight', 0),\n                            'contribution': analysis.get('permitting_contribution', 0)\n                        },\n                        'data_quality': {\n                            'raw_score': analysis.get('data_quality_score', 0),\n                            'weight': analysis.get('data_quality_weight', 0),\n                            'contribution': analysis.get('data_quality_contribution', 0)\n                        }\n                    }\n                }\n                \n                docs = ProjectManager.get_project_documents(project['id'])\n                uploaded_files = [doc['file_name'] for doc in docs]\n                recommendations = analysis.get('recommendations', [])\n                \n                # Extract sustainability data if available\n                sustainability_scoring = None\n                sustainability_analysis = None\n                \n                if analysis.get('sustainability_score') is not None:\n                    # Determine rating based on sustainability score\n                    sust_score = analysis['sustainability_score']\n                    if sust_score >= 80:\n                        rating = \"EXCELLENT\"\n                        description = \"Industry-leading sustainability practices - ESG excellence\"\n                    elif sust_score >= 65:\n                        rating = \"GOOD\"\n                        description = \"Strong sustainability performance - above industry standards\"\n                    elif sust_score >= 50:\n                        rating = \"MODERATE\"\n                        description = \"Acceptable sustainability performance - meets basic standards\"\n                    else:\n                        rating = \"NEEDS IMPROVEMENT\"\n                        description = \"Sustainability concerns - requires significant improvements\"\n                    \n                    sustainability_scoring = {\n                        'sustainability_score': sust_score,\n                        'rating': rating,\n                        'description': description,\n                        'category_contributions': {\n                            'environmental': {\n                                'raw_score': analysis.get('environmental_score', 0),\n                                'weight': analysis.get('environmental_weight', 0),\n                                'contribution': analysis.get('environmental_contribution', 0)\n                            },\n                            'social': {\n                                'raw_score': analysis.get('social_score', 0),\n                                'weight': analysis.get('social_weight', 0),\n                                'contribution': analysis.get('social_contribution', 0)\n                            },\n                            'governance': {\n                                'raw_score': analysis.get('governance_score', 0),\n                                'weight': analysis.get('governance_weight', 0),\n                                'contribution': analysis.get('governance_contribution', 0)\n                            },\n                            'climate': {\n                                'raw_score': analysis.get('climate_score', 0),\n                                'weight': analysis.get('climate_weight', 0),\n                                'contribution': analysis.get('climate_contribution', 0)\n                            }\n                        }\n                    }\n                    \n                    # Extract sustainability analysis from ai_analysis_raw\n                    sustainability_raw = ai_raw_data.get('sustainability_categories', {})\n                    if sustainability_raw:\n                        sustainability_analysis = {\n                            'sustainability_categories': sustainability_raw,\n                            'overall_sustainability_notes': ai_raw_data.get('overall_sustainability_notes', '')\n                        }\n                \n                pdf_bytes = ReportGenerator.generate_pdf_report(\n                    project['name'],\n                    analysis_data,\n                    scoring_result,\n                    uploaded_files,\n                    recommendations,\n                    sustainability_analysis=sustainability_analysis,\n                    sustainability_scoring=sustainability_scoring,\n                    analysis_type=analysis_type\n                )\n                \n                st.download_button(\n                    label=\"ðŸ“¥ Download PDF\",\n                    data=pdf_bytes,\n                    file_name=f\"{project['name']}_Report_{analysis['id']}.pdf\",\n                    mime=\"application/pdf\",\n                    use_container_width=True,\n                    key=f\"download_{analysis['id']}\"\n                )\n            \n            st.markdown(\"<br>\", unsafe_allow_html=True)\n    else:\n        st.info(\"No reports available. Run an analysis to generate reports!\")\n        \n        if st.button(\"âž• Create New Analysis\", type=\"primary\"):\n            st.session_state.current_page = 'ai_agent'\n            st.session_state.view_mode = 'new_analysis'\n            st.rerun()\n","path":null,"size_bytes":15324,"size_tokens":null},"components/navigation.py":{"content":"import streamlit as st\n\ndef render_top_navigation():\n    \"\"\"Render the top navigation bar with main menu items\"\"\"\n    \n    st.markdown(\"\"\"\n    <style>\n        .top-nav {\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            padding: 1rem 2rem;\n            margin: -1rem -1rem 2rem -1rem;\n            border-radius: 0;\n            box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n        }\n        .nav-container {\n            display: flex;\n            justify-content: space-between;\n            align-items: center;\n            max-width: 1400px;\n            margin: 0 auto;\n        }\n        .nav-brand {\n            font-size: 1.5rem;\n            font-weight: 800;\n            color: white;\n            text-decoration: none;\n        }\n        .nav-menu {\n            display: flex;\n            gap: 2rem;\n            list-style: none;\n            margin: 0;\n            padding: 0;\n        }\n        .nav-item {\n            color: white;\n            cursor: pointer;\n            font-weight: 500;\n            padding: 0.5rem 1rem;\n            border-radius: 6px;\n            transition: all 0.3s ease;\n        }\n        .nav-item:hover {\n            background: rgba(255,255,255,0.2);\n        }\n        .nav-item.active {\n            background: rgba(255,255,255,0.3);\n        }\n        .user-menu-btn {\n            background: rgba(255,255,255,0.2);\n            color: white;\n            border: none;\n            padding: 0.5rem 1rem;\n            border-radius: 6px;\n            cursor: pointer;\n            font-weight: 500;\n            transition: all 0.3s ease;\n        }\n        .user-menu-btn:hover {\n            background: rgba(255,255,255,0.3);\n        }\n        /* Responsive navigation - prevent text squeezing */\n        .stButton button {\n            height: auto !important;\n            min-height: 2.5rem !important;\n        }\n        div[data-testid=\"column\"] button {\n            white-space: nowrap !important;\n            min-width: fit-content !important;\n            padding: 0.5rem 0.4rem !important;\n            font-size: 0.85rem !important;\n        }\n        div[data-testid=\"column\"] button div {\n            display: flex !important;\n            flex-direction: row !important;\n            align-items: center !important;\n            justify-content: center !important;\n            width: 100% !important;\n        }\n        div[data-testid=\"column\"] button p {\n            white-space: nowrap !important;\n            font-size: inherit !important;\n            margin: 0 !important;\n            line-height: 1.3 !important;\n            overflow: visible !important;\n        }\n        /* When sidebar is open, make navigation more compact */\n        [data-testid=\"stSidebar\"][aria-expanded=\"true\"] ~ [data-testid=\"stMainBlockContainer\"] div[data-testid=\"column\"] button {\n            font-size: 0.75rem !important;\n            padding: 0.5rem 0.3rem !important;\n        }\n        /* Responsive breakpoints with better handling */\n        @media (max-width: 1400px) {\n            div[data-testid=\"column\"] button {\n                font-size: 0.8rem !important;\n                padding: 0.5rem 0.35rem !important;\n            }\n        }\n        @media (max-width: 1200px) {\n            div[data-testid=\"column\"] button {\n                font-size: 0.75rem !important;\n                padding: 0.5rem 0.3rem !important;\n            }\n        }\n        @media (max-width: 1000px) {\n            div[data-testid=\"column\"] button {\n                font-size: 0.7rem !important;\n                padding: 0.4rem 0.25rem !important;\n            }\n        }\n        .logo-container img {\n            max-height: 40px;\n            width: auto;\n        }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n    \n    # Initialize current page if not set\n    if 'current_page' not in st.session_state:\n        st.session_state.current_page = 'dashboard'\n    \n    # Create navigation bar with better proportions\n    col1, col2, col3, col4, col5, col6, col_spacer = st.columns([1.0, 1.2, 1.0, 1.0, 1.2, 1.2, 1.2])\n    \n    with col1:\n        st.image(\"attached_assets/plot_1761827661787.png\", width=90)\n    \n    with col2:\n        if st.button(\"ðŸ“Š Dashboard\", use_container_width=True, key=\"nav_dashboard\"):\n            st.session_state.current_page = 'dashboard'\n            st.rerun()\n    \n    with col3:\n        if st.button(\"ðŸ“ Projects\", use_container_width=True, key=\"nav_projects\"):\n            st.session_state.current_page = 'projects'\n            st.rerun()\n    \n    with col4:\n        if st.button(\"ðŸ“„ Reports\", use_container_width=True, key=\"nav_reports\"):\n            st.session_state.current_page = 'reports'\n            st.rerun()\n    \n    with col5:\n        if st.button(\"ðŸ’° Financials\", use_container_width=True, key=\"nav_financials\"):\n            st.session_state.current_page = 'financials'\n            st.rerun()\n    \n    with col6:\n        if st.button(\"ðŸ¤– AI Agent\", use_container_width=True, key=\"nav_ai_agent\"):\n            st.session_state.current_page = 'ai_agent'\n            st.rerun()\n    \n    with col_spacer:\n        # User menu button will be rendered separately\n        pass\n    \n    return st.session_state.current_page\n\n\ndef render_user_menu_dropdown(current_user):\n    \"\"\"Render user dropdown menu with profile, settings, etc.\"\"\"\n    \n    st.markdown(\"\"\"\n    <style>\n        .user-menu-container {\n            position: relative;\n        }\n        .user-info-box {\n            background: #F8F9FB;\n            padding: 1rem;\n            border-radius: 8px;\n            border: 1px solid #E5E7EB;\n            margin-bottom: 1rem;\n        }\n        .user-email {\n            color: #64748B;\n            font-size: 0.875rem;\n        }\n        .menu-section {\n            margin-top: 1rem;\n        }\n        .menu-section-title {\n            font-size: 0.75rem;\n            font-weight: 600;\n            color: #64748B;\n            text-transform: uppercase;\n            letter-spacing: 0.5px;\n            margin-bottom: 0.5rem;\n        }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n    \n    with st.sidebar:\n        st.markdown(\"---\")\n        st.markdown(f\"### ðŸ‘¤ {current_user.get('username', 'User')}\")\n        st.markdown(f\"<div class='user-email'>{current_user.get('email', '')}</div>\", unsafe_allow_html=True)\n        \n        st.markdown(\"#### User Menu\")\n        \n        if st.button(\"ðŸ‘¤ Profile\", use_container_width=True):\n            st.session_state.current_page = 'profile'\n            st.rerun()\n        \n        if st.button(\"âš™ï¸ Account Settings\", use_container_width=True):\n            st.session_state.current_page = 'account_settings'\n            st.rerun()\n        \n        if st.button(\"ðŸŽ¨ App Settings\", use_container_width=True):\n            st.session_state.current_page = 'app_settings'\n            st.rerun()\n        \n        if st.button(\"ðŸ“Š Usage\", use_container_width=True):\n            st.session_state.current_page = 'billing'\n            st.rerun()\n        \n        if st.button(\"ðŸ‘¥ Team / Members\", use_container_width=True):\n            st.session_state.current_page = 'team'\n            st.rerun()\n        \n        # Admin Panel - show for all admin users\n        if current_user.get('is_admin'):\n            st.markdown(\"---\")\n            st.markdown(\"#### Administration\")\n            if st.button(\"Admin Panel\", use_container_width=True, type=\"secondary\"):\n                st.session_state.current_page = 'admin_panel'\n                st.rerun()\n            if st.button(\"Comparables Admin\", use_container_width=True, type=\"secondary\"):\n                st.session_state.current_page = 'admin_comparables'\n                st.rerun()\n        \n        st.markdown(\"---\")\n        \n        if st.button(\"ðŸšª Logout\", use_container_width=True, type=\"primary\"):\n            # Clear session\n            for key in list(st.session_state.keys()):\n                del st.session_state[key]\n            st.rerun()\n","path":null,"size_bytes":7931,"size_tokens":null},".streamlit/config.toml":{"content":"[server]\nheadless = true\naddress = \"0.0.0.0\"\nport = 5000\nmaxUploadSize = 10000\nmaxMessageSize = 10000\nenableXsrfProtection = false","path":null,"size_bytes":130,"size_tokens":null},"page_modules/profile_page.py":{"content":"import streamlit as st\nfrom database import get_db_session\nfrom models import User\n\ndef render_profile_page(current_user):\n    \"\"\"Render user profile page for viewing/editing profile information\"\"\"\n    \n    st.title(\"ðŸ‘¤ Profile\")\n    st.markdown(\"### Manage Your Personal Information\")\n    \n    # current_user is already a dictionary with all user data\n    # Profile editing form\n    st.markdown(\"#### Basic Information\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        full_name = st.text_input(\"Full Name\", value=current_user.get('full_name') or \"\", placeholder=\"John Doe\")\n        email = st.text_input(\"Email Address\", value=current_user.get('email') or \"\", placeholder=\"user@example.com\")\n        st.caption(\"You can change your email address\")\n    \n    with col2:\n        company = st.text_input(\"Company\", value=current_user.get('company') or \"\", placeholder=\"Mining Corp Inc.\")\n        role = st.text_input(\"Role / Position\", value=current_user.get('role') or \"\", placeholder=\"Senior Geologist\")\n    \n    phone = st.text_input(\"Phone Number\", value=current_user.get('phone') or \"\", placeholder=\"+1 (555) 123-4567\")\n    \n    st.markdown(\"---\")\n    \n    # Save button\n    col_save, col_cancel = st.columns([1, 4])\n    \n    with col_save:\n        if st.button(\"ðŸ’¾ Save Changes\", type=\"primary\", use_container_width=True):\n            with get_db_session() as db:\n                # Check if email is being changed and if new email already exists\n                email_changed = email != current_user.get('email')\n                if email_changed:\n                    existing_user = db.query(User).filter(User.email == email, User.id != current_user['id']).first()\n                    if existing_user:\n                        st.error(\"âŒ This email address is already in use by another account.\")\n                        st.stop()\n                \n                user_to_update = db.query(User).filter(User.id == current_user['id']).first()\n                user_to_update.full_name = full_name\n                user_to_update.email = email\n                user_to_update.company = company\n                user_to_update.role = role\n                user_to_update.phone = phone\n                db.commit()\n                \n                # Update session state with all fields\n                st.session_state.current_user = {\n                    'id': user_to_update.id,\n                    'email': user_to_update.email,\n                    'username': user_to_update.username,\n                    'created_at': user_to_update.created_at,\n                    'last_login': user_to_update.last_login,\n                    'password_hash': user_to_update.password_hash,\n                    'full_name': user_to_update.full_name,\n                    'company': user_to_update.company,\n                    'role': user_to_update.role,\n                    'phone': user_to_update.phone,\n                    'avatar_url': user_to_update.avatar_url,\n                    'api_key': user_to_update.api_key,\n                    'mfa_enabled': user_to_update.mfa_enabled,\n                    'mfa_secret': user_to_update.mfa_secret,\n                    'theme': user_to_update.theme,\n                    'notifications_enabled': user_to_update.notifications_enabled,\n                    'ai_behavior_settings': user_to_update.ai_behavior_settings,\n                    'plan_type': user_to_update.plan_type,\n                    'usage_count': user_to_update.usage_count,\n                    'usage_limit': user_to_update.usage_limit,\n                    'billing_status': user_to_update.billing_status\n                }\n                st.session_state.user_email = email\n            \n            st.success(\"âœ… Profile updated successfully!\")\n            st.rerun()\n    \n    # Account Information\n    st.markdown(\"---\")\n    st.markdown(\"#### Account Information\")\n    \n    info_col1, info_col2 = st.columns(2)\n    \n    with info_col1:\n        st.markdown(f\"\"\"\n        <div style=\"background: #F8F9FB; padding: 1rem; border-radius: 8px; margin-bottom: 1rem;\">\n            <div style=\"font-size: 0.875rem; color: #64748B; margin-bottom: 0.25rem;\">Username</div>\n            <div style=\"font-weight: 600; color: #0F172A;\">{current_user.get('username', 'N/A')}</div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n        \n        created_at = current_user.get('created_at')\n        created_at_str = created_at.strftime('%B %d, %Y') if hasattr(created_at, 'strftime') else 'Recently'\n        st.markdown(f\"\"\"\n        <div style=\"background: #F8F9FB; padding: 1rem; border-radius: 8px;\">\n            <div style=\"font-size: 0.875rem; color: #64748B; margin-bottom: 0.25rem;\">Member Since</div>\n            <div style=\"font-weight: 600; color: #0F172A;\">{created_at_str}</div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    with info_col2:\n        plan_type = current_user.get('plan_type', 'free')\n        st.markdown(f\"\"\"\n        <div style=\"background: #F8F9FB; padding: 1rem; border-radius: 8px; margin-bottom: 1rem;\">\n            <div style=\"font-size: 0.875rem; color: #64748B; margin-bottom: 0.25rem;\">Account Type</div>\n            <div style=\"font-weight: 600; color: #0F172A;\">{plan_type.upper()}</div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n        \n        last_login = current_user.get('last_login')\n        last_login_str = last_login.strftime('%B %d, %Y %H:%M') if last_login and hasattr(last_login, 'strftime') else 'N/A'\n        st.markdown(f\"\"\"\n        <div style=\"background: #F8F9FB; padding: 1rem; border-radius: 8px;\">\n            <div style=\"font-size: 0.875rem; color: #64748B; margin-bottom: 0.25rem;\">Last Login</div>\n            <div style=\"font-weight: 600; color: #0F172A;\">{last_login_str}</div>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n","path":null,"size_bytes":5820,"size_tokens":null},"migrate_app.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nMigration script to integrate navigation into app.py\n\"\"\"\n\n# Read the backup file\nwith open('app_backup.py', 'r') as f:\n    lines = f.readlines()\n\n# New imports to add after the existing imports\nnew_imports = \"\"\"\n# Import page modules\nfrom pages.dashboard_page import render_dashboard\nfrom pages.projects_page import render_projects_page\nfrom pages.reports_page import render_reports_page\nfrom pages.profile_page import render_profile_page\nfrom pages.account_settings_page import render_account_settings_page\nfrom pages.app_settings_page import render_app_settings_page\nfrom pages.billing_page import render_billing_page\nfrom pages.team_page import render_team_page\nfrom components.navigation import render_top_navigation, render_user_menu_dropdown\n\"\"\"\n\n# Find where to insert navigation setup\nauth_line_idx = None\nfor i, line in enumerate(lines):\n    if line.strip().startswith('current_user = require_auth()'):\n        auth_line_idx = i\n        break\n\n# Find where CSS ends\ncss_end_idx = None\nfor i, line in enumerate(lines):\n    if '\"\", unsafe_allow_html=True)' in line and i > 100:\n        css_end_idx = i\n        break\n\n# Build the new file\noutput_lines = []\n\n# Add lines up to and including TemplateManager import\nfor i, line in enumerate(lines):\n    output_lines.append(line)\n    if 'from template_manager import TemplateManager' in line:\n        output_lines.append(new_imports)\n        break\n\n# Continue with the rest until auth line\nstart_idx = len(output_lines)\nfor i in range(start_idx, auth_line_idx + 1):\n    output_lines.append(lines[i])\n\n# Add navigation setup\noutput_lines.append('\\n')\noutput_lines.append('# Initialize current_page in session state if not set\\n')\noutput_lines.append(\"if 'current_page' not in st.session_state:\\n\")\noutput_lines.append(\"    st.session_state.current_page = 'dashboard'\\n\")\noutput_lines.append('\\n')\noutput_lines.append('# Render top navigation and get current page\\n')\noutput_lines.append('current_page = render_top_navigation()\\n')\noutput_lines.append('\\n')\noutput_lines.append('# Render user menu in sidebar\\n')\noutput_lines.append('render_user_menu_dropdown(current_user)\\n')\noutput_lines.append('\\n')\n\n# Add lines from after auth to CSS end\nfor i in range(auth_line_idx + 1, css_end_idx + 1):\n    if 'render_user_info()' not in lines[i]:  # Skip old user info call\n        output_lines.append(lines[i])\n\n# Add page routing\noutput_lines.append('\\n')\noutput_lines.append('# Page Routing Logic\\n')\noutput_lines.append(\"if current_page == 'dashboard':\\n\")\noutput_lines.append('    render_dashboard(current_user)\\n')\noutput_lines.append('\\n')\noutput_lines.append(\"elif current_page == 'projects':\\n\")\noutput_lines.append('    render_projects_page(current_user)\\n')\noutput_lines.append('\\n')\noutput_lines.append(\"elif current_page == 'reports':\\n\")\noutput_lines.append('    render_reports_page(current_user)\\n')\noutput_lines.append('\\n')\noutput_lines.append(\"elif current_page == 'profile':\\n\")\noutput_lines.append('    render_profile_page(current_user)\\n')\noutput_lines.append('\\n')\noutput_lines.append(\"elif current_page == 'account_settings':\\n\")\noutput_lines.append('    render_account_settings_page(current_user)\\n')\noutput_lines.append('\\n')\noutput_lines.append(\"elif current_page == 'app_settings':\\n\")\noutput_lines.append('    render_app_settings_page(current_user)\\n')\noutput_lines.append('\\n')\noutput_lines.append(\"elif current_page == 'billing':\\n\")\noutput_lines.append('    render_billing_page(current_user)\\n')\noutput_lines.append('\\n')\noutput_lines.append(\"elif current_page == 'team':\\n\")\noutput_lines.append('    render_team_page(current_user)\\n')\noutput_lines.append('\\n')\noutput_lines.append(\"elif current_page == 'ai_agent':\\n\")\noutput_lines.append('    # AI Agent page - existing functionality\\n')\n\n# Add all remaining lines with proper indentation (4 spaces)\nfor i in range(css_end_idx + 1, len(lines)):\n    line = lines[i]\n    # Add 4 spaces of indentation for all lines in AI agent\n    if line.strip():  # Non-empty lines\n        output_lines.append('    ' + line)\n    else:  # Empty lines\n        output_lines.append(line)\n\n# Write the new file\nwith open('app.py', 'w') as f:\n    f.writelines(output_lines)\n\nprint(\"Migration completed successfully!\")\nprint(f\"Total lines: {len(output_lines)}\")\n","path":null,"size_bytes":4292,"size_tokens":null},"page_modules/account_settings_page.py":{"content":"import streamlit as st\nfrom database import get_db_session\nfrom models import User\nimport secrets\n\ndef render_account_settings_page(current_user):\n    \"\"\"Render account settings page for email, password, MFA, API key management\"\"\"\n    \n    st.title(\"âš™ï¸ Account Settings\")\n    st.markdown(\"### Manage Security and Access\")\n    \n    # current_user is already a dictionary with all user data\n    # Security Settings\n    st.markdown(\"#### ðŸ” Security Settings\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.markdown(\"**Email Address**\")\n        st.text_input(\"Current Email\", value=current_user.get('email', ''), disabled=True)\n        st.caption(\"Contact support to change your email address\")\n        \n        st.markdown(\"<br>\", unsafe_allow_html=True)\n        \n        st.markdown(\"**Password**\")\n        if st.button(\"ðŸ”‘ Change Password\", use_container_width=True):\n            st.info(\"Password change functionality coming soon!\")\n    \n    with col2:\n        st.markdown(\"**Multi-Factor Authentication (MFA)**\")\n        mfa_enabled = st.checkbox(\"Enable MFA\", value=current_user.get('mfa_enabled') or False)\n        \n        if mfa_enabled != current_user.get('mfa_enabled'):\n            with get_db_session() as db:\n                user_to_update = db.query(User).filter(User.id == current_user['id']).first()\n                user_to_update.mfa_enabled = mfa_enabled\n                if mfa_enabled and not user_to_update.mfa_secret:\n                    user_to_update.mfa_secret = secrets.token_urlsafe(32)\n                db.commit()\n            \n            st.success(f\"âœ… MFA {'enabled' if mfa_enabled else 'disabled'} successfully!\")\n            st.rerun()\n        \n        if mfa_enabled:\n            st.success(\"âœ… MFA is currently enabled\")\n        else:\n            st.warning(\"âš ï¸ Enable MFA for enhanced security\")\n    \n    st.markdown(\"---\")\n    \n    # API Key Management\n    st.markdown(\"#### ðŸ”‘ API Key Management\")\n    st.caption(\"Use API keys to integrate Oreplot with other applications\")\n    \n    if current_user.get('api_key'):\n        col_key, col_actions = st.columns([3, 1])\n        \n        with col_key:\n            st.text_input(\"Your API Key\", value=current_user.get('api_key', ''), type=\"password\", disabled=True)\n        \n        with col_actions:\n            if st.button(\"ðŸ—‘ï¸ Revoke\", use_container_width=True):\n                with get_db_session() as db:\n                    user_to_update = db.query(User).filter(User.id == current_user['id']).first()\n                    user_to_update.api_key = None\n                    db.commit()\n                st.success(\"API key revoked!\")\n                st.rerun()\n    else:\n        if st.button(\"âž• Generate New API Key\", type=\"primary\"):\n            new_api_key = f\"ore_{secrets.token_urlsafe(32)}\"\n            \n            with get_db_session() as db:\n                user_to_update = db.query(User).filter(User.id == current_user['id']).first()\n                user_to_update.api_key = new_api_key\n                db.commit()\n            \n            st.success(f\"âœ… New API key generated: `{new_api_key}`\")\n            st.warning(\"âš ï¸ Copy this key now. You won't be able to see it again!\")\n            st.rerun()\n    \n    st.markdown(\"---\")\n    \n    # Session Management\n    st.markdown(\"#### ðŸ”’ Session Management\")\n    \n    col_session1, col_session2 = st.columns(2)\n    \n    with col_session1:\n        st.markdown(\"**Active Sessions**\")\n        st.info(\"1 active session (this device)\")\n    \n    with col_session2:\n        if st.button(\"ðŸšª Sign Out All Devices\", use_container_width=True):\n            st.info(\"Sign out from all devices functionality coming soon!\")\n    \n    st.markdown(\"---\")\n    \n    # Danger Zone\n    st.markdown(\"#### âš ï¸ Danger Zone\")\n    \n    with st.expander(\"Delete Account\"):\n        st.warning(\"âš ï¸ **Warning:** This action is irreversible. All your projects and data will be permanently deleted.\")\n        st.text_input(\"Type 'DELETE' to confirm\", key=\"delete_confirm\")\n        \n        if st.button(\"ðŸ—‘ï¸ Permanently Delete Account\", type=\"secondary\"):\n            if st.session_state.get('delete_confirm') == 'DELETE':\n                st.error(\"Account deletion is not yet implemented. Please contact support.\")\n            else:\n                st.error(\"Please type 'DELETE' to confirm\")\n","path":null,"size_bytes":4367,"size_tokens":null},"create_admin.py":{"content":"import bcrypt\nfrom database import get_db_session\nfrom models import User\nfrom datetime import datetime\n\ndef hash_password(password):\n    \"\"\"Secure password hashing using bcrypt with salt\"\"\"\n    salt = bcrypt.gensalt()\n    return bcrypt.hashpw(password.encode(), salt).decode()\n\ndef create_admin_user(email, password):\n    \"\"\"Create an admin user with the specified credentials\"\"\"\n    with get_db_session() as db:\n        existing_user = db.query(User).filter(User.email == email).first()\n        \n        if existing_user:\n            existing_user.password_hash = hash_password(password)\n            existing_user.is_admin = True\n            existing_user.username = email.split('@')[0]\n            existing_user.last_login = datetime.utcnow()\n            print(f\"Updated existing user {email} to admin\")\n        else:\n            admin_user = User(\n                email=email,\n                username=email.split('@')[0],\n                password_hash=hash_password(password),\n                is_admin=True,\n                created_at=datetime.utcnow(),\n                last_login=datetime.utcnow(),\n                plan_type='enterprise',\n                usage_limit=999999\n            )\n            db.add(admin_user)\n            print(f\"Created new admin user: {email}\")\n        \n        db.commit()\n        print(\"Admin user created successfully!\")\n\nif __name__ == \"__main__\":\n    create_admin_user(\"cokhaligzada@gmail.com\", \"A12345+xz\")\n","path":null,"size_bytes":1447,"size_tokens":null},"page_modules/login_page.py":{"content":"import streamlit as st\nimport bcrypt\nfrom database import get_db_session\nfrom models import User\nfrom datetime import datetime\n\ndef verify_password(password, hashed_password):\n    \"\"\"Verify password against bcrypt hash\"\"\"\n    try:\n        return bcrypt.checkpw(password.encode(), hashed_password.encode())\n    except:\n        return False\n\ndef render_login_page():\n    \"\"\"Render the login page with email/password authentication\"\"\"\n    \n    st.markdown(\"\"\"\n    <style>\n        .login-container {\n            max-width: 400px;\n            margin: 4rem auto;\n            padding: 2rem;\n            background: white;\n            border-radius: 12px;\n            box-shadow: 0 4px 12px rgba(0,0,0,0.1);\n        }\n        .login-header {\n            text-align: center;\n            margin-bottom: 2rem;\n        }\n        .login-logo {\n            max-width: 60px;\n            margin: 0 auto 0.5rem;\n        }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n    \n    col1, col2, col3 = st.columns([1, 2, 1])\n    \n    with col2:\n        st.markdown(\"## Welcome to Oreplot\")\n        st.markdown(\"### Mining Due Diligence Platform\")\n        st.markdown(\"---\")\n        \n        email = st.text_input(\"ðŸ“§ Email Address\", placeholder=\"Enter your email\")\n        password = st.text_input(\"ðŸ”’ Password\", type=\"password\", placeholder=\"Enter your password\")\n        \n        col_login, col_space = st.columns([1, 1])\n        \n        with col_login:\n            if st.button(\"ðŸ” Sign In\", use_container_width=True, type=\"primary\"):\n                if not email or not password:\n                    st.error(\"Please enter both email and password\")\n                else:\n                    with get_db_session() as db:\n                        user = db.query(User).filter(User.email == email).first()\n                        \n                        if user and user.password_hash and verify_password(password, user.password_hash):\n                            # Update last login\n                            user.last_login = datetime.utcnow()\n                            db.commit()\n                            \n                            # Store user in session state\n                            st.session_state.current_user = {\n                                'id': user.id,\n                                'email': user.email,\n                                'username': user.username,\n                                'created_at': user.created_at,\n                                'last_login': user.last_login,\n                                'password_hash': user.password_hash,\n                                'is_admin': user.is_admin,\n                                'full_name': user.full_name,\n                                'company': user.company,\n                                'role': user.role,\n                                'phone': user.phone,\n                                'avatar_url': user.avatar_url,\n                                'api_key': user.api_key,\n                                'mfa_enabled': user.mfa_enabled,\n                                'mfa_secret': user.mfa_secret,\n                                'theme': user.theme,\n                                'notifications_enabled': user.notifications_enabled,\n                                'ai_behavior_settings': user.ai_behavior_settings,\n                                'plan_type': user.plan_type,\n                                'usage_count': user.usage_count,\n                                'usage_limit': user.usage_limit,\n                                'billing_status': user.billing_status\n                            }\n                            st.session_state.user_id = user.id\n                            st.session_state.user_email = user.email\n                            st.session_state.username = user.username\n                            st.session_state.authenticated = True\n                            \n                            # Session persistence enabled - user stays logged in until manual logout\n                            \n                            st.success(\"âœ… Login successful! Redirecting...\")\n                            st.rerun()\n                        else:\n                            st.error(\"âŒ Invalid email or password\")\n        \n        st.markdown(\"---\")\n        st.markdown(\"<small>Don't have an account? Contact your administrator.</small>\", unsafe_allow_html=True)\n        st.markdown(\"<small>Â©2025 Copyright Oreplot. All rights reserved.</small>\", unsafe_allow_html=True)\n","path":null,"size_bytes":4519,"size_tokens":null},"create_customer.py":{"content":"import bcrypt\nfrom database import get_db_session\nfrom models import User\nfrom datetime import datetime\n\ndef hash_password(password):\n    \"\"\"Secure password hashing using bcrypt with salt\"\"\"\n    salt = bcrypt.gensalt()\n    return bcrypt.hashpw(password.encode(), salt).decode()\n\ndef create_customer_user(email, password):\n    \"\"\"Create a customer user with the specified credentials\"\"\"\n    with get_db_session() as db:\n        existing_user = db.query(User).filter(User.email == email).first()\n        \n        if existing_user:\n            existing_user.password_hash = hash_password(password)\n            existing_user.is_admin = False\n            existing_user.username = email.split('@')[0]\n            existing_user.last_login = datetime.utcnow()\n            print(f\"Updated existing user {email} to customer\")\n        else:\n            customer_user = User(\n                email=email,\n                username=email.split('@')[0],\n                password_hash=hash_password(password),\n                is_admin=False,\n                created_at=datetime.utcnow(),\n                last_login=datetime.utcnow(),\n                plan_type='free',\n                usage_limit=10\n            )\n            db.add(customer_user)\n            print(f\"Created new customer user: {email}\")\n        \n        db.commit()\n        print(\"Customer user created successfully!\")\n\nif __name__ == \"__main__\":\n    create_customer_user(\"tacs8419@gmail.com\", \"ABCD12345xz\")\n","path":null,"size_bytes":1460,"size_tokens":null},"page_modules/admin_panel_page.py":{"content":"import streamlit as st\nimport bcrypt\nfrom database import get_db_session\nfrom models import User, Project, Analysis\nfrom datetime import datetime\nfrom sqlalchemy import func\n\ndef hash_password(password):\n    \"\"\"Secure password hashing using bcrypt with salt\"\"\"\n    salt = bcrypt.gensalt()\n    return bcrypt.hashpw(password.encode(), salt).decode()\n\ndef render_admin_panel_page(current_user):\n    \"\"\"Render the admin panel with full platform control\"\"\"\n    \n    if not current_user.get('is_admin'):\n        st.error(\"Access Denied: You must be an administrator to access this page.\")\n        st.stop()\n    \n    st.title(\"Admin Panel\")\n    st.markdown(\"### Platform Administration & Management\")\n    \n    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([\n        \"User Management\",\n        \"AI Access Control\",\n        \"AI Training\",\n        \"Platform Statistics\",\n        \"Cache & Maintenance\",\n        \"Compute Usage\",\n        \"UI Customization\"\n    ])\n    \n    # Tab 1: User Management\n    with tab1:\n        st.markdown(\"#### Create New User Account\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            new_email = st.text_input(\"Email\", placeholder=\"user@example.com\")\n            new_username = st.text_input(\"Username\", placeholder=\"johndoe\")\n            new_full_name = st.text_input(\"Full Name\", placeholder=\"John Doe\")\n        \n        with col2:\n            new_password = st.text_input(\"Password\", type=\"password\", placeholder=\"Enter secure password\")\n            new_is_admin = st.checkbox(\"Administrator Account\")\n            new_plan_type = st.selectbox(\"Plan Type\", [\"free\", \"starter\", \"professional\", \"enterprise\"])\n        \n        if st.button(\"âž• Create User Account\", type=\"primary\"):\n            if new_email and new_username and new_password:\n                with get_db_session() as db:\n                    existing = db.query(User).filter(\n                        (User.email == new_email) | (User.username == new_username)\n                    ).first()\n                    \n                    if existing:\n                        st.error(\"âŒ Email or username already exists\")\n                    else:\n                        new_user = User(\n                            email=new_email,\n                            username=new_username,\n                            full_name=new_full_name,\n                            password_hash=hash_password(new_password),\n                            is_admin=new_is_admin,\n                            plan_type=new_plan_type,\n                            created_at=datetime.utcnow(),\n                            usage_limit=10 if new_plan_type == 'free' else 999999\n                        )\n                        db.add(new_user)\n                        db.commit()\n                        st.success(f\"âœ… User account created: {new_email}\")\n                        st.rerun()\n            else:\n                st.error(\"âŒ Please fill in all required fields\")\n        \n        st.markdown(\"---\")\n        st.markdown(\"#### All Users\")\n        \n        with get_db_session() as db:\n            users = db.query(User).order_by(User.created_at.desc()).all()\n            \n            # Convert to list of dicts to avoid session issues\n            users_data = []\n            for u in users:\n                users_data.append({\n                    'id': u.id,\n                    'email': u.email,\n                    'username': u.username,\n                    'full_name': u.full_name,\n                    'is_admin': u.is_admin,\n                    'plan_type': u.plan_type,\n                    'usage_count': u.usage_count,\n                    'usage_limit': u.usage_limit,\n                    'created_at': u.created_at,\n                    'last_login': u.last_login\n                })\n        \n        for user_data in users_data:\n            with st.expander(f\"ðŸ‘¤ {user_data['email']} {'(Admin)' if user_data['is_admin'] else ''}\"):\n                info_col1, info_col2 = st.columns(2)\n                \n                with info_col1:\n                    st.markdown(f\"**Username:** {user_data['username']}\")\n                    st.markdown(f\"**Full Name:** {user_data['full_name'] or 'Not set'}\")\n                    st.markdown(f\"**Plan Type:** {user_data['plan_type'].upper()}\")\n                    st.markdown(f\"**Admin:** {'Yes' if user_data['is_admin'] else 'No'}\")\n                \n                with info_col2:\n                    created_str = user_data['created_at'].strftime('%Y-%m-%d %H:%M') if user_data['created_at'] else 'Unknown'\n                    last_login_str = user_data['last_login'].strftime('%Y-%m-%d %H:%M') if user_data['last_login'] else 'Never'\n                    st.markdown(f\"**Created:** {created_str}\")\n                    st.markdown(f\"**Last Login:** {last_login_str}\")\n                    st.markdown(f\"**Usage:** {user_data['usage_count']} / {user_data['usage_limit']}\")\n                    \n                    if st.button(f\"ðŸ—‘ï¸ Delete User\", key=f\"delete_{user_data['id']}\"):\n                        if user_data['id'] != current_user['id']:\n                            with get_db_session() as db:\n                                user_to_delete = db.query(User).filter(User.id == user_data['id']).first()\n                                if user_to_delete:\n                                    db.delete(user_to_delete)\n                                    db.commit()\n                                    st.success(\"User deleted\")\n                                    st.rerun()\n                        else:\n                            st.error(\"Cannot delete your own account\")\n    \n    # Tab 2: AI Access Control\n    with tab2:\n        st.markdown(\"#### ðŸ¤– AI Tier Access Management\")\n        st.markdown(\"Control which AI features users can access. **Oreplot Light** includes standard analysis, while **Oreplot Advanced** includes PwC-style valuation, market multiples, Kilburn method, and Monte Carlo simulations.\")\n        \n        st.markdown(\"---\")\n        \n        st.markdown(\"##### AI Access Tiers\")\n        tier_col1, tier_col2, tier_col3 = st.columns(3)\n        \n        with tier_col1:\n            st.markdown(\"\"\"\n            **ðŸ”µ Oreplot Light**\n            - Document analysis\n            - Dual scoring (Investment + Sustainability)\n            - PDF report generation\n            - Comparables matching\n            - Template management\n            \"\"\")\n        \n        with tier_col2:\n            st.markdown(\"\"\"\n            **ðŸŸ£ Oreplot Advanced**\n            - Market Multiples Analysis\n            - PwC Cost Approach (Kilburn)\n            - Monte Carlo Risk Modeling\n            - EV/Resource Benchmarking\n            - Financial Valuation Reports\n            \"\"\")\n        \n        with tier_col3:\n            st.markdown(\"\"\"\n            **ðŸŸ¢ Both**\n            - Full access to Oreplot Light\n            - Full access to Oreplot Advanced\n            - All features unlocked\n            - Admin default setting\n            \"\"\")\n        \n        st.markdown(\"---\")\n        st.markdown(\"#### User AI Access Settings\")\n        \n        with get_db_session() as db:\n            all_users = db.query(User).order_by(User.email).all()\n            \n            users_ai_data = []\n            for u in all_users:\n                users_ai_data.append({\n                    'id': u.id,\n                    'email': u.email,\n                    'username': u.username,\n                    'is_admin': u.is_admin,\n                    'plan_type': u.plan_type,\n                    'ai_tier_access': getattr(u, 'ai_tier_access', 'light_ai') or 'light_ai'\n                })\n        \n        with get_db_session() as db:\n            light_ai_count = db.query(func.count(User.id)).filter(User.ai_tier_access == 'light_ai').scalar() or 0\n            advanced_ai_count = db.query(func.count(User.id)).filter(User.ai_tier_access == 'advanced_ai').scalar() or 0\n            both_count = db.query(func.count(User.id)).filter(User.ai_tier_access == 'both').scalar() or 0\n        \n        stat_col1, stat_col2, stat_col3 = st.columns(3)\n        with stat_col1:\n            st.metric(\"Oreplot Light Users\", light_ai_count)\n        with stat_col2:\n            st.metric(\"Oreplot Advanced Users\", advanced_ai_count)\n        with stat_col3:\n            st.metric(\"Both Access Users\", both_count)\n        \n        st.markdown(\"---\")\n        \n        for user_ai in users_ai_data:\n            with st.expander(f\"{'ðŸ‘‘ ' if user_ai['is_admin'] else 'ðŸ‘¤ '}{user_ai['email']} - Current: **{user_ai['ai_tier_access'].upper().replace('_', ' ')}**\"):\n                col_info, col_action = st.columns([2, 1])\n                \n                with col_info:\n                    st.markdown(f\"**Username:** {user_ai['username']}\")\n                    st.markdown(f\"**Plan Type:** {user_ai['plan_type'].upper()}\")\n                    st.markdown(f\"**Admin:** {'Yes' if user_ai['is_admin'] else 'No'}\")\n                \n                with col_action:\n                    current_tier = user_ai['ai_tier_access']\n                    tier_options = ['light_ai', 'advanced_ai', 'both']\n                    tier_labels = ['ðŸ”µ Oreplot Light Only', 'ðŸŸ£ Oreplot Advanced Only', 'ðŸŸ¢ Both (Full Access)']\n                    \n                    current_index = tier_options.index(current_tier) if current_tier in tier_options else 0\n                    \n                    new_tier = st.selectbox(\n                        \"AI Access Level\",\n                        options=tier_options,\n                        format_func=lambda x: tier_labels[tier_options.index(x)],\n                        index=current_index,\n                        key=f\"ai_tier_{user_ai['id']}\"\n                    )\n                    \n                    if st.button(\"ðŸ’¾ Update Access\", key=f\"update_ai_{user_ai['id']}\"):\n                        with get_db_session() as db:\n                            user_to_update = db.query(User).filter(User.id == user_ai['id']).first()\n                            if user_to_update:\n                                user_to_update.ai_tier_access = new_tier\n                                db.commit()\n                                st.success(f\"âœ… Updated {user_ai['email']} to {new_tier.upper().replace('_', ' ')}\")\n                                st.rerun()\n        \n        st.markdown(\"---\")\n        st.markdown(\"#### Bulk AI Access Update\")\n        \n        bulk_col1, bulk_col2 = st.columns(2)\n        \n        with bulk_col1:\n            bulk_tier = st.selectbox(\n                \"Set all non-admin users to:\",\n                options=['light_ai', 'advanced_ai', 'both'],\n                format_func=lambda x: {'light_ai': 'ðŸ”µ Light AI Only', 'advanced_ai': 'ðŸŸ£ Advanced AI Only', 'both': 'ðŸŸ¢ Both (Full Access)'}[x],\n                key=\"bulk_ai_tier\"\n            )\n        \n        with bulk_col2:\n            st.markdown(\"\")\n            st.markdown(\"\")\n            if st.button(\"âš¡ Apply to All Non-Admin Users\", type=\"secondary\"):\n                with get_db_session() as db:\n                    result = db.query(User).filter(User.is_admin == False).update({'ai_tier_access': bulk_tier})\n                    db.commit()\n                    st.success(f\"Updated {result} non-admin users to {bulk_tier.upper().replace('_', ' ')}\")\n                    st.rerun()\n    \n    # Tab 3: AI Training\n    with tab3:\n        from page_modules.ai_training_page import render_ai_training_page\n        render_ai_training_page(current_user)\n    \n    # Tab 4: Platform Statistics\n    with tab4:\n        with get_db_session() as db:\n            total_users = db.query(func.count(User.id)).scalar()\n            total_projects = db.query(func.count(Project.id)).scalar()\n            total_analyses = db.query(func.count(Analysis.id)).scalar()\n            \n            admin_count = db.query(func.count(User.id)).filter(User.is_admin == True).scalar()\n            \n            # Get users by plan type\n            free_users = db.query(func.count(User.id)).filter(User.plan_type == 'free').scalar()\n            paid_users = total_users - free_users\n        \n        st.markdown(\"#### Platform Overview\")\n        \n        col1, col2, col3, col4 = st.columns(4)\n        \n        with col1:\n            st.metric(\"Total Users\", total_users)\n        with col2:\n            st.metric(\"Total Projects\", total_projects)\n        with col3:\n            st.metric(\"Total Analyses\", total_analyses)\n        with col4:\n            st.metric(\"Admin Users\", admin_count)\n        \n        st.markdown(\"---\")\n        \n        col5, col6, col7, col8 = st.columns(4)\n        \n        with col5:\n            st.metric(\"Free Plan Users\", free_users)\n        with col6:\n            st.metric(\"Paid Plan Users\", paid_users)\n        with col7:\n            avg_usage = db.query(func.avg(User.usage_count)).scalar() or 0\n            st.metric(\"Avg Usage per User\", f\"{avg_usage:.1f}\")\n        with col8:\n            st.metric(\"Active Today\", \"-\")\n    \n    # Tab 5: Cache & Maintenance\n    with tab5:\n        st.markdown(\"#### System Cache Management\")\n        \n        st.info(\"ðŸ”§ Clear cached data to free up resources and improve performance\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            if st.button(\"ðŸ—‘ï¸ Clear Streamlit Cache\", use_container_width=True):\n                st.cache_data.clear()\n                st.cache_resource.clear()\n                st.success(\"âœ… Streamlit cache cleared successfully\")\n        \n        with col2:\n            if st.button(\"ðŸ”„ Restart Application\", use_container_width=True):\n                st.warning(\"âš ï¸ Application restart requested (requires manual server restart)\")\n        \n        st.markdown(\"---\")\n        st.markdown(\"#### Database Maintenance\")\n        \n        with get_db_session() as db:\n            orphaned_docs = db.query(func.count(Analysis.id)).filter(Analysis.project_id == None).scalar()\n            \n        st.markdown(f\"**Orphaned Analyses:** {orphaned_docs}\")\n        \n        if st.button(\"ðŸ§¹ Clean Orphaned Data\"):\n            st.info(\"Data cleanup completed\")\n    \n    # Tab 6: Compute Usage\n    with tab6:\n        st.markdown(\"#### User Compute Unit Consumption\")\n        \n        with get_db_session() as db:\n            users_usage = db.query(User).order_by(User.usage_count.desc()).limit(20).all()\n            \n            # Materialize data\n            usage_data = []\n            for u in users_usage:\n                usage_data.append({\n                    'email': u.email,\n                    'username': u.username,\n                    'usage_count': u.usage_count,\n                    'usage_limit': u.usage_limit,\n                    'plan_type': u.plan_type\n                })\n        \n        for data in usage_data:\n            progress = min(data['usage_count'] / data['usage_limit'], 1.0) if data['usage_limit'] > 0 else 0\n            \n            st.markdown(f\"**{data['email']}** ({data['plan_type']})\")\n            st.progress(progress)\n            st.markdown(f\"{data['usage_count']} / {data['usage_limit']} analyses\")\n            st.markdown(\"---\")\n    \n    # Tab 7: UI Customization\n    with tab7:\n        st.markdown(\"#### Platform UI Configuration\")\n        \n        st.warning(\"ðŸš§ UI customization features coming soon\")\n        \n        st.markdown(\"**Planned Features:**\")\n        st.markdown(\"- Theme customization (colors, fonts)\")\n        st.markdown(\"- Logo and branding management\")\n        st.markdown(\"- Email template customization\")\n        st.markdown(\"- Dashboard layout configuration\")\n        st.markdown(\"- Custom CSS injection\")\n","path":null,"size_bytes":15648,"size_tokens":null},"page_modules/comparables_page.py":{"content":"import streamlit as st\nfrom database import SessionLocal\nfrom comparables_manager import ComparablesManager\nfrom models import ComparableProject\n\ndef render_comparables_page():\n    \"\"\"Render the Global Comparables Database page\"\"\"\n    st.markdown(\"<h1 style='background: linear-gradient(135deg, #3B82F6 0%, #8B5CF6 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-size: 2.5rem; font-weight: 700; margin-bottom: 1.5rem;'>ðŸŒ Global Comparables Database</h1>\", unsafe_allow_html=True)\n    st.markdown(\"<p style='color: #64748B; font-size: 1.1rem; margin-bottom: 2rem;'>Benchmark your mining projects against real-world analogues</p>\", unsafe_allow_html=True)\n    \n    db = SessionLocal()\n    \n    try:\n        tabs = st.tabs([\"ðŸ” Browse Projects\", \"ðŸ“Š Benchmark Statistics\", \"â„¹ï¸ About\"])\n        \n        with tabs[0]:\n            render_browse_comparables(db)\n        \n        with tabs[1]:\n            render_benchmark_stats(db)\n        \n        with tabs[2]:\n            render_about_section()\n    \n    finally:\n        db.close()\n\ndef render_browse_comparables(db):\n    \"\"\"Render the browse comparables interface\"\"\"\n    st.subheader(\"Browse Comparable Mining Projects\")\n    \n    col1, col2, col3 = st.columns([2, 1, 1])\n    \n    with col1:\n        search_term = st.text_input(\"ðŸ” Search projects\", placeholder=\"Project name, company, or location...\")\n    \n    with col2:\n        commodity_filter = st.selectbox(\n            \"Commodity\",\n            [\"All\", \"Copper\", \"Gold\", \"Lithium\", \"Nickel\", \"Zinc\", \"Silver\", \"PGM\"],\n            index=0\n        )\n    \n    with col3:\n        stage_filter = st.selectbox(\n            \"Project Stage\",\n            [\"All\", \"exploration\", \"development\", \"production\"],\n            index=0\n        )\n    \n    col4, col5 = st.columns(2)\n    with col4:\n        min_score = st.slider(\"Minimum Overall Score\", 0.0, 10.0, 0.0, 0.5)\n    with col5:\n        max_score = st.slider(\"Maximum Overall Score\", 0.0, 10.0, 10.0, 0.5)\n    \n    filters = {}\n    if search_term:\n        filters['search'] = search_term\n    if commodity_filter != \"All\":\n        filters['commodity'] = commodity_filter\n    if stage_filter != \"All\":\n        filters['project_stage'] = stage_filter\n    if min_score > 0:\n        filters['min_score'] = min_score\n    if max_score < 10:\n        filters['max_score'] = max_score\n    \n    comparables = ComparablesManager.get_all_comparables(db, filters)\n    \n    st.markdown(f\"**Found {len(comparables)} projects**\")\n    \n    if not comparables:\n        st.info(\"No projects match your search criteria. Try adjusting the filters.\")\n        return\n    \n    for comp in comparables:\n        with st.expander(f\"**{comp.name}** - {comp.commodity or 'N/A'} ({comp.project_stage or 'N/A'})\"):\n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                st.markdown(\"#### Project Details\")\n                st.markdown(f\"**Company:** {comp.company or 'N/A'}\")\n                st.markdown(f\"**Location:** {comp.location}, {comp.country}\" if comp.location and comp.country else f\"{comp.country or comp.location or 'N/A'}\")\n                st.markdown(f\"**Commodity:** {comp.commodity or 'N/A'}\")\n                st.markdown(f\"**Stage:** {comp.project_stage or 'N/A'}\")\n                st.markdown(f\"**Status:** {comp.status or 'N/A'}\")\n            \n            with col2:\n                st.markdown(\"#### Resource & Economics\")\n                if comp.total_resource_mt and comp.grade:\n                    st.markdown(f\"**Resource:** {comp.total_resource_mt:.1f} Mt @ {comp.grade:.2f} {comp.grade_unit or ''}\")\n                if comp.capex_millions_usd:\n                    st.markdown(f\"**CAPEX:** ${comp.capex_millions_usd:.0f}M USD\")\n                if comp.npv_millions_usd:\n                    st.markdown(f\"**NPV:** ${comp.npv_millions_usd:.0f}M USD\")\n                if comp.irr_percent:\n                    st.markdown(f\"**IRR:** {comp.irr_percent:.1f}%\")\n                if comp.mine_life_years:\n                    st.markdown(f\"**Mine Life:** {comp.mine_life_years:.0f} years\")\n            \n            with col3:\n                st.markdown(\"#### Scores\")\n                if comp.overall_score:\n                    st.metric(\"Overall Score\", f\"{comp.overall_score:.1f}/10\")\n                \n                score_cols = st.columns(2)\n                with score_cols[0]:\n                    if comp.geology_score:\n                        st.markdown(f\"**Geology:** {comp.geology_score:.1f}/10\")\n                    if comp.resource_score:\n                        st.markdown(f\"**Resource:** {comp.resource_score:.1f}/10\")\n                    if comp.economics_score:\n                        st.markdown(f\"**Economics:** {comp.economics_score:.1f}/10\")\n                \n                with score_cols[1]:\n                    if comp.legal_score:\n                        st.markdown(f\"**Legal:** {comp.legal_score:.1f}/10\")\n                    if comp.permitting_score:\n                        st.markdown(f\"**Permitting:** {comp.permitting_score:.1f}/10\")\n                    if comp.data_quality_score:\n                        st.markdown(f\"**Data Quality:** {comp.data_quality_score:.1f}/10\")\n            \n            if comp.notes:\n                st.markdown(\"---\")\n                st.markdown(f\"**Notes:** {comp.notes}\")\n            \n            if comp.data_source:\n                st.markdown(f\"**Source:** {comp.data_source}\")\n\ndef render_benchmark_stats(db):\n    \"\"\"Render benchmark statistics\"\"\"\n    st.subheader(\"Benchmark Statistics\")\n    \n    commodity_filter = st.selectbox(\n        \"Filter by Commodity\",\n        [\"All Commodities\", \"Copper\", \"Gold\", \"Lithium\", \"Nickel\", \"Zinc\"],\n        key=\"stats_commodity\"\n    )\n    \n    commodity = None if commodity_filter == \"All Commodities\" else commodity_filter\n    stats = ComparablesManager.get_benchmark_stats(db, commodity)\n    \n    st.markdown(f\"### Statistics for {commodity_filter}\")\n    st.markdown(f\"**Total Projects:** {stats['total_projects']}\")\n    \n    st.markdown(\"### Average Scores\")\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.metric(\"Overall Score\", f\"{stats['avg_overall_score']:.2f}/10\")\n        st.metric(\"Geology Score\", f\"{stats['avg_geology_score']:.2f}/10\")\n    \n    with col2:\n        st.metric(\"Resource Score\", f\"{stats['avg_resource_score']:.2f}/10\")\n        st.metric(\"Economics Score\", f\"{stats['avg_economics_score']:.2f}/10\")\n    \n    with col3:\n        st.metric(\"Legal Score\", f\"{stats['avg_legal_score']:.2f}/10\")\n        st.metric(\"Permitting Score\", f\"{stats['avg_permitting_score']:.2f}/10\")\n    \n    st.markdown(\"### Economic Metrics\")\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        if stats['avg_capex_millions'] > 0:\n            st.metric(\"Avg CAPEX\", f\"${stats['avg_capex_millions']:.0f}M USD\")\n    \n    with col2:\n        if stats['avg_npv_millions'] > 0:\n            st.metric(\"Avg NPV\", f\"${stats['avg_npv_millions']:.0f}M USD\")\n    \n    with col3:\n        if stats['avg_irr_percent'] > 0:\n            st.metric(\"Avg IRR\", f\"{stats['avg_irr_percent']:.1f}%\")\n\ndef render_about_section():\n    \"\"\"Render about section for comparables database\"\"\"\n    st.markdown(\"\"\"\n    ### About the Global Comparables Database\n    \n    The Global Comparables Database contains reference data from real mining projects worldwide, \n    enabling you to benchmark your projects against industry standards and similar assets.\n    \n    #### Data Sources\n    - NI 43-101 Technical Reports\n    - Prefeasibility and Feasibility Studies\n    - Company investor presentations\n    - Public filings and disclosures\n    - Industry databases and reports\n    \n    #### Key Features\n    - **Comprehensive Coverage**: Projects across all commodities and development stages\n    - **Detailed Metrics**: Resource estimates, economics, scores, and risk factors\n    - **Benchmarking**: Compare your projects against similar assets\n    - **Statistical Analysis**: Industry averages and percentile rankings\n    \n    #### Use Cases\n    - Validate project valuations\n    - Assess project competitiveness\n    - Identify peer groups\n    - Support investment decisions\n    - Due diligence reference\n    \n    #### Data Quality\n    All data is sourced from public disclosures and technical reports. Quality ratings \n    indicate reliability of source data (High, Medium, Low).\n    \"\"\")\n","path":null,"size_bytes":8410,"size_tokens":null},"comparables_manager.py":{"content":"from sqlalchemy.orm import Session\nfrom sqlalchemy import and_, or_, func\nfrom models import ComparableProject\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom format_utils import format_currency\n\nclass ComparablesManager:\n    \"\"\"Manager for Global Comparables Database operations\"\"\"\n    \n    @staticmethod\n    def get_all_comparables(db: Session, filters: Dict[str, Any] = None) -> List[ComparableProject]:\n        \"\"\"Get all comparable projects with optional filters\"\"\"\n        query = db.query(ComparableProject)\n        \n        if filters:\n            if filters.get('commodity'):\n                query = query.filter(ComparableProject.commodity.ilike(f\"%{filters['commodity']}%\"))\n            \n            if filters.get('country'):\n                query = query.filter(ComparableProject.country.ilike(f\"%{filters['country']}%\"))\n            \n            if filters.get('project_stage'):\n                query = query.filter(ComparableProject.project_stage == filters['project_stage'])\n            \n            if filters.get('min_score') is not None:\n                query = query.filter(ComparableProject.overall_score >= filters['min_score'])\n            \n            if filters.get('max_score') is not None:\n                query = query.filter(ComparableProject.overall_score <= filters['max_score'])\n            \n            if filters.get('search'):\n                search_term = f\"%{filters['search']}%\"\n                query = query.filter(\n                    or_(\n                        ComparableProject.name.ilike(search_term),\n                        ComparableProject.company.ilike(search_term),\n                        ComparableProject.location.ilike(search_term)\n                    )\n                )\n        \n        return query.order_by(ComparableProject.overall_score.desc()).all()\n    \n    @staticmethod\n    def get_comparable_by_id(db: Session, comparable_id: int) -> Optional[ComparableProject]:\n        \"\"\"Get a specific comparable project by ID\"\"\"\n        return db.query(ComparableProject).filter(ComparableProject.id == comparable_id).first()\n    \n    @staticmethod\n    def get_similar_comparables(db: Session, commodity: str, project_stage: str = None, limit: int = 5) -> List[ComparableProject]:\n        \"\"\"Find similar comparable projects based on commodity and stage\"\"\"\n        query = db.query(ComparableProject).filter(\n            ComparableProject.commodity.ilike(f\"%{commodity}%\")\n        )\n        \n        if project_stage:\n            query = query.filter(ComparableProject.project_stage == project_stage)\n        \n        return query.order_by(ComparableProject.overall_score.desc()).limit(limit).all()\n    \n    @staticmethod\n    def get_benchmark_stats(db: Session, commodity: str = None) -> Dict[str, Any]:\n        \"\"\"Get statistical benchmarks for comparison\"\"\"\n        query = db.query(ComparableProject)\n        \n        if commodity:\n            query = query.filter(ComparableProject.commodity.ilike(f\"%{commodity}%\"))\n        \n        stats = {\n            'total_projects': query.count(),\n            'avg_overall_score': query.with_entities(func.avg(ComparableProject.overall_score)).scalar() or 0,\n            'avg_geology_score': query.with_entities(func.avg(ComparableProject.geology_score)).scalar() or 0,\n            'avg_resource_score': query.with_entities(func.avg(ComparableProject.resource_score)).scalar() or 0,\n            'avg_economics_score': query.with_entities(func.avg(ComparableProject.economics_score)).scalar() or 0,\n            'avg_legal_score': query.with_entities(func.avg(ComparableProject.legal_score)).scalar() or 0,\n            'avg_permitting_score': query.with_entities(func.avg(ComparableProject.permitting_score)).scalar() or 0,\n            'avg_data_quality_score': query.with_entities(func.avg(ComparableProject.data_quality_score)).scalar() or 0,\n            'avg_capex_millions': query.with_entities(func.avg(ComparableProject.capex_millions_usd)).scalar() or 0,\n            'avg_irr_percent': query.with_entities(func.avg(ComparableProject.irr_percent)).scalar() or 0,\n            'avg_npv_millions': query.with_entities(func.avg(ComparableProject.npv_millions_usd)).scalar() or 0,\n        }\n        \n        return stats\n    \n    @staticmethod\n    def compare_project_to_benchmarks(current_analysis: Dict[str, Any], comparables: List[ComparableProject]) -> Dict[str, Any]:\n        \"\"\"Compare a project's scores against comparable projects\"\"\"\n        if not comparables:\n            return {\n                'comparison_available': False,\n                'message': 'No comparable projects found for benchmarking'\n            }\n        \n        comparable_scores = {\n            'overall': [c.overall_score for c in comparables if c.overall_score],\n            'geology': [c.geology_score for c in comparables if c.geology_score],\n            'resource': [c.resource_score for c in comparables if c.resource_score],\n            'economics': [c.economics_score for c in comparables if c.economics_score],\n            'legal': [c.legal_score for c in comparables if c.legal_score],\n            'permitting': [c.permitting_score for c in comparables if c.permitting_score],\n            'data_quality': [c.data_quality_score for c in comparables if c.data_quality_score],\n        }\n        \n        def get_percentile(value, scores_list):\n            if not scores_list or value is None:\n                return None\n            scores_list = sorted(scores_list)\n            count_below = sum(1 for s in scores_list if s < value)\n            return round((count_below / len(scores_list)) * 100, 1)\n        \n        comparison = {\n            'comparison_available': True,\n            'comparables_count': len(comparables),\n            'current_scores': {\n                'overall': current_analysis.get('total_score'),\n                'geology': current_analysis.get('geology_score'),\n                'resource': current_analysis.get('resource_score'),\n                'economics': current_analysis.get('economics_score'),\n                'legal': current_analysis.get('legal_score'),\n                'permitting': current_analysis.get('permitting_score'),\n                'data_quality': current_analysis.get('data_quality_score'),\n            },\n            'percentiles': {\n                'overall': get_percentile(current_analysis.get('total_score'), comparable_scores['overall']),\n                'geology': get_percentile(current_analysis.get('geology_score'), comparable_scores['geology']),\n                'resource': get_percentile(current_analysis.get('resource_score'), comparable_scores['resource']),\n                'economics': get_percentile(current_analysis.get('economics_score'), comparable_scores['economics']),\n                'legal': get_percentile(current_analysis.get('legal_score'), comparable_scores['legal']),\n                'permitting': get_percentile(current_analysis.get('permitting_score'), comparable_scores['permitting']),\n                'data_quality': get_percentile(current_analysis.get('data_quality_score'), comparable_scores['data_quality']),\n            },\n            'benchmarks': {\n                'overall_avg': sum(comparable_scores['overall']) / len(comparable_scores['overall']) if comparable_scores['overall'] else 0,\n                'geology_avg': sum(comparable_scores['geology']) / len(comparable_scores['geology']) if comparable_scores['geology'] else 0,\n                'resource_avg': sum(comparable_scores['resource']) / len(comparable_scores['resource']) if comparable_scores['resource'] else 0,\n                'economics_avg': sum(comparable_scores['economics']) / len(comparable_scores['economics']) if comparable_scores['economics'] else 0,\n                'legal_avg': sum(comparable_scores['legal']) / len(comparable_scores['legal']) if comparable_scores['legal'] else 0,\n                'permitting_avg': sum(comparable_scores['permitting']) / len(comparable_scores['permitting']) if comparable_scores['permitting'] else 0,\n                'data_quality_avg': sum(comparable_scores['data_quality']) / len(comparable_scores['data_quality']) if comparable_scores['data_quality'] else 0,\n            }\n        }\n        \n        return comparison\n    \n    @staticmethod\n    def format_comparable_for_display(comparable: ComparableProject) -> Dict[str, Any]:\n        \"\"\"Format a comparable project for UI display\"\"\"\n        return {\n            'id': comparable.id,\n            'name': comparable.name,\n            'company': comparable.company,\n            'location': f\"{comparable.location}, {comparable.country}\" if comparable.location and comparable.country else comparable.country or comparable.location or 'Unknown',\n            'commodity': comparable.commodity or 'N/A',\n            'stage': comparable.project_stage or 'N/A',\n            'resource': f\"{comparable.total_resource_mt:.1f} Mt @ {comparable.grade:.2f} {comparable.grade_unit}\" if comparable.total_resource_mt and comparable.grade else 'N/A',\n            'capex': format_currency(comparable.capex_millions_usd, decimals=0) if comparable.capex_millions_usd else 'N/A',\n            'npv': format_currency(comparable.npv_millions_usd, decimals=0) if comparable.npv_millions_usd else 'N/A',\n            'irr': f\"{comparable.irr_percent:.1f}%\" if comparable.irr_percent else 'N/A',\n            'overall_score': comparable.overall_score or 0,\n            'scores': {\n                'geology': comparable.geology_score or 0,\n                'resource': comparable.resource_score or 0,\n                'economics': comparable.economics_score or 0,\n                'legal': comparable.legal_score or 0,\n                'permitting': comparable.permitting_score or 0,\n                'data_quality': comparable.data_quality_score or 0,\n            },\n            'data_source': comparable.data_source,\n            'notes': comparable.notes\n        }\n","path":null,"size_bytes":9901,"size_tokens":null},"create_test_user.py":{"content":"import bcrypt\nfrom database import get_db_session\nfrom models import User\nfrom datetime import datetime\n\ndef hash_password(password):\n    \"\"\"Secure password hashing using bcrypt with salt\"\"\"\n    salt = bcrypt.gensalt()\n    return bcrypt.hashpw(password.encode(), salt).decode()\n\ndef create_test_user(email, password):\n    \"\"\"Create a test user with the specified credentials\"\"\"\n    with get_db_session() as db:\n        existing_user = db.query(User).filter(User.email == email).first()\n        \n        if existing_user:\n            existing_user.password_hash = hash_password(password)\n            existing_user.is_admin = True\n            existing_user.username = email.split('@')[0]\n            existing_user.last_login = datetime.utcnow()\n            existing_user.plan_type = 'enterprise'\n            existing_user.usage_limit = 999999\n            print(f\"Updated existing user {email}\")\n        else:\n            test_user = User(\n                email=email,\n                username=email.split('@')[0],\n                password_hash=hash_password(password),\n                is_admin=True,\n                created_at=datetime.utcnow(),\n                last_login=datetime.utcnow(),\n                plan_type='enterprise',\n                usage_limit=999999\n            )\n            db.add(test_user)\n            print(f\"Created new test user: {email}\")\n        \n        db.commit()\n        print(\"Test user created successfully!\")\n\nif __name__ == \"__main__\":\n    create_test_user(\"test@test.com\", \"test123\")\n","path":null,"size_bytes":1517,"size_tokens":null},"financial_exports.py":{"content":"import pandas as pd\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment, Border, Side\nfrom openpyxl.utils.dataframe import dataframe_to_rows\nfrom io import BytesIO\nfrom typing import Dict, List\nfrom datetime import datetime\nfrom format_utils import format_currency\n\nclass FinancialExporter:\n    \"\"\"Export financial models to Excel format\"\"\"\n    \n    def __init__(self):\n        self.header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')\n        self.header_font = Font(bold=True, color='FFFFFF', size=11)\n        self.title_font = Font(bold=True, size=14)\n        self.border = Border(\n            left=Side(style='thin'),\n            right=Side(style='thin'),\n            top=Side(style='thin'),\n            bottom=Side(style='thin')\n        )\n    \n    def export_cashflow_model(\n        self,\n        project_name: str,\n        model_name: str,\n        cashflow_data: Dict,\n        metrics: Dict,\n        assumptions: Dict\n    ) -> bytes:\n        \"\"\"\n        Export complete cash flow model to Excel\n        \n        Args:\n            project_name: Name of the mining project\n            model_name: Name of the financial model\n            cashflow_data: Dictionary with years, revenue, costs, etc.\n            metrics: NPV, IRR, payback, etc.\n            assumptions: Input assumptions (prices, costs, etc.)\n        \n        Returns:\n            Excel file as bytes\n        \"\"\"\n        wb = Workbook()\n        \n        self._add_summary_sheet(wb, project_name, model_name, metrics, assumptions)\n        \n        self._add_cashflow_sheet(wb, cashflow_data)\n        \n        self._add_assumptions_sheet(wb, assumptions)\n        \n        buffer = BytesIO()\n        wb.save(buffer)\n        buffer.seek(0)\n        \n        return buffer.getvalue()\n    \n    def _add_summary_sheet(self, wb: Workbook, project_name: str, model_name: str, metrics: Dict, assumptions: Dict):\n        \"\"\"Add summary sheet with key metrics\"\"\"\n        if 'Sheet' in wb.sheetnames:\n            ws = wb['Sheet']\n            ws.title = 'Summary'\n        else:\n            ws = wb.create_sheet('Summary', 0)\n        \n        ws.column_dimensions['A'].width = 25\n        ws.column_dimensions['B'].width = 20\n        \n        row = 1\n        \n        ws.merge_cells(f'A{row}:B{row}')\n        cell = ws[f'A{row}']\n        cell.value = 'Financial Model Summary'\n        cell.font = self.title_font\n        row += 2\n        \n        ws[f'A{row}'] = 'Project Name:'\n        ws[f'B{row}'] = project_name\n        ws[f'A{row}'].font = Font(bold=True)\n        row += 1\n        \n        ws[f'A{row}'] = 'Model Name:'\n        ws[f'B{row}'] = model_name\n        ws[f'A{row}'].font = Font(bold=True)\n        row += 1\n        \n        ws[f'A{row}'] = 'Generated:'\n        ws[f'B{row}'] = datetime.now().strftime('%Y-%m-%d %H:%M')\n        ws[f'A{row}'].font = Font(bold=True)\n        row += 2\n        \n        ws.merge_cells(f'A{row}:B{row}')\n        cell = ws[f'A{row}']\n        cell.value = 'Key Financial Metrics'\n        cell.font = Font(bold=True, size=12)\n        cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n        row += 1\n        \n        ws[f'A{row}'] = 'Net Present Value (NPV)'\n        ws[f'B{row}'] = format_currency(metrics.get('npv', 0), decimals=2)\n        row += 1\n        \n        ws[f'A{row}'] = 'Internal Rate of Return (IRR)'\n        ws[f'B{row}'] = f\"{metrics.get('irr', 0):.2f}%\" if metrics.get('irr') else \"N/A\"\n        row += 1\n        \n        ws[f'A{row}'] = 'Payback Period'\n        ws[f'B{row}'] = f\"{metrics.get('payback', 0):.2f} years\" if metrics.get('payback') else \"Never\"\n        row += 1\n        \n        ws[f'A{row}'] = 'Total Production'\n        ws[f'B{row}'] = f\"{assumptions.get('total_production', 0):,.0f} tonnes\"\n        row += 2\n        \n        ws.merge_cells(f'A{row}:B{row}')\n        cell = ws[f'A{row}']\n        cell.value = 'Base Assumptions'\n        cell.font = Font(bold=True, size=12)\n        cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n        row += 1\n        \n        ws[f'A{row}'] = 'Mine Life'\n        ws[f'B{row}'] = f\"{assumptions.get('mine_life', 0)} years\"\n        row += 1\n        \n        ws[f'A{row}'] = 'Commodity Price'\n        ws[f'B{row}'] = f\"${assumptions.get('commodity_price', 0):,.2f} per unit\"\n        row += 1\n        \n        ws[f'A{row}'] = 'Initial CAPEX'\n        ws[f'B{row}'] = format_currency(assumptions.get('initial_capex', 0), decimals=2)\n        row += 1\n        \n        ws[f'A{row}'] = 'OPEX per unit'\n        ws[f'B{row}'] = f\"${assumptions.get('opex_per_unit', 0):,.2f}\"\n        row += 1\n        \n        ws[f'A{row}'] = 'Discount Rate'\n        ws[f'B{row}'] = f\"{assumptions.get('discount_rate', 0):.1f}%\"\n        row += 1\n    \n    def _add_cashflow_sheet(self, wb: Workbook, cashflow_data: Dict):\n        \"\"\"Add detailed cash flow table\"\"\"\n        ws = wb.create_sheet('Cash Flow')\n        \n        df = pd.DataFrame({\n            'Year': cashflow_data['years'],\n            'Production (tonnes)': cashflow_data['production'],\n            'Revenue ($M)': cashflow_data['revenue'],\n            'Operating Costs ($M)': cashflow_data['operating_costs'],\n            'CAPEX ($M)': cashflow_data['capex'],\n            'Royalties ($M)': cashflow_data.get('royalties', [0] * len(cashflow_data['years'])),\n            'EBITDA ($M)': cashflow_data['ebitda'],\n            'Taxes ($M)': cashflow_data['taxes'],\n            'Net Cash Flow ($M)': cashflow_data['net_cashflow']\n        })\n        \n        for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=True), 1):\n            for c_idx, value in enumerate(row, 1):\n                cell = ws.cell(row=r_idx, column=c_idx)\n                cell.value = value\n                cell.border = self.border\n                cell.alignment = Alignment(horizontal='right' if c_idx > 1 else 'center')\n                \n                if r_idx == 1:\n                    cell.fill = self.header_fill\n                    cell.font = self.header_font\n                    cell.alignment = Alignment(horizontal='center')\n        \n        for col in range(1, 10):\n            ws.column_dimensions[chr(64 + col)].width = 15\n        \n        ws.column_dimensions['A'].width = 8\n    \n    def _add_assumptions_sheet(self, wb: Workbook, assumptions: Dict):\n        \"\"\"Add detailed assumptions sheet\"\"\"\n        ws = wb.create_sheet('Assumptions')\n        \n        ws.column_dimensions['A'].width = 30\n        ws.column_dimensions['B'].width = 20\n        \n        row = 1\n        \n        ws[f'A{row}'] = 'Parameter'\n        ws[f'B{row}'] = 'Value'\n        ws[f'A{row}'].font = self.header_font\n        ws[f'B{row}'].font = self.header_font\n        ws[f'A{row}'].fill = self.header_fill\n        ws[f'B{row}'].fill = self.header_fill\n        row += 1\n        \n        for key, value in assumptions.items():\n            ws[f'A{row}'] = key.replace('_', ' ').title()\n            if isinstance(value, (int, float)):\n                ws[f'B{row}'] = value\n            else:\n                ws[f'B{row}'] = str(value)\n            row += 1\n    \n    def export_sensitivity_analysis(\n        self,\n        project_name: str,\n        sensitivity_results: List[Dict],\n        variable_name: str\n    ) -> bytes:\n        \"\"\"\n        Export sensitivity analysis results to Excel\n        \n        Args:\n            project_name: Name of the project\n            sensitivity_results: List of sensitivity results\n            variable_name: Name of the variable being analyzed\n        \n        Returns:\n            Excel file as bytes\n        \"\"\"\n        wb = Workbook()\n        ws = wb.active\n        ws.title = 'Sensitivity Analysis'\n        \n        ws.column_dimensions['A'].width = 15\n        ws.column_dimensions['B'].width = 15\n        ws.column_dimensions['C'].width = 15\n        ws.column_dimensions['D'].width = 15\n        \n        row = 1\n        \n        ws.merge_cells(f'A{row}:D{row}')\n        cell = ws[f'A{row}']\n        cell.value = f'Sensitivity Analysis: {variable_name.replace(\"_\", \" \").title()}'\n        cell.font = self.title_font\n        row += 1\n        \n        ws[f'A{row}'] = f'Project: {project_name}'\n        row += 2\n        \n        headers = ['Change (%)', 'Value', 'NPV ($M)', 'IRR (%)']\n        for c_idx, header in enumerate(headers, 1):\n            cell = ws.cell(row=row, column=c_idx)\n            cell.value = header\n            cell.font = self.header_font\n            cell.fill = self.header_fill\n        row += 1\n        \n        for result in sensitivity_results:\n            ws.cell(row=row, column=1).value = f\"{result['variation_pct']:+.0f}%\"\n            ws.cell(row=row, column=2).value = result['value']\n            ws.cell(row=row, column=3).value = result['npv']\n            ws.cell(row=row, column=4).value = result['irr']\n            row += 1\n        \n        buffer = BytesIO()\n        wb.save(buffer)\n        buffer.seek(0)\n        \n        return buffer.getvalue()\n\n\ndef create_financial_exporter() -> FinancialExporter:\n    \"\"\"Factory function to create FinancialExporter instance\"\"\"\n    return FinancialExporter()\n","path":null,"size_bytes":9177,"size_tokens":null},"financial_engine.py":{"content":"import numpy as np\nimport pandas as pd\nfrom typing import Dict, List, Tuple, Optional\nfrom datetime import datetime\n\nclass FinancialEngine:\n    \"\"\"Core financial calculation engine for DCF models, NPV/IRR, and sensitivity analysis\"\"\"\n    \n    def __init__(self):\n        pass\n    \n    @staticmethod\n    def calculate_npv(cashflows: List[float], discount_rate: float) -> float:\n        \"\"\"\n        Calculate Net Present Value (NPV) of a series of cash flows\n        \n        Args:\n            cashflows: List of annual cash flows (year 0 = initial investment, typically negative)\n            discount_rate: Annual discount rate as decimal (e.g., 0.10 for 10%)\n        \n        Returns:\n            NPV in the same currency units as cash flows\n        \"\"\"\n        if not cashflows:\n            return 0.0\n        \n        npv = 0.0\n        for year, cashflow in enumerate(cashflows):\n            npv += cashflow / ((1 + discount_rate) ** year)\n        \n        return round(npv, 2)\n    \n    @staticmethod\n    def calculate_irr(cashflows: List[float], guess: float = 0.1, max_iterations: int = 100, tolerance: float = 1e-6) -> Optional[float]:\n        \"\"\"\n        Calculate Internal Rate of Return (IRR) using Newton-Raphson method\n        \n        Args:\n            cashflows: List of annual cash flows\n            guess: Initial guess for IRR (default 10%)\n            max_iterations: Maximum number of iterations\n            tolerance: Convergence tolerance\n        \n        Returns:\n            IRR as percentage, or None if calculation fails\n        \"\"\"\n        if not cashflows or len(cashflows) < 2:\n            return None\n        \n        try:\n            rate = guess\n            \n            for _ in range(max_iterations):\n                npv = sum(cf / ((1 + rate) ** t) for t, cf in enumerate(cashflows))\n                \n                npv_derivative = sum(-t * cf / ((1 + rate) ** (t + 1)) for t, cf in enumerate(cashflows))\n                \n                if abs(npv_derivative) < tolerance:\n                    return None\n                \n                new_rate = rate - npv / npv_derivative\n                \n                if abs(new_rate - rate) < tolerance:\n                    if -0.99 <= new_rate <= 10.0:\n                        return round(new_rate * 100, 2)\n                    return None\n                \n                rate = new_rate\n            \n            return None\n        except:\n            return None\n    \n    @staticmethod\n    def calculate_payback_period(cashflows: List[float]) -> Optional[float]:\n        \"\"\"\n        Calculate payback period in years\n        \n        Args:\n            cashflows: List of annual cash flows\n        \n        Returns:\n            Number of years to recover initial investment, or None if never recovered\n        \"\"\"\n        if not cashflows or len(cashflows) < 2:\n            return None\n        \n        cumulative = 0.0\n        for year, cashflow in enumerate(cashflows):\n            cumulative += cashflow\n            if cumulative >= 0:\n                if year == 0:\n                    return 0.0\n                \n                previous_cumulative = cumulative - cashflow\n                fraction = abs(previous_cumulative) / abs(cashflow)\n                return round(year - 1 + fraction, 2)\n        \n        return None\n    \n    @staticmethod\n    def generate_production_profile(\n        mine_life_years: int,\n        annual_production_target: float,\n        ramp_up_years: int = 1,\n        ramp_down_years: int = 0\n    ) -> List[float]:\n        \"\"\"\n        Generate production profile with ramp-up and ramp-down\n        \n        Args:\n            mine_life_years: Total mine life\n            annual_production_target: Target annual production at full capacity\n            ramp_up_years: Years to reach full production\n            ramp_down_years: Years of declining production\n        \n        Returns:\n            List of annual production volumes\n        \"\"\"\n        profile = []\n        \n        for year in range(mine_life_years):\n            if year < ramp_up_years:\n                production = annual_production_target * (year + 1) / ramp_up_years\n            elif year >= mine_life_years - ramp_down_years:\n                years_from_end = mine_life_years - year\n                production = annual_production_target * years_from_end / (ramp_down_years + 1)\n            else:\n                production = annual_production_target\n            \n            profile.append(round(production, 2))\n        \n        return profile\n    \n    def generate_cashflow_model(\n        self,\n        mine_life_years: int,\n        production_profile: List[float],\n        commodity_price: float,\n        opex_per_unit: float,\n        initial_capex: float,\n        sustaining_capex_annual: float = 0,\n        royalty_rate: float = 0.03,\n        tax_rate: float = 0.30,\n        recovery_rate: float = 1.0\n    ) -> Dict:\n        \"\"\"\n        Generate complete cash flow model\n        \n        Args:\n            mine_life_years: Total mine life\n            production_profile: List of annual production volumes\n            commodity_price: Price per unit of commodity\n            opex_per_unit: Operating cost per unit produced\n            initial_capex: Initial capital expenditure (year 0)\n            sustaining_capex_annual: Annual sustaining capital\n            royalty_rate: Royalty percentage (decimal)\n            tax_rate: Corporate tax rate (decimal)\n            recovery_rate: Metallurgical recovery rate (decimal)\n        \n        Returns:\n            Dictionary with detailed cash flow breakdown\n        \"\"\"\n        years = list(range(mine_life_years + 1))\n        revenue = []\n        operating_costs = []\n        capex = []\n        royalties = []\n        ebitda = []\n        taxes = []\n        net_cashflow = []\n        \n        for year in years:\n            if year == 0:\n                revenue.append(0)\n                operating_costs.append(0)\n                capex.append(-initial_capex)\n                royalties.append(0)\n                ebitda.append(0)\n                taxes.append(0)\n                net_cashflow.append(-initial_capex)\n            else:\n                production = production_profile[year - 1]\n                recoverable_production = production * recovery_rate\n                \n                year_revenue = recoverable_production * commodity_price\n                year_opex = production * opex_per_unit\n                year_royalty = year_revenue * royalty_rate\n                year_ebitda = year_revenue - year_opex - year_royalty\n                year_tax = max(0, year_ebitda * tax_rate)\n                year_net = year_ebitda - year_tax - sustaining_capex_annual\n                \n                revenue.append(round(year_revenue, 2))\n                operating_costs.append(round(-year_opex, 2))\n                capex.append(round(-sustaining_capex_annual, 2))\n                royalties.append(round(-year_royalty, 2))\n                ebitda.append(round(year_ebitda, 2))\n                taxes.append(round(-year_tax, 2))\n                net_cashflow.append(round(year_net, 2))\n        \n        return {\n            'years': years,\n            'production': [0] + production_profile,\n            'revenue': revenue,\n            'operating_costs': operating_costs,\n            'capex': capex,\n            'royalties': royalties,\n            'ebitda': ebitda,\n            'taxes': taxes,\n            'net_cashflow': net_cashflow\n        }\n    \n    def calculate_sensitivity_analysis(\n        self,\n        base_params: Dict,\n        variable_name: str,\n        variation_range: List[float],\n        discount_rate: float\n    ) -> List[Dict]:\n        \"\"\"\n        Perform sensitivity analysis by varying one parameter\n        \n        Args:\n            base_params: Base case parameters\n            variable_name: Name of parameter to vary (e.g., 'commodity_price', 'opex_per_unit')\n            variation_range: List of percentage changes (e.g., [-20, -10, 0, 10, 20])\n            discount_rate: Discount rate for NPV calculation\n        \n        Returns:\n            List of results for each variation\n        \"\"\"\n        results = []\n        \n        for variation_pct in variation_range:\n            params = base_params.copy()\n            \n            base_value = params.get(variable_name, 0)\n            new_value = base_value * (1 + variation_pct / 100)\n            params[variable_name] = new_value\n            \n            cashflow_model = self.generate_cashflow_model(**params)\n            npv = self.calculate_npv(cashflow_model['net_cashflow'], discount_rate)\n            irr = self.calculate_irr(cashflow_model['net_cashflow'])\n            \n            results.append({\n                'variable': variable_name,\n                'variation_pct': variation_pct,\n                'value': round(new_value, 2),\n                'npv': npv,\n                'irr': irr\n            })\n        \n        return results\n    \n    def calculate_multi_variable_sensitivity(\n        self,\n        base_params: Dict,\n        variables_to_vary: List[str],\n        variation_range: List[float],\n        discount_rate: float\n    ) -> pd.DataFrame:\n        \"\"\"\n        Perform sensitivity analysis on multiple variables (tornado chart data)\n        \n        Args:\n            base_params: Base case parameters\n            variables_to_vary: List of parameter names to vary\n            variation_range: Typically [-20, 20] for -20% to +20%\n            discount_rate: Discount rate for NPV\n        \n        Returns:\n            DataFrame with sensitivity results for tornado chart\n        \"\"\"\n        results = []\n        \n        for variable in variables_to_vary:\n            for variation_pct in variation_range:\n                params = base_params.copy()\n                base_value = params.get(variable, 0)\n                new_value = base_value * (1 + variation_pct / 100)\n                params[variable] = new_value\n                \n                cashflow_model = self.generate_cashflow_model(**params)\n                npv = self.calculate_npv(cashflow_model['net_cashflow'], discount_rate)\n                \n                results.append({\n                    'variable': variable,\n                    'variation_pct': variation_pct,\n                    'npv': npv\n                })\n        \n        df = pd.DataFrame(results)\n        \n        pivot_df = df.pivot(index='variable', columns='variation_pct', values='npv')\n        \n        if len(variation_range) >= 2:\n            pivot_df['range'] = abs(pivot_df[variation_range[-1]] - pivot_df[variation_range[0]])\n            pivot_df = pivot_df.sort_values('range', ascending=False)\n        \n        return pivot_df\n    \n    @staticmethod\n    def calculate_project_valuation(\n        npv: float,\n        resource_tonnes: float,\n        annual_production: float,\n        comparable_multiples: Optional[Dict] = None\n    ) -> Dict:\n        \"\"\"\n        Calculate project valuation using multiple methods\n        \n        Args:\n            npv: Net present value\n            resource_tonnes: Total resource in tonnes\n            annual_production: Annual production in tonnes\n            comparable_multiples: Dictionary with industry multiples\n        \n        Returns:\n            Dictionary with valuation estimates\n        \"\"\"\n        valuation = {\n            'dcf_valuation': npv,\n            'resource_value_per_tonne': npv / resource_tonnes if resource_tonnes > 0 else 0,\n            'production_multiple': npv / annual_production if annual_production > 0 else 0\n        }\n        \n        if comparable_multiples:\n            if 'ev_per_resource' in comparable_multiples:\n                valuation['comp_valuation_resource'] = resource_tonnes * comparable_multiples['ev_per_resource']\n            \n            if 'ev_per_production' in comparable_multiples:\n                valuation['comp_valuation_production'] = annual_production * comparable_multiples['ev_per_production']\n        \n        return {k: round(v, 2) for k, v in valuation.items()}\n","path":null,"size_bytes":12031,"size_tokens":null},"market_data.py":{"content":"import requests\nimport os\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\nfrom database import get_db_session\nfrom models import CommodityPriceSnapshot\n\nclass MarketDataProvider:\n    \"\"\"\n    Fetches and caches real-time commodity prices from Metals-API\n    Free tier: 100 API calls/month\n    \"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None):\n        self.api_key = api_key or os.getenv('METALS_API_KEY', '')\n        self.base_url = 'https://metals-api.com/api'\n        self.cache_ttl_hours = 1\n        \n        self.commodity_symbols = {\n            'gold': 'XAU',\n            'silver': 'XAG',\n            'copper': 'XCU',\n            'lithium': 'LITHIUM',\n            'platinum': 'XPT',\n            'palladium': 'XPD',\n            'aluminum': 'ALU',\n            'zinc': 'ZINC',\n            'nickel': 'NI',\n            'lead': 'LEAD',\n            'tin': 'TIN',\n            'iron_ore': 'IRON'\n        }\n        \n        self.commodity_units = {\n            'gold': 'USD/troy oz',\n            'silver': 'USD/troy oz',\n            'copper': 'USD/lb',\n            'lithium': 'USD/kg',\n            'platinum': 'USD/troy oz',\n            'palladium': 'USD/troy oz',\n            'aluminum': 'USD/lb',\n            'zinc': 'USD/lb',\n            'nickel': 'USD/lb',\n            'lead': 'USD/lb',\n            'tin': 'USD/lb',\n            'iron_ore': 'USD/tonne'\n        }\n    \n    def get_commodity_price(self, commodity: str, use_cache: bool = True) -> Optional[Dict]:\n        \"\"\"\n        Get current price for a commodity\n        \n        Args:\n            commodity: Commodity name (e.g., 'gold', 'copper', 'lithium')\n            use_cache: Whether to use cached data (default True)\n        \n        Returns:\n            Dictionary with price data or None if unavailable\n        \"\"\"\n        commodity = commodity.lower()\n        \n        if use_cache:\n            cached_price = self._get_cached_price(commodity)\n            if cached_price:\n                return cached_price\n        \n        if self.api_key:\n            live_price = self._fetch_live_price(commodity)\n            if live_price:\n                self._cache_price(commodity, live_price)\n                return live_price\n        \n        return self._get_mock_price(commodity)\n    \n    def get_multiple_commodities(self, commodities: List[str], use_cache: bool = True) -> Dict[str, Dict]:\n        \"\"\"\n        Get prices for multiple commodities\n        \n        Args:\n            commodities: List of commodity names\n            use_cache: Whether to use cached data\n        \n        Returns:\n            Dictionary mapping commodity names to price data\n        \"\"\"\n        results = {}\n        for commodity in commodities:\n            price_data = self.get_commodity_price(commodity, use_cache)\n            if price_data:\n                results[commodity] = price_data\n        \n        return results\n    \n    def _get_cached_price(self, commodity: str) -> Optional[Dict]:\n        \"\"\"Retrieve cached price if still valid\"\"\"\n        try:\n            with get_db_session() as db:\n                cutoff_time = datetime.utcnow() - timedelta(hours=self.cache_ttl_hours)\n                \n                snapshot = db.query(CommodityPriceSnapshot).filter(\n                    CommodityPriceSnapshot.commodity == commodity,\n                    CommodityPriceSnapshot.fetched_at >= cutoff_time\n                ).order_by(CommodityPriceSnapshot.fetched_at.desc()).first()\n                \n                if snapshot:\n                    return {\n                        'commodity': snapshot.commodity,\n                        'price': snapshot.price,\n                        'currency': snapshot.currency,\n                        'unit': snapshot.unit,\n                        'change_24h': snapshot.price_change_24h,\n                        'change_pct_24h': snapshot.price_change_percent_24h,\n                        'high_52w': snapshot.high_52w,\n                        'low_52w': snapshot.low_52w,\n                        'source': snapshot.source,\n                        'fetched_at': snapshot.fetched_at.isoformat(),\n                        'from_cache': True\n                    }\n        except Exception as e:\n            print(f\"Error retrieving cached price: {e}\")\n        \n        return None\n    \n    def _fetch_live_price(self, commodity: str) -> Optional[Dict]:\n        \"\"\"Fetch live price from Metals-API\"\"\"\n        try:\n            symbol = self.commodity_symbols.get(commodity)\n            if not symbol:\n                return None\n            \n            url = f\"{self.base_url}/latest\"\n            params = {\n                'access_key': self.api_key,\n                'base': 'USD',\n                'symbols': symbol\n            }\n            \n            response = requests.get(url, params=params, timeout=10)\n            \n            if response.status_code == 200:\n                data = response.json()\n                \n                if data.get('success') and symbol in data.get('rates', {}):\n                    rate = data['rates'][symbol]\n                    \n                    price = 1 / rate if rate > 0 else 0\n                    \n                    if commodity in ['copper', 'aluminum', 'zinc', 'nickel', 'lead', 'tin']:\n                        price = price / 14.5833\n                    elif commodity == 'lithium':\n                        price = price * 31.1035\n                    elif commodity == 'iron_ore':\n                        price = price * 907.185\n                    \n                    return {\n                        'commodity': commodity,\n                        'price': round(price, 2),\n                        'currency': 'USD',\n                        'unit': self.commodity_units.get(commodity, 'USD'),\n                        'change_24h': None,\n                        'change_pct_24h': None,\n                        'high_52w': None,\n                        'low_52w': None,\n                        'source': 'Metals-API',\n                        'fetched_at': datetime.utcnow().isoformat(),\n                        'from_cache': False\n                    }\n        except Exception as e:\n            print(f\"Error fetching live price for {commodity}: {e}\")\n        \n        return None\n    \n    def _cache_price(self, commodity: str, price_data: Dict):\n        \"\"\"Cache price data to database\"\"\"\n        try:\n            with get_db_session() as db:\n                expires_at = datetime.utcnow() + timedelta(hours=self.cache_ttl_hours)\n                \n                snapshot = CommodityPriceSnapshot(\n                    commodity=commodity,\n                    price=price_data['price'],\n                    currency=price_data['currency'],\n                    unit=price_data['unit'],\n                    price_change_24h=price_data.get('change_24h'),\n                    price_change_percent_24h=price_data.get('change_pct_24h'),\n                    high_52w=price_data.get('high_52w'),\n                    low_52w=price_data.get('low_52w'),\n                    source=price_data['source'],\n                    source_url='https://metals-api.com',\n                    fetched_at=datetime.utcnow(),\n                    expires_at=expires_at\n                )\n                \n                db.add(snapshot)\n                db.commit()\n        except Exception as e:\n            print(f\"Error caching price data: {e}\")\n    \n    def _get_mock_price(self, commodity: str) -> Dict:\n        \"\"\"\n        Return mock/fallback prices for development and testing\n        Based on approximate market prices as of 2025\n        \"\"\"\n        mock_prices = {\n            'gold': 3980.00,\n            'silver': 48.50,\n            'copper': 0.31,\n            'lithium': 22000.00,\n            'platinum': 1850.00,\n            'palladium': 1450.00,\n            'aluminum': 0.09,\n            'zinc': 0.12,\n            'nickel': 0.72,\n            'lead': 0.08,\n            'tin': 1.20,\n            'iron_ore': 105.00\n        }\n        \n        price = mock_prices.get(commodity, 0)\n        \n        return {\n            'commodity': commodity,\n            'price': price,\n            'currency': 'USD',\n            'unit': self.commodity_units.get(commodity, 'USD'),\n            'change_24h': None,\n            'change_pct_24h': None,\n            'high_52w': None,\n            'low_52w': None,\n            'source': 'Mock Data (Development)',\n            'fetched_at': datetime.utcnow().isoformat(),\n            'from_cache': False\n        }\n    \n    def get_historical_data(self, commodity: str, days: int = 30) -> List[Dict]:\n        \"\"\"\n        Get historical price data (if API key supports it)\n        \n        Args:\n            commodity: Commodity name\n            days: Number of days of historical data\n        \n        Returns:\n            List of price data points\n        \"\"\"\n        if not self.api_key:\n            return []\n        \n        try:\n            symbol = self.commodity_symbols.get(commodity.lower())\n            if not symbol:\n                return []\n            \n            url = f\"{self.base_url}/timeseries\"\n            \n            end_date = datetime.utcnow()\n            start_date = end_date - timedelta(days=days)\n            \n            params = {\n                'access_key': self.api_key,\n                'start_date': start_date.strftime('%Y-%m-%d'),\n                'end_date': end_date.strftime('%Y-%m-%d'),\n                'symbols': symbol\n            }\n            \n            response = requests.get(url, params=params, timeout=15)\n            \n            if response.status_code == 200:\n                data = response.json()\n                \n                if data.get('success') and 'rates' in data:\n                    historical = []\n                    for date_str, rates in data['rates'].items():\n                        if symbol in rates:\n                            rate = rates[symbol]\n                            price = 1 / rate if rate > 0 else 0\n                            \n                            historical.append({\n                                'date': date_str,\n                                'price': round(price, 2)\n                            })\n                    \n                    return sorted(historical, key=lambda x: x['date'])\n        except Exception as e:\n            print(f\"Error fetching historical data: {e}\")\n        \n        return []\n    \n    def get_all_mining_commodities(self, use_cache: bool = True) -> Dict[str, Dict]:\n        \"\"\"Get prices for all common mining commodities\"\"\"\n        mining_commodities = ['gold', 'silver', 'copper', 'lithium', 'zinc', 'nickel', 'iron_ore']\n        return self.get_multiple_commodities(mining_commodities, use_cache)\n\n\ndef get_market_data_provider() -> MarketDataProvider:\n    \"\"\"Factory function to create MarketDataProvider instance\"\"\"\n    return MarketDataProvider()\n","path":null,"size_bytes":10884,"size_tokens":null},"page_modules/financials_page.py":{"content":"import streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom project_manager import ProjectManager\nfrom financial_engine import FinancialEngine\nfrom market_data import get_market_data_provider\nfrom comparables_manager import ComparablesManager\nfrom financial_exports import create_financial_exporter\nfrom database import get_db_session\nfrom models import FinancialModel, FinancialScenario\nimport json\n\ndef render_financials_page(current_user):\n    \"\"\"Render the Financial Analysis & Valuation page\"\"\"\n    \n    st.markdown(\"## ðŸ’° Financial Analysis & Valuation\")\n    st.markdown(\"Comprehensive NPV/IRR modeling, sensitivity analysis, and project valuations\")\n    \n    st.markdown(\"---\")\n    \n    tab1, tab2, tab3, tab4 = st.tabs([\n        \"ðŸ“Š NPV/IRR Calculator\",\n        \"ðŸ“ˆ Sensitivity Analysis\",\n        \"ðŸ’Ž Valuation Module\",\n        \"ðŸŒ Market Data\"\n    ])\n    \n    with tab1:\n        render_npv_irr_calculator(current_user)\n    \n    with tab2:\n        render_sensitivity_analysis(current_user)\n    \n    with tab3:\n        render_valuation_module(current_user)\n    \n    with tab4:\n        render_market_data(current_user)\n\n\ndef render_npv_irr_calculator(current_user):\n    \"\"\"NPV/IRR Calculator tab with interactive DCF model\"\"\"\n    \n    st.markdown(\"### ðŸ“Š NPV/IRR Calculator\")\n    st.markdown(\"Build discounted cash flow models and calculate project economics\")\n    \n    projects = ProjectManager.get_user_projects(current_user['id'])\n    \n    if not projects:\n        st.info(\"ðŸ‘‰ No projects found. Create a project in the Projects page first.\")\n        return\n    \n    col_select1, col_select2 = st.columns([2, 1])\n    \n    with col_select1:\n        project_names = [p['name'] for p in projects]\n        selected_project_name = st.selectbox(\n            \"Select Project\",\n            project_names,\n            key=\"npv_project_select\"\n        )\n    \n    with col_select2:\n        model_name = st.text_input(\"Model Name\", value=\"Base Case Model\", key=\"npv_model_name\")\n    \n    selected_project = next((p for p in projects if p['name'] == selected_project_name), None)\n    \n    if selected_project:\n        st.markdown(\"---\")\n        st.markdown(\"#### Production & Pricing Assumptions\")\n        \n        col_prod1, col_prod2, col_prod3 = st.columns(3)\n        \n        with col_prod1:\n            mine_life_years = st.number_input(\n                \"Mine Life (years)\",\n                min_value=1,\n                max_value=50,\n                value=15,\n                key=\"npv_mine_life\"\n            )\n            \n            annual_production = st.number_input(\n                \"Annual Production (tonnes)\",\n                min_value=1000.0,\n                value=500000.0,\n                step=10000.0,\n                format=\"%.0f\",\n                key=\"npv_annual_prod\"\n            )\n        \n        with col_prod2:\n            commodity_price = st.number_input(\n                f\"Commodity Price (USD per unit)\",\n                min_value=0.01,\n                value=50.0,\n                step=1.0,\n                format=\"%.2f\",\n                key=\"npv_commodity_price\"\n            )\n            \n            recovery_rate = st.slider(\n                \"Metallurgical Recovery Rate (%)\",\n                min_value=0,\n                max_value=100,\n                value=85,\n                key=\"npv_recovery\"\n            ) / 100\n        \n        with col_prod3:\n            ramp_up_years = st.number_input(\n                \"Ramp-up Period (years)\",\n                min_value=0,\n                max_value=5,\n                value=1,\n                key=\"npv_ramp_up\"\n            )\n            \n            total_resource = st.number_input(\n                \"Total Resource (million tonnes)\",\n                min_value=0.1,\n                value=10.0,\n                step=0.5,\n                format=\"%.2f\",\n                key=\"npv_resource\"\n            )\n        \n        st.markdown(\"#### Cost Assumptions\")\n        \n        col_cost1, col_cost2, col_cost3 = st.columns(3)\n        \n        with col_cost1:\n            initial_capex = st.number_input(\n                \"Initial CAPEX (USD millions)\",\n                min_value=1.0,\n                value=150.0,\n                step=10.0,\n                format=\"%.2f\",\n                key=\"npv_capex\"\n            )\n            \n            opex_per_unit = st.number_input(\n                \"Operating Cost (USD per tonne)\",\n                min_value=0.01,\n                value=25.0,\n                step=1.0,\n                format=\"%.2f\",\n                key=\"npv_opex\"\n            )\n        \n        with col_cost2:\n            sustaining_capex = st.number_input(\n                \"Sustaining CAPEX (USD millions/year)\",\n                min_value=0.0,\n                value=5.0,\n                step=1.0,\n                format=\"%.2f\",\n                key=\"npv_sustaining_capex\"\n            )\n            \n            royalty_rate = st.slider(\n                \"Royalty Rate (%)\",\n                min_value=0,\n                max_value=20,\n                value=3,\n                key=\"npv_royalty\"\n            ) / 100\n        \n        with col_cost3:\n            discount_rate = st.slider(\n                \"Discount Rate (%)\",\n                min_value=1,\n                max_value=25,\n                value=10,\n                key=\"npv_discount\"\n            ) / 100\n            \n            tax_rate = st.slider(\n                \"Corporate Tax Rate (%)\",\n                min_value=0,\n                max_value=50,\n                value=30,\n                key=\"npv_tax\"\n            ) / 100\n        \n        st.markdown(\"---\")\n        \n        if st.button(\"ðŸ§® Calculate NPV / IRR\", type=\"primary\", use_container_width=True):\n            \n            with st.spinner(\"Calculating financial metrics...\"):\n                engine = FinancialEngine()\n                \n                production_profile = engine.generate_production_profile(\n                    mine_life_years=mine_life_years,\n                    annual_production_target=annual_production,\n                    ramp_up_years=ramp_up_years\n                )\n                \n                cashflow_model = engine.generate_cashflow_model(\n                    mine_life_years=mine_life_years,\n                    production_profile=production_profile,\n                    commodity_price=commodity_price,\n                    opex_per_unit=opex_per_unit,\n                    initial_capex=initial_capex,\n                    sustaining_capex_annual=sustaining_capex,\n                    royalty_rate=royalty_rate,\n                    tax_rate=tax_rate,\n                    recovery_rate=recovery_rate\n                )\n                \n                npv = engine.calculate_npv(cashflow_model['net_cashflow'], discount_rate)\n                irr = engine.calculate_irr(cashflow_model['net_cashflow'])\n                payback = engine.calculate_payback_period(cashflow_model['net_cashflow'])\n                \n                st.success(\"âœ… Calculation complete!\")\n                \n                st.markdown(\"### ðŸ“Š Results\")\n                \n                col_metric1, col_metric2, col_metric3, col_metric4 = st.columns(4)\n                \n                with col_metric1:\n                    st.metric(\n                        label=\"Net Present Value\",\n                        value=f\"${npv:,.2f}M\",\n                        delta=\"After-tax, discounted\"\n                    )\n                \n                with col_metric2:\n                    st.metric(\n                        label=\"Internal Rate of Return\",\n                        value=f\"{irr:.2f}%\" if irr else \"N/A\",\n                        delta=\"Annual return\"\n                    )\n                \n                with col_metric3:\n                    st.metric(\n                        label=\"Payback Period\",\n                        value=f\"{payback:.1f} years\" if payback else \"Never\",\n                        delta=\"Time to recover CAPEX\"\n                    )\n                \n                with col_metric4:\n                    st.metric(\n                        label=\"Total Production\",\n                        value=f\"{sum(production_profile)/1000000:.2f}Mt\",\n                        delta=f\"{mine_life_years} year mine life\"\n                    )\n                \n                st.markdown(\"---\")\n                \n                st.markdown(\"### ðŸ’µ Cash Flow Table\")\n                \n                df_cashflow = pd.DataFrame({\n                    'Year': cashflow_model['years'],\n                    'Production (t)': [f\"{p:,.0f}\" for p in cashflow_model['production']],\n                    'Revenue ($M)': [f\"{r:,.2f}\" for r in cashflow_model['revenue']],\n                    'OPEX ($M)': [f\"{c:,.2f}\" for c in cashflow_model['operating_costs']],\n                    'CAPEX ($M)': [f\"{c:,.2f}\" for c in cashflow_model['capex']],\n                    'EBITDA ($M)': [f\"{e:,.2f}\" for e in cashflow_model['ebitda']],\n                    'Taxes ($M)': [f\"{t:,.2f}\" for t in cashflow_model['taxes']],\n                    'Net CF ($M)': [f\"{n:,.2f}\" for n in cashflow_model['net_cashflow']]\n                })\n                \n                st.dataframe(df_cashflow, use_container_width=True, height=400)\n                \n                st.markdown(\"---\")\n                \n                st.markdown(\"### ðŸ“ˆ Cash Flow Chart\")\n                \n                chart_df = pd.DataFrame({\n                    'Year': cashflow_model['years'],\n                    'Net Cash Flow': cashflow_model['net_cashflow'],\n                    'Cumulative Cash Flow': np.cumsum(cashflow_model['net_cashflow'])\n                })\n                \n                st.line_chart(chart_df.set_index('Year'), use_container_width=True)\n                \n                st.markdown(\"---\")\n                \n                col_export1, col_export2 = st.columns([1, 1])\n                \n                with col_export1:\n                    exporter = create_financial_exporter()\n                    excel_data = exporter.export_cashflow_model(\n                        project_name=selected_project['name'],\n                        model_name=model_name,\n                        cashflow_data=cashflow_model,\n                        metrics={'npv': npv, 'irr': irr, 'payback': payback},\n                        assumptions={\n                            'mine_life': mine_life_years,\n                            'commodity_price': commodity_price,\n                            'initial_capex': initial_capex,\n                            'opex_per_unit': opex_per_unit,\n                            'discount_rate': discount_rate * 100,\n                            'tax_rate': tax_rate * 100,\n                            'total_production': sum(production_profile)\n                        }\n                    )\n                    \n                    st.download_button(\n                        label=\"ðŸ“¥ Download Excel\",\n                        data=excel_data,\n                        file_name=f\"{selected_project['name']}_{model_name}_Financial_Model.xlsx\",\n                        mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n                        use_container_width=True\n                    )\n                \n                with col_export2:\n                    save_model = st.checkbox(\"ðŸ’¾ Save to database\", value=True)\n                \n                if save_model:\n                    try:\n                        with get_db_session() as db:\n                            financial_model = FinancialModel(\n                                project_id=selected_project['id'],\n                                user_id=current_user['id'],\n                                name=model_name,\n                                description=f\"Financial model for {selected_project['name']}\",\n                                model_type='dcf',\n                                base_commodity_price=commodity_price,\n                                commodity_price_unit='USD',\n                                production_profile=json.dumps(production_profile),\n                                initial_capex_millions=initial_capex,\n                                sustaining_capex_millions=sustaining_capex,\n                                opex_per_unit=opex_per_unit,\n                                opex_unit='USD/tonne',\n                                discount_rate=discount_rate * 100,\n                                mine_life_years=mine_life_years,\n                                ramp_up_years=ramp_up_years,\n                                revenue_assumptions=json.dumps({\n                                    'recovery_rate': recovery_rate,\n                                    'royalty_rate': royalty_rate\n                                }),\n                                cost_assumptions=json.dumps(cashflow_model),\n                                tax_assumptions=json.dumps({'tax_rate': tax_rate}),\n                                calculated_npv=npv,\n                                calculated_irr=irr,\n                                calculated_payback_years=payback,\n                                calculated_metrics=json.dumps({\n                                    'total_production': sum(production_profile),\n                                    'total_revenue': sum(cashflow_model['revenue']),\n                                    'total_opex': sum(cashflow_model['operating_costs'])\n                                })\n                            )\n                            \n                            db.add(financial_model)\n                            db.commit()\n                            \n                            st.success(f\"âœ… Financial model '{model_name}' saved successfully!\")\n                    except Exception as e:\n                        st.error(f\"Error saving financial model: {e}\")\n\n\ndef render_sensitivity_analysis(current_user):\n    \"\"\"Sensitivity Analysis tab with tornado charts\"\"\"\n    \n    st.markdown(\"### ðŸ“ˆ Sensitivity Analysis\")\n    st.markdown(\"Analyze how changes in key variables impact NPV and IRR\")\n    \n    projects = ProjectManager.get_user_projects(current_user['id'])\n    \n    if not projects:\n        st.info(\"ðŸ‘‰ No projects found. Create a project in the Projects page first.\")\n        return\n    \n    project_names = [p['name'] for p in projects]\n    selected_project_name = st.selectbox(\n        \"Select Project\",\n        project_names,\n        key=\"sensitivity_project_select\"\n    )\n    \n    selected_project = next((p for p in projects if p['name'] == selected_project_name), None)\n    \n    if selected_project:\n        st.markdown(\"---\")\n        st.markdown(\"#### Base Case Assumptions\")\n        \n        col_base1, col_base2 = st.columns(2)\n        \n        with col_base1:\n            base_commodity_price = st.number_input(\"Commodity Price (USD)\", value=50.0, key=\"sens_price\")\n            base_opex = st.number_input(\"OPEX per unit (USD)\", value=25.0, key=\"sens_opex\")\n            base_capex = st.number_input(\"Initial CAPEX (USD millions)\", value=150.0, key=\"sens_capex\")\n        \n        with col_base2:\n            base_production = st.number_input(\"Annual Production (tonnes)\", value=500000.0, key=\"sens_prod\")\n            base_discount = st.slider(\"Discount Rate (%)\", 1, 25, 10, key=\"sens_discount\") / 100\n            mine_life = st.number_input(\"Mine Life (years)\", value=15, key=\"sens_mine_life\")\n        \n        st.markdown(\"#### Sensitivity Range\")\n        \n        col_range1, col_range2 = st.columns(2)\n        \n        with col_range1:\n            variation_low = st.number_input(\"Low Variation (%)\", value=-20, key=\"sens_low\")\n        \n        with col_range2:\n            variation_high = st.number_input(\"High Variation (%)\", value=20, key=\"sens_high\")\n        \n        st.markdown(\"---\")\n        \n        if st.button(\"ðŸ”„ Run Sensitivity Analysis\", type=\"primary\", use_container_width=True):\n            \n            with st.spinner(\"Running sensitivity analysis...\"):\n                engine = FinancialEngine()\n                \n                base_params = {\n                    'mine_life_years': mine_life,\n                    'production_profile': engine.generate_production_profile(mine_life, base_production),\n                    'commodity_price': base_commodity_price,\n                    'opex_per_unit': base_opex,\n                    'initial_capex': base_capex,\n                    'sustaining_capex_annual': 5.0,\n                    'royalty_rate': 0.03,\n                    'tax_rate': 0.30,\n                    'recovery_rate': 0.85\n                }\n                \n                variables = ['commodity_price', 'opex_per_unit', 'initial_capex']\n                variation_range = [variation_low, variation_high]\n                \n                sensitivity_df = engine.calculate_multi_variable_sensitivity(\n                    base_params,\n                    variables,\n                    variation_range,\n                    base_discount\n                )\n                \n                st.success(\"âœ… Sensitivity analysis complete!\")\n                \n                st.markdown(\"### ðŸ“Š Tornado Chart (NPV Sensitivity)\")\n                \n                st.markdown(\"**NPV Impact by Variable**\")\n                \n                tornado_data = []\n                for var in sensitivity_df.index:\n                    low_npv = sensitivity_df.loc[var, variation_low]\n                    high_npv = sensitivity_df.loc[var, variation_high]\n                    range_val = abs(high_npv - low_npv)\n                    \n                    tornado_data.append({\n                        'Variable': var.replace('_', ' ').title(),\n                        'Low NPV': low_npv,\n                        'High NPV': high_npv,\n                        'Range': range_val\n                    })\n                \n                tornado_df = pd.DataFrame(tornado_data).sort_values('Range', ascending=False)\n                \n                st.dataframe(tornado_df, use_container_width=True)\n                \n                st.markdown(\"---\")\n                \n                st.markdown(\"### ðŸ“‰ Detailed Sensitivity Tables\")\n                \n                for variable in variables:\n                    st.markdown(f\"#### {variable.replace('_', ' ').title()}\")\n                    \n                    results = engine.calculate_sensitivity_analysis(\n                        base_params,\n                        variable,\n                        list(range(variation_low, variation_high + 1, 5)),\n                        base_discount\n                    )\n                    \n                    results_df = pd.DataFrame(results)\n                    results_df['variation_pct'] = results_df['variation_pct'].apply(lambda x: f\"{x:+.0f}%\")\n                    results_df['value'] = results_df['value'].apply(lambda x: f\"{x:,.2f}\")\n                    results_df['npv'] = results_df['npv'].apply(lambda x: f\"${x:,.2f}M\")\n                    results_df['irr'] = results_df['irr'].apply(lambda x: f\"{x:.2f}%\" if x else \"N/A\")\n                    \n                    st.dataframe(\n                        results_df[['variation_pct', 'value', 'npv', 'irr']],\n                        column_config={\n                            'variation_pct': 'Change',\n                            'value': 'Value',\n                            'npv': 'NPV',\n                            'irr': 'IRR'\n                        },\n                        use_container_width=True,\n                        hide_index=True\n                    )\n                    \n                    st.markdown(\"---\")\n\n\ndef render_valuation_module(current_user):\n    \"\"\"Valuation Module tab with comparables analysis\"\"\"\n    \n    st.markdown(\"### ðŸ’Ž Valuation Module\")\n    st.markdown(\"Project valuation using DCF and comparable company analysis\")\n    \n    projects = ProjectManager.get_user_projects(current_user['id'])\n    \n    if not projects:\n        st.info(\"ðŸ‘‰ No projects found. Create a project in the Projects page first.\")\n        return\n    \n    project_names = [p['name'] for p in projects]\n    selected_project_name = st.selectbox(\n        \"Select Project\",\n        project_names,\n        key=\"valuation_project_select\"\n    )\n    \n    selected_project = next((p for p in projects if p['name'] == selected_project_name), None)\n    \n    if selected_project:\n        st.markdown(\"---\")\n        st.markdown(\"#### Project Parameters\")\n        \n        col_val1, col_val2, col_val3 = st.columns(3)\n        \n        with col_val1:\n            npv = st.number_input(\"NPV (USD millions)\", value=250.0, key=\"val_npv\")\n            resource_tonnes = st.number_input(\"Total Resource (million tonnes)\", value=10.0, key=\"val_resource\")\n        \n        with col_val2:\n            annual_production = st.number_input(\"Annual Production (tonnes)\", value=500000.0, key=\"val_production\")\n            commodity = st.selectbox(\"Commodity\", [\"Gold\", \"Silver\", \"Copper\", \"Lithium\", \"Iron Ore\"], key=\"val_commodity\")\n        \n        with col_val3:\n            project_stage = st.selectbox(\"Project Stage\", [\"Exploration\", \"Development\", \"Production\"], key=\"val_stage\")\n            region = st.selectbox(\"Region\", [\"North America\", \"South America\", \"Africa\", \"Asia\", \"Australia\"], key=\"val_region\")\n        \n        st.markdown(\"---\")\n        \n        if st.button(\"ðŸ“Š Calculate Valuation\", type=\"primary\", use_container_width=True):\n            \n            with st.spinner(\"Calculating valuation...\"):\n                engine = FinancialEngine()\n                comparables_mgr = ComparablesManager()\n                \n                comparable_multiples = {\n                    'ev_per_resource': 25.0,\n                    'ev_per_production': 500.0\n                }\n                \n                valuation = engine.calculate_project_valuation(\n                    npv=npv,\n                    resource_tonnes=resource_tonnes * 1000000,\n                    annual_production=annual_production,\n                    comparable_multiples=comparable_multiples\n                )\n                \n                st.success(\"âœ… Valuation complete!\")\n                \n                st.markdown(\"### ðŸ’° Valuation Results\")\n                \n                col_result1, col_result2, col_result3 = st.columns(3)\n                \n                with col_result1:\n                    st.metric(\n                        label=\"DCF Valuation\",\n                        value=f\"${valuation['dcf_valuation']:,.2f}M\",\n                        delta=\"Net Present Value\"\n                    )\n                \n                with col_result2:\n                    st.metric(\n                        label=\"Value per Tonne Resource\",\n                        value=f\"${valuation['resource_value_per_tonne']:.2f}\",\n                        delta=\"Resource Multiple\"\n                    )\n                \n                with col_result3:\n                    st.metric(\n                        label=\"Production Multiple\",\n                        value=f\"${valuation['production_multiple']:,.2f}\",\n                        delta=\"Per tonne annual production\"\n                    )\n                \n                st.markdown(\"---\")\n                \n                st.markdown(\"### ðŸŒ Comparable Valuations\")\n                \n                if 'comp_valuation_resource' in valuation:\n                    col_comp1, col_comp2 = st.columns(2)\n                    \n                    with col_comp1:\n                        st.metric(\n                            label=\"Comparable Valuation (Resource)\",\n                            value=f\"${valuation.get('comp_valuation_resource', 0):,.2f}M\",\n                            delta=f\"Based on industry multiples\"\n                        )\n                    \n                    with col_comp2:\n                        st.metric(\n                            label=\"Comparable Valuation (Production)\",\n                            value=f\"${valuation.get('comp_valuation_production', 0):,.2f}M\",\n                            delta=f\"Based on production metrics\"\n                        )\n                \n                st.markdown(\"---\")\n                \n                st.markdown(\"### ðŸ“‹ Valuation Summary\")\n                \n                avg_valuation = (\n                    valuation['dcf_valuation'] + \n                    valuation.get('comp_valuation_resource', valuation['dcf_valuation']) + \n                    valuation.get('comp_valuation_production', valuation['dcf_valuation'])\n                ) / 3\n                \n                st.info(f\"\"\"\n                **Average Valuation:** ${avg_valuation:,.2f}M\n                \n                **Valuation Range:** ${min(valuation['dcf_valuation'], valuation.get('comp_valuation_resource', valuation['dcf_valuation'])):,.2f}M - ${max(valuation['dcf_valuation'], valuation.get('comp_valuation_resource', valuation['dcf_valuation'])):,.2f}M\n                \n                **Methodology:** DCF analysis combined with comparable company multiples\n                \"\"\")\n\n\ndef render_market_data(current_user):\n    \"\"\"Market Data tab with real-time commodity prices\"\"\"\n    \n    st.markdown(\"### ðŸŒ Market Data - Real-Time Commodity Prices\")\n    st.markdown(\"Live commodity pricing from Metals-API with 1-hour caching\")\n    \n    market_data = get_market_data_provider()\n    \n    col_refresh1, col_refresh2 = st.columns([1, 3])\n    \n    with col_refresh1:\n        use_cache = st.checkbox(\"Use Cached Data\", value=True, key=\"market_use_cache\")\n    \n    with col_refresh2:\n        if st.button(\"ðŸ”„ Refresh Prices\", type=\"primary\"):\n            use_cache = False\n    \n    st.markdown(\"---\")\n    \n    with st.spinner(\"Fetching commodity prices...\"):\n        commodities_data = market_data.get_all_mining_commodities(use_cache=use_cache)\n    \n    if commodities_data:\n        st.markdown(\"### ðŸ’µ Current Commodity Prices\")\n        \n        price_data = []\n        for commodity, data in commodities_data.items():\n            price_data.append({\n                'Commodity': commodity.title(),\n                'Price': f\"${data['price']:,.2f}\",\n                'Unit': data['unit'],\n                'Source': data['source'],\n                '24h Change': f\"{data.get('change_pct_24h', 0):.2f}%\" if data.get('change_pct_24h') else \"N/A\",\n                'Last Updated': data.get('fetched_at', 'N/A')[:19],\n                'Cached': 'âœ…' if data.get('from_cache') else 'ðŸ”´'\n            })\n        \n        price_df = pd.DataFrame(price_data)\n        \n        st.dataframe(\n            price_df,\n            use_container_width=True,\n            height=400,\n            column_config={\n                'Commodity': st.column_config.TextColumn('Commodity', width='medium'),\n                'Price': st.column_config.TextColumn('Price', width='medium'),\n                'Unit': st.column_config.TextColumn('Unit', width='medium'),\n                '24h Change': st.column_config.TextColumn('24h Change', width='small'),\n                'Source': st.column_config.TextColumn('Source', width='medium'),\n                'Cached': st.column_config.TextColumn('Cache', width='small')\n            }\n        )\n        \n        st.markdown(\"---\")\n        \n        st.markdown(\"### ðŸ” Individual Commodity Lookup\")\n        \n        col_lookup1, col_lookup2 = st.columns([2, 1])\n        \n        with col_lookup1:\n            lookup_commodity = st.selectbox(\n                \"Select Commodity\",\n                [\"gold\", \"silver\", \"copper\", \"lithium\", \"platinum\", \"palladium\", \"zinc\", \"nickel\"],\n                key=\"market_lookup\"\n            )\n        \n        with col_lookup2:\n            if st.button(\"ðŸ”Ž Get Price\", use_container_width=True):\n                commodity_data = market_data.get_commodity_price(lookup_commodity, use_cache=use_cache)\n                \n                if commodity_data:\n                    col_price1, col_price2, col_price3 = st.columns(3)\n                    \n                    with col_price1:\n                        st.metric(\n                            label=f\"{lookup_commodity.title()} Price\",\n                            value=f\"${commodity_data['price']:,.2f}\",\n                            delta=commodity_data['unit']\n                        )\n                    \n                    with col_price2:\n                        st.metric(\n                            label=\"24h Change\",\n                            value=f\"{commodity_data.get('change_pct_24h', 0):.2f}%\" if commodity_data.get('change_pct_24h') else \"N/A\",\n                            delta=\"Percentage change\"\n                        )\n                    \n                    with col_price3:\n                        st.metric(\n                            label=\"Data Source\",\n                            value=commodity_data['source'],\n                            delta=\"From cache\" if commodity_data.get('from_cache') else \"Live API\"\n                        )\n        \n        st.markdown(\"---\")\n        \n        st.info(\"\"\"\n        **ðŸ’¡ Data Source Information:**\n        \n        - **Free Tier:** 100 API calls per month via Metals-API\n        - **Cache Duration:** 1 hour (to minimize API usage)\n        - **Mock Data:** Displayed when API key is not configured\n        - **To enable live data:** Add your Metals-API key as `METALS_API_KEY` in secrets\n        \n        Get your free API key at: https://metals-api.com\n        \"\"\")\n    else:\n        st.warning(\"âš ï¸ Unable to fetch commodity prices. Please check your API configuration.\")\n","path":null,"size_bytes":29591,"size_tokens":null},"comparables_matcher.py":{"content":"from typing import List, Dict, Any, Optional\nfrom database import get_db_session\nfrom models import ComparableProject, ComparableMatch, Analysis\nfrom sqlalchemy import and_, or_\n\n\nclass ComparablesMatchingService:\n    \"\"\"Service for matching projects to comparable mining projects from the database\"\"\"\n    \n    MATCHING_WEIGHTS = {\n        'commodity': 0.35,\n        'deposit_style': 0.25,\n        'development_stage': 0.20,\n        'jurisdiction': 0.10,\n        'scale': 0.10\n    }\n    \n    @staticmethod\n    def _normalize_commodity(commodity: str) -> str:\n        \"\"\"Normalize commodity names for matching\"\"\"\n        if not commodity:\n            return ''\n        commodity = commodity.lower().strip()\n        \n        commodity_groups = {\n            'gold': ['au', 'gold'],\n            'copper': ['cu', 'copper'],\n            'silver': ['ag', 'silver'],\n            'lithium': ['li', 'lithium', 'spodumene', 'pegmatite'],\n            'zinc': ['zn', 'zinc'],\n            'nickel': ['ni', 'nickel'],\n            'lead': ['pb', 'lead'],\n            'iron': ['fe', 'iron', 'iron ore'],\n            'platinum': ['pt', 'platinum', 'pgm', 'pge'],\n            'rare earth': ['ree', 'rare earth', 'neodymium', 'praseodymium'],\n            'uranium': ['u', 'uranium', 'u3o8'],\n        }\n        \n        for group, variants in commodity_groups.items():\n            if any(var in commodity for var in variants):\n                return group\n        \n        return commodity\n    \n    @staticmethod\n    def _extract_project_attributes(analysis_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Extract matching attributes from analysis data\"\"\"\n        attributes = {\n            'commodity': '',\n            'deposit_style': '',\n            'development_stage': 'exploration',\n            'country': '',\n            'resource_size': None\n        }\n        \n        categories = analysis_data.get('categories', {})\n        \n        all_facts = []\n        for cat_key, cat_data in categories.items():\n            if isinstance(cat_data, dict):\n                all_facts.extend(cat_data.get('facts_found', []))\n        \n        facts_text = ' '.join(all_facts).lower()\n        \n        for commodity_type in ['gold', 'copper', 'silver', 'lithium', 'zinc', 'nickel', 'iron', 'platinum', 'uranium']:\n            if commodity_type in facts_text:\n                attributes['commodity'] = commodity_type\n                break\n        \n        deposit_keywords = {\n            'porphyry': ['porphyry'],\n            'epithermal': ['epithermal'],\n            'orogenic': ['orogenic', 'orogenic gold'],\n            'vms': ['vms', 'volcanogenic', 'massive sulfide'],\n            'skarn': ['skarn'],\n            'irgs': ['irgs', 'intrusion-related'],\n            'sedex': ['sedex', 'sedimentary exhalative']\n        }\n        \n        for deposit_type, keywords in deposit_keywords.items():\n            if any(kw in facts_text for kw in keywords):\n                attributes['deposit_style'] = deposit_type\n                break\n        \n        if 'production' in facts_text or 'operating' in facts_text or 'producing' in facts_text:\n            attributes['development_stage'] = 'production'\n        elif 'feasibility' in facts_text or 'pfs' in facts_text or 'dfs' in facts_text:\n            attributes['development_stage'] = 'development'\n        elif 'resource' in facts_text or 'mre' in facts_text or 'indicated' in facts_text or 'inferred' in facts_text:\n            attributes['development_stage'] = 'resource'\n        else:\n            attributes['development_stage'] = 'exploration'\n        \n        for country in ['canada', 'usa', 'australia', 'chile', 'peru', 'mexico', 'brazil', 'south africa']:\n            if country in facts_text:\n                attributes['country'] = country\n                break\n        \n        import re\n        tonnage_patterns = [\n            r'(\\d+\\.?\\d*)\\s*(?:million|m)\\s*(?:tonnes?|tons?|mt)',\n            r'(\\d+\\.?\\d*)\\s*moz',\n            r'(\\d+\\.?\\d*)\\s*(?:billion|b)\\s*(?:tonnes?|tons?)',\n        ]\n        for pattern in tonnage_patterns:\n            match = re.search(pattern, facts_text, re.IGNORECASE)\n            if match:\n                try:\n                    value = float(match.group(1))\n                    if 'billion' in facts_text[match.start():match.end()].lower():\n                        value *= 1000\n                    if 'moz' in facts_text[match.start():match.end()].lower():\n                        value *= 0.031\n                    attributes['resource_size'] = value\n                    break\n                except:\n                    pass\n        \n        return attributes\n    \n    @staticmethod\n    def _calculate_similarity(project_attrs: Dict[str, Any], comparable: ComparableProject) -> float:\n        \"\"\"Calculate similarity score between project and comparable\"\"\"\n        score = 0.0\n        \n        if project_attrs['commodity'] and comparable.commodity:\n            proj_commodity = ComparablesMatchingService._normalize_commodity(project_attrs['commodity'])\n            comp_commodity = ComparablesMatchingService._normalize_commodity(comparable.commodity)\n            if proj_commodity == comp_commodity:\n                score += ComparablesMatchingService.MATCHING_WEIGHTS['commodity']\n            elif proj_commodity in comp_commodity or comp_commodity in proj_commodity:\n                score += ComparablesMatchingService.MATCHING_WEIGHTS['commodity'] * 0.5\n        \n        if project_attrs['deposit_style'] and comparable.deposit_style:\n            if project_attrs['deposit_style'].lower() in comparable.deposit_style.lower():\n                score += ComparablesMatchingService.MATCHING_WEIGHTS['deposit_style']\n        elif project_attrs['deposit_style'] and comparable.geology_type:\n            if project_attrs['deposit_style'].lower() in comparable.geology_type.lower():\n                score += ComparablesMatchingService.MATCHING_WEIGHTS['deposit_style'] * 0.7\n        \n        stage_mapping = {\n            'exploration': 1,\n            'resource': 2,\n            'development': 3,\n            'production': 4\n        }\n        proj_stage = stage_mapping.get(project_attrs['development_stage'], 1)\n        comp_stage = stage_mapping.get(comparable.project_stage, 1) if comparable.project_stage else 1\n        \n        stage_diff = abs(proj_stage - comp_stage)\n        if stage_diff == 0:\n            score += ComparablesMatchingService.MATCHING_WEIGHTS['development_stage']\n        elif stage_diff == 1:\n            score += ComparablesMatchingService.MATCHING_WEIGHTS['development_stage'] * 0.6\n        elif stage_diff == 2:\n            score += ComparablesMatchingService.MATCHING_WEIGHTS['development_stage'] * 0.3\n        \n        if project_attrs['country'] and comparable.country:\n            if project_attrs['country'].lower() in comparable.country.lower():\n                score += ComparablesMatchingService.MATCHING_WEIGHTS['jurisdiction']\n            elif comparable.country.lower() in ['canada', 'australia', 'usa']:\n                score += ComparablesMatchingService.MATCHING_WEIGHTS['jurisdiction'] * 0.5\n        \n        if project_attrs['resource_size'] and comparable.total_resource_mt:\n            proj_size = project_attrs['resource_size']\n            comp_size = comparable.total_resource_mt\n            ratio = min(proj_size, comp_size) / max(proj_size, comp_size)\n            if ratio >= 0.5:\n                score += ComparablesMatchingService.MATCHING_WEIGHTS['scale'] * ratio\n        elif comparable.total_resource_mt and comparable.total_resource_mt > 0:\n            score += ComparablesMatchingService.MATCHING_WEIGHTS['scale'] * 0.3\n        \n        return min(score, 1.0)\n    \n    @staticmethod\n    def find_top_comparables(\n        analysis_id: int,\n        analysis_data: Dict[str, Any],\n        top_n: int = 3\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Find top N most similar comparable projects\"\"\"\n        project_attrs = ComparablesMatchingService._extract_project_attributes(analysis_data)\n        \n        with get_db_session() as session:\n            query = session.query(ComparableProject).filter(\n                ComparableProject.approved_for_display == True\n            )\n            \n            if project_attrs['commodity']:\n                normalized_commodity = ComparablesMatchingService._normalize_commodity(project_attrs['commodity'])\n                query = query.filter(\n                    or_(\n                        ComparableProject.commodity.ilike(f'%{normalized_commodity}%'),\n                        ComparableProject.commodity.ilike(f'%{project_attrs[\"commodity\"]}%'),\n                        ComparableProject.commodity_group.ilike(f'%{normalized_commodity}%')\n                    )\n                )\n            \n            comparables = query.all()\n            \n            scored_comparables = []\n            for comp in comparables:\n                similarity_score = ComparablesMatchingService._calculate_similarity(project_attrs, comp)\n                \n                if similarity_score > 0.2:\n                    scored_comparables.append({\n                        'comparable': comp,\n                        'similarity_score': similarity_score,\n                        'match_criteria': {\n                            'commodity_match': project_attrs['commodity'],\n                            'deposit_style_match': project_attrs['deposit_style'],\n                            'stage_match': project_attrs['development_stage']\n                        }\n                    })\n            \n            scored_comparables.sort(key=lambda x: x['similarity_score'], reverse=True)\n            top_comparables = scored_comparables[:top_n]\n            \n            session.query(ComparableMatch).filter(\n                ComparableMatch.analysis_id == analysis_id\n            ).delete()\n            \n            for idx, match_data in enumerate(top_comparables, 1):\n                comparable_match = ComparableMatch(\n                    analysis_id=analysis_id,\n                    comparable_id=match_data['comparable'].id,\n                    similarity_score=match_data['similarity_score'],\n                    match_criteria=match_data['match_criteria'],\n                    rank=idx\n                )\n                session.add(comparable_match)\n            \n            session.commit()\n            \n            return [\n                {\n                    'id': match['comparable'].id,\n                    'name': match['comparable'].name,\n                    'company': match['comparable'].company,\n                    'location': match['comparable'].location,\n                    'country': match['comparable'].country,\n                    'commodity': match['comparable'].commodity,\n                    'project_stage': match['comparable'].project_stage,\n                    'geology_type': match['comparable'].geology_type,\n                    'total_resource_mt': match['comparable'].total_resource_mt,\n                    'grade': match['comparable'].grade,\n                    'grade_unit': match['comparable'].grade_unit,\n                    'npv_millions_usd': match['comparable'].npv_millions_usd,\n                    'irr_percent': match['comparable'].irr_percent,\n                    'capex_millions_usd': match['comparable'].capex_millions_usd,\n                    'similarity_score': match['similarity_score'],\n                    'match_criteria': match['match_criteria']\n                }\n                for match in top_comparables\n            ]\n    \n    @staticmethod\n    def get_comparables_for_analysis(analysis_id: int) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve previously matched comparables for an analysis\"\"\"\n        with get_db_session() as session:\n            matches = session.query(ComparableMatch, ComparableProject).join(\n                ComparableProject,\n                ComparableMatch.comparable_id == ComparableProject.id\n            ).filter(\n                ComparableMatch.analysis_id == analysis_id\n            ).order_by(\n                ComparableMatch.rank\n            ).all()\n            \n            return [\n                {\n                    'id': comp.id,\n                    'name': comp.name,\n                    'company': comp.company,\n                    'location': comp.location,\n                    'country': comp.country,\n                    'commodity': comp.commodity,\n                    'project_stage': comp.project_stage,\n                    'geology_type': comp.geology_type,\n                    'total_resource_mt': comp.total_resource_mt,\n                    'grade': comp.grade,\n                    'grade_unit': comp.grade_unit,\n                    'npv_millions_usd': comp.npv_millions_usd,\n                    'irr_percent': comp.irr_percent,\n                    'capex_millions_usd': comp.capex_millions_usd,\n                    'similarity_score': match.similarity_score,\n                    'match_criteria': match.match_criteria\n                }\n                for match, comp in matches\n            ]\n","path":null,"size_bytes":13052,"size_tokens":null},"page_modules/admin_comparables_page.py":{"content":"\"\"\"\nAdmin page for managing comparables database\nOnly accessible to admin users (cokhaligzada@gmail.com)\n\"\"\"\n\nimport streamlit as st\nfrom datetime import datetime\nfrom comparables_ingestion import ComparablesIngestionService\nfrom comparables_scheduler import get_scheduler_status, trigger_manual_update, start_scheduler\n\ndef render_admin_comparables_page(current_user):\n    \"\"\"Render the admin comparables management page\"\"\"\n    \n    # Check if user is admin\n    if current_user['email'] != 'cokhaligzada@gmail.com':\n        st.error(\"â›” Access Denied - Admin privileges required\")\n        st.info(\"This page is only accessible to system administrators.\")\n        return\n    \n    st.title(\"ðŸ”§ Comparables Database Management\")\n    st.markdown(\"*Admin panel for managing the global comparables database*\")\n    \n    # Create tabs\n    tab1, tab2, tab3 = st.tabs([\"ðŸ“‹ Pending Approvals\", \"â° Scheduler Status\", \"ðŸ“Š Ingestion History\"])\n    \n    with tab1:\n        render_pending_approvals()\n    \n    with tab2:\n        render_scheduler_status()\n    \n    with tab3:\n        render_ingestion_history()\n\ndef render_pending_approvals():\n    \"\"\"Render pending project approvals\"\"\"\n    st.subheader(\"Pending Project Approvals\")\n    st.markdown(\"Review and approve new comparable projects before they appear in the public database.\")\n    \n    # Get pending projects\n    pending_projects = ComparablesIngestionService.get_pending_projects(limit=50)\n    \n    if not pending_projects:\n        st.info(\"âœ… No projects pending approval\")\n        return\n    \n    st.markdown(f\"**{len(pending_projects)} projects awaiting review**\")\n    \n    # Display each pending project\n    for project in pending_projects:\n        with st.expander(f\"ðŸ“ {project.name} - {project.commodity} ({project.country})\"):\n            col1, col2 = st.columns(2)\n            \n            with col1:\n                st.markdown(\"**Basic Information**\")\n                st.write(f\"**Company:** {project.company or 'N/A'}\")\n                st.write(f\"**Location:** {project.location or 'N/A'}\")\n                st.write(f\"**Country:** {project.country or 'N/A'}\")\n                st.write(f\"**Commodity:** {project.commodity or 'N/A'}\")\n                st.write(f\"**Commodity Group:** {project.commodity_group or 'N/A'}\")\n                st.write(f\"**Project Stage:** {project.project_stage or 'N/A'}\")\n                st.write(f\"**Development Stage:** {project.development_stage_detail or 'N/A'}\")\n                st.write(f\"**Deposit Style:** {project.deposit_style or 'N/A'}\")\n            \n            with col2:\n                st.markdown(\"**Technical Data**\")\n                if project.total_resource_mt:\n                    st.write(f\"**Resource:** {project.total_resource_mt:.2f} Mt\")\n                if project.grade and project.grade_unit:\n                    st.write(f\"**Grade:** {project.grade:.2f} {project.grade_unit}\")\n                if project.capex_millions_usd:\n                    st.write(f\"**CAPEX:** ${project.capex_millions_usd:.1f}M\")\n                if project.npv_millions_usd:\n                    st.write(f\"**NPV:** ${project.npv_millions_usd:.1f}M\")\n                if project.irr_percent:\n                    st.write(f\"**IRR:** {project.irr_percent:.1f}%\")\n                if project.jurisdiction_risk_band:\n                    st.write(f\"**Jurisdiction Risk:** {project.jurisdiction_risk_band}\")\n                if project.overall_score:\n                    st.write(f\"**Overall Score:** {project.overall_score:.0f}/100\")\n            \n            st.markdown(\"---\")\n            st.write(f\"**Data Source:** {project.data_source or 'Unknown'}\")\n            st.write(f\"**Created:** {project.created_at.strftime('%Y-%m-%d %H:%M') if project.created_at else 'N/A'}\")\n            \n            # Action buttons\n            col1, col2, col3 = st.columns([1, 1, 3])\n            \n            with col1:\n                if st.button(\"âœ… Approve\", key=f\"approve_{project.id}\", use_container_width=True):\n                    if ComparablesIngestionService.approve_project(project.id):\n                        st.success(f\"âœ… {project.name} approved!\")\n                        st.rerun()\n                    else:\n                        st.error(\"Failed to approve project\")\n            \n            with col2:\n                if st.button(\"âŒ Reject\", key=f\"reject_{project.id}\", use_container_width=True):\n                    if ComparablesIngestionService.reject_project(project.id):\n                        st.success(f\"ðŸ—‘ï¸ {project.name} rejected and deleted\")\n                        st.rerun()\n                    else:\n                        st.error(\"Failed to reject project\")\n\ndef render_scheduler_status():\n    \"\"\"Render scheduler status and controls\"\"\"\n    st.subheader(\"â° Weekly Update Scheduler\")\n    st.markdown(\"Automatic weekly ingestion of new comparable projects.\")\n    \n    # Start scheduler if not running\n    try:\n        start_scheduler()\n    except Exception as e:\n        st.warning(f\"Note: Scheduler initialization - {str(e)}\")\n    \n    # Get status\n    status = get_scheduler_status()\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        if status['running']:\n            st.success(\"âœ… Scheduler Active\")\n        else:\n            st.error(\"âŒ Scheduler Inactive\")\n    \n    with col2:\n        if status.get('next_run'):\n            st.info(f\"â° Next Run: {status['next_run']}\")\n        else:\n            st.info(\"â° Next Run: Not scheduled\")\n    \n    st.markdown(\"---\")\n    \n    # Scheduler configuration\n    st.markdown(\"**Schedule Configuration**\")\n    st.write(\"- **Frequency:** Weekly (Every Sunday)\")\n    st.write(\"- **Time:** 2:00 AM UTC\")\n    st.write(\"- **Action:** Fetch and add new comparable projects for admin review\")\n    \n    st.markdown(\"---\")\n    \n    # Manual trigger\n    st.markdown(\"**Manual Controls**\")\n    \n    if st.button(\"â–¶ï¸ Run Update Now\", use_container_width=True):\n        with st.spinner(\"Running manual update...\"):\n            try:\n                trigger_manual_update()\n                st.success(\"âœ… Manual update completed! Check Ingestion History for results.\")\n                st.rerun()\n            except Exception as e:\n                st.error(f\"âŒ Update failed: {str(e)}\")\n\ndef render_ingestion_history():\n    \"\"\"Render ingestion job history\"\"\"\n    st.subheader(\"ðŸ“Š Ingestion History\")\n    st.markdown(\"Recent database update jobs and their results.\")\n    \n    # Get history\n    jobs = ComparablesIngestionService.get_ingestion_history(limit=20)\n    \n    if not jobs:\n        st.info(\"No ingestion jobs found\")\n        return\n    \n    # Display as table\n    for job in jobs:\n        with st.expander(\n            f\"{'âœ…' if job.status == 'completed' else 'âŒ' if job.status == 'failed' else 'â³'} \"\n            f\"Job #{job.id} - {job.status.upper()} - {job.started_at.strftime('%Y-%m-%d %H:%M')}\"\n        ):\n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                st.metric(\"Total Records\", job.total_records or 0)\n            \n            with col2:\n                st.metric(\"Successful\", job.successful_records or 0)\n            \n            with col3:\n                st.metric(\"Failed\", job.failed_records or 0)\n            \n            if job.completed_at:\n                duration = (job.completed_at - job.started_at).total_seconds()\n                st.write(f\"**Duration:** {duration:.1f} seconds\")\n            \n            if job.error_log:\n                st.error(f\"**Error Log:**\\n{job.error_log}\")\n","path":null,"size_bytes":7550,"size_tokens":null},"comparables_ingestion.py":{"content":"\"\"\"\nComparables Database Ingestion Service\nFetches mining project data from external sources and ingests into database\n\"\"\"\n\nimport json\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nfrom database import SessionLocal\nfrom models import ComparableProject, IngestionJob, ComparableIngestion\nimport os\nfrom openai import OpenAI\n\nclass ComparablesIngestionService:\n    \"\"\"Service for ingesting comparable mining projects into the database\"\"\"\n    \n    @staticmethod\n    def create_ingestion_job() -> int:\n        \"\"\"Create a new ingestion job and return its ID\"\"\"\n        db = SessionLocal()\n        try:\n            job = IngestionJob(\n                status='in_progress',\n                started_at=datetime.utcnow(),\n                total_records=0,\n                successful_records=0,\n                failed_records=0\n            )\n            db.add(job)\n            db.commit()\n            db.refresh(job)\n            return job.id\n        finally:\n            db.close()\n    \n    @staticmethod\n    def update_job_status(job_id: int, status: str, total: int = None, \n                         successful: int = None, failed: int = None, error_log: str = None):\n        \"\"\"Update ingestion job status\"\"\"\n        db = SessionLocal()\n        try:\n            job = db.query(IngestionJob).filter(IngestionJob.id == job_id).first()\n            if job:\n                job.status = status\n                if status == 'completed' or status == 'failed':\n                    job.completed_at = datetime.utcnow()\n                if total is not None:\n                    job.total_records = total\n                if successful is not None:\n                    job.successful_records = successful\n                if failed is not None:\n                    job.failed_records = failed\n                if error_log:\n                    job.error_log = error_log\n                db.commit()\n        finally:\n            db.close()\n    \n    @staticmethod\n    def ingest_from_ai_research(job_id: int, research_query: str = \"recent mining project developments\") -> Dict[str, Any]:\n        \"\"\"\n        Use AI to research and generate comparable project data\n        This is a placeholder that uses GPT to generate realistic mining project data\n        In production, this would connect to real data sources like SEDAR+, S&P Capital IQ, etc.\n        \"\"\"\n        db = SessionLocal()\n        try:\n            # Get OpenAI client\n            api_key = os.getenv('AI_INTEGRATIONS_OPENAI_API_KEY')\n            base_url = os.getenv('AI_INTEGRATIONS_OPENAI_BASE_URL')\n            \n            if not api_key:\n                raise ValueError(\"OpenAI API key not configured\")\n            \n            client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)\n            \n            # Research prompt to generate mining project data\n            research_prompt = f\"\"\"Generate a list of 5 real or realistic mining projects for a comparables database.\nFor each project, provide:\n- name: Project name\n- company: Operating company\n- location: Geographic location\n- country: Country\n- commodity: Primary commodity (Gold, Copper, Lithium, Nickel, etc.)\n- commodity_group: Precious Metals, Base Metals, Battery Metals, Industrial Minerals\n- project_stage: exploration, development, production, closed\n- development_stage_detail: Early-Stage Exploration, Advanced Exploration, Pre-Feasibility, Feasibility, Construction, Operating, Care & Maintenance\n- deposit_style: Porphyry, Epithermal, VMS, Sediment-Hosted, Pegmatite, etc.\n- geology_type: Detailed deposit type\n- total_resource_mt: Total resource in million tonnes (realistic number)\n- grade: Average grade (realistic for commodity)\n- grade_unit: g/t, %, ppm\n- capex_millions_usd: Capital expenditure estimate\n- opex_per_tonne_usd: Operating cost per tonne\n- npv_millions_usd: Net present value\n- irr_percent: Internal rate of return\n- payback_years: Payback period\n- annual_production: Annual production estimate\n- production_unit: tonnes, oz, kg\n- mine_life_years: Mine life\n- jurisdiction_risk_band: Low Risk, Moderate Risk, High Risk, Very High Risk\n- political_risk_score: 0-10 (0=lowest risk)\n- data_source: Source of information\n- overall_score: Overall project quality score 0-100\n\nReturn ONLY valid JSON array with exactly 5 projects. No markdown, no explanations.\"\"\"\n\n            response = client.chat.completions.create(\n                model=\"gpt-5.1\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a mining industry data expert. Generate realistic mining project data in JSON format.\"},\n                    {\"role\": \"user\", \"content\": research_prompt}\n                ],\n                temperature=0.7,\n                reasoning_effort=\"high\"\n            )\n            \n            content = response.choices[0].message.content.strip()\n            \n            # Clean up markdown code blocks if present\n            if content.startswith(\"```\"):\n                content = content.split(\"```\")[1]\n                if content.startswith(\"json\"):\n                    content = content[4:]\n                content = content.strip()\n            \n            projects_data = json.loads(content)\n            \n            total_projects = len(projects_data)\n            successful = 0\n            failed = 0\n            \n            for project_data in projects_data:\n                try:\n                    # Create comparables ingestion record\n                    ingestion_record = ComparableIngestion(\n                        job_id=job_id,\n                        project_name=project_data.get('name', 'Unknown Project'),\n                        status='pending'\n                    )\n                    db.add(ingestion_record)\n                    db.flush()\n                    \n                    # Create comparable project (pending approval)\n                    comparable = ComparableProject(\n                        name=project_data.get('name'),\n                        company=project_data.get('company'),\n                        location=project_data.get('location'),\n                        country=project_data.get('country'),\n                        commodity=project_data.get('commodity'),\n                        commodity_group=project_data.get('commodity_group'),\n                        project_stage=project_data.get('project_stage'),\n                        development_stage_detail=project_data.get('development_stage_detail'),\n                        deposit_style=project_data.get('deposit_style'),\n                        geology_type=project_data.get('geology_type'),\n                        total_resource_mt=project_data.get('total_resource_mt'),\n                        grade=project_data.get('grade'),\n                        grade_unit=project_data.get('grade_unit'),\n                        capex_millions_usd=project_data.get('capex_millions_usd'),\n                        opex_per_tonne_usd=project_data.get('opex_per_tonne_usd'),\n                        npv_millions_usd=project_data.get('npv_millions_usd'),\n                        irr_percent=project_data.get('irr_percent'),\n                        payback_years=project_data.get('payback_years'),\n                        annual_production=project_data.get('annual_production'),\n                        production_unit=project_data.get('production_unit'),\n                        mine_life_years=project_data.get('mine_life_years'),\n                        jurisdiction_risk_band=project_data.get('jurisdiction_risk_band'),\n                        political_risk_score=project_data.get('political_risk_score'),\n                        overall_score=project_data.get('overall_score'),\n                        data_source=project_data.get('data_source', 'AI Research'),\n                        data_quality='medium',\n                        approved_for_display=False,  # Pending admin approval\n                        status='pending_approval'\n                    )\n                    \n                    db.add(comparable)\n                    db.flush()\n                    \n                    # Update ingestion record\n                    ingestion_record.status = 'success'\n                    successful += 1\n                    \n                except Exception as e:\n                    failed += 1\n                    if ingestion_record:\n                        ingestion_record.status = 'failed'\n                        ingestion_record.error_message = str(e)\n            \n            db.commit()\n            \n            # Update job status\n            ComparablesIngestionService.update_job_status(\n                job_id=job_id,\n                status='completed',\n                total=total_projects,\n                successful=successful,\n                failed=failed\n            )\n            \n            return {\n                'success': True,\n                'total': total_projects,\n                'successful': successful,\n                'failed': failed,\n                'job_id': job_id\n            }\n            \n        except Exception as e:\n            ComparablesIngestionService.update_job_status(\n                job_id=job_id,\n                status='failed',\n                error_log=str(e)\n            )\n            return {\n                'success': False,\n                'error': str(e),\n                'job_id': job_id\n            }\n        finally:\n            db.close()\n    \n    @staticmethod\n    def run_weekly_ingestion() -> Dict[str, Any]:\n        \"\"\"\n        Main entry point for weekly ingestion job\n        This would be called by the scheduler\n        \"\"\"\n        job_id = ComparablesIngestionService.create_ingestion_job()\n        result = ComparablesIngestionService.ingest_from_ai_research(job_id)\n        return result\n    \n    @staticmethod\n    def get_pending_projects(limit: int = 50) -> List[ComparableProject]:\n        \"\"\"Get all projects pending approval\"\"\"\n        db = SessionLocal()\n        try:\n            return db.query(ComparableProject).filter(\n                ComparableProject.approved_for_display == False,\n                ComparableProject.status == 'pending_approval'\n            ).order_by(ComparableProject.created_at.desc()).limit(limit).all()\n        finally:\n            db.close()\n    \n    @staticmethod\n    def approve_project(project_id: int) -> bool:\n        \"\"\"Approve a project for display\"\"\"\n        db = SessionLocal()\n        try:\n            project = db.query(ComparableProject).filter(\n                ComparableProject.id == project_id\n            ).first()\n            \n            if project:\n                project.approved_for_display = True\n                project.status = 'active'\n                project.updated_at = datetime.utcnow()\n                db.commit()\n                return True\n            return False\n        finally:\n            db.close()\n    \n    @staticmethod\n    def reject_project(project_id: int) -> bool:\n        \"\"\"Reject and delete a project\"\"\"\n        db = SessionLocal()\n        try:\n            project = db.query(ComparableProject).filter(\n                ComparableProject.id == project_id\n            ).first()\n            \n            if project:\n                db.delete(project)\n                db.commit()\n                return True\n            return False\n        finally:\n            db.close()\n    \n    @staticmethod\n    def get_ingestion_history(limit: int = 10) -> List[IngestionJob]:\n        \"\"\"Get recent ingestion jobs\"\"\"\n        db = SessionLocal()\n        try:\n            return db.query(IngestionJob).order_by(\n                IngestionJob.started_at.desc()\n            ).limit(limit).all()\n        finally:\n            db.close()\n","path":null,"size_bytes":11718,"size_tokens":null},"comparables_scheduler.py":{"content":"\"\"\"\nScheduler for weekly comparables database updates\n\"\"\"\n\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom apscheduler.triggers.cron import CronTrigger\nfrom comparables_ingestion import ComparablesIngestionService\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Global scheduler instance\nscheduler = None\n\ndef run_weekly_update():\n    \"\"\"Execute weekly comparables database update\"\"\"\n    try:\n        logger.info(\"Starting weekly comparables database update...\")\n        result = ComparablesIngestionService.run_weekly_ingestion()\n        \n        if result.get('success'):\n            logger.info(f\"Weekly update completed successfully. Total: {result['total']}, Success: {result['successful']}, Failed: {result['failed']}\")\n        else:\n            logger.error(f\"Weekly update failed: {result.get('error')}\")\n    except Exception as e:\n        logger.error(f\"Error during weekly update: {str(e)}\")\n\ndef start_scheduler():\n    \"\"\"Start the background scheduler for weekly updates\"\"\"\n    global scheduler\n    \n    if scheduler is not None:\n        logger.warning(\"Scheduler already running\")\n        return scheduler\n    \n    scheduler = BackgroundScheduler()\n    \n    # Schedule to run every Sunday at 2:00 AM\n    scheduler.add_job(\n        run_weekly_update,\n        CronTrigger(day_of_week='sun', hour=2, minute=0),\n        id='weekly_comparables_update',\n        name='Weekly Comparables Database Update',\n        replace_existing=True\n    )\n    \n    scheduler.start()\n    logger.info(\"Comparables update scheduler started. Next run: Sunday 2:00 AM\")\n    \n    return scheduler\n\ndef stop_scheduler():\n    \"\"\"Stop the background scheduler\"\"\"\n    global scheduler\n    \n    if scheduler is not None:\n        scheduler.shutdown()\n        scheduler = None\n        logger.info(\"Comparables update scheduler stopped\")\n\ndef trigger_manual_update():\n    \"\"\"Manually trigger an update outside the schedule\"\"\"\n    logger.info(\"Manual update triggered\")\n    return run_weekly_update()\n\ndef get_scheduler_status():\n    \"\"\"Get current scheduler status\"\"\"\n    global scheduler\n    \n    if scheduler is None:\n        return {\n            'running': False,\n            'next_run': None\n        }\n    \n    jobs = scheduler.get_jobs()\n    if jobs:\n        next_run = jobs[0].next_run_time\n        return {\n            'running': True,\n            'next_run': next_run.isoformat() if next_run else None,\n            'jobs': [{'id': j.id, 'name': j.name, 'next_run': j.next_run_time.isoformat() if j.next_run_time else None} for j in jobs]\n        }\n    \n    return {\n        'running': True,\n        'next_run': None,\n        'jobs': []\n    }\n","path":null,"size_bytes":2719,"size_tokens":null},"ai_access_control.py":{"content":"\"\"\"\nAI Access Control Utilities\nManages access control for Light AI and Advanced AI tiers\n\"\"\"\n\nfrom database import get_db_session\nfrom models import User\n\nAI_TIER_LIGHT = 'light_ai'\nAI_TIER_ADVANCED = 'advanced_ai'\nAI_TIER_BOTH = 'both'\n\nVALID_TIERS = [AI_TIER_LIGHT, AI_TIER_ADVANCED, AI_TIER_BOTH]\n\n\ndef get_user_ai_tier(user_id: int) -> str:\n    \"\"\"Get the AI tier access for a user\"\"\"\n    with get_db_session() as db:\n        user = db.query(User).filter(User.id == user_id).first()\n        if user:\n            tier = getattr(user, 'ai_tier_access', AI_TIER_LIGHT)\n            return tier if tier in VALID_TIERS else AI_TIER_LIGHT\n        return AI_TIER_LIGHT\n\n\ndef _is_admin_strict(user_data: dict) -> bool:\n    \"\"\"Strictly check if user is admin (handles string/bool serialization)\"\"\"\n    is_admin = user_data.get('is_admin')\n    if is_admin is True:\n        return True\n    if isinstance(is_admin, str) and is_admin.lower() == 'true':\n        return True\n    return False\n\n\ndef has_light_ai_access(user_data: dict) -> bool:\n    \"\"\"Check if user has access to Light AI features\"\"\"\n    if _is_admin_strict(user_data):\n        return True\n    \n    tier = user_data.get('ai_tier_access', AI_TIER_LIGHT)\n    return tier in [AI_TIER_LIGHT, AI_TIER_BOTH]\n\n\ndef has_advanced_ai_access(user_data: dict) -> bool:\n    \"\"\"Check if user has access to Advanced AI features\"\"\"\n    if _is_admin_strict(user_data):\n        return True\n    \n    tier = user_data.get('ai_tier_access', AI_TIER_LIGHT)\n    return tier in [AI_TIER_ADVANCED, AI_TIER_BOTH]\n\n\ndef has_both_access(user_data: dict) -> bool:\n    \"\"\"Check if user has access to both Light and Advanced AI\"\"\"\n    if _is_admin_strict(user_data):\n        return True\n    \n    tier = user_data.get('ai_tier_access', AI_TIER_LIGHT)\n    return tier == AI_TIER_BOTH\n\n\ndef get_user_ai_features(user_data: dict) -> dict:\n    \"\"\"Get detailed AI feature access for a user\"\"\"\n    is_admin = _is_admin_strict(user_data)\n    tier = user_data.get('ai_tier_access', AI_TIER_LIGHT)\n    \n    if is_admin:\n        tier = AI_TIER_BOTH\n    \n    light_access = tier in [AI_TIER_LIGHT, AI_TIER_BOTH] or is_admin\n    advanced_access = tier in [AI_TIER_ADVANCED, AI_TIER_BOTH] or is_admin\n    \n    return {\n        'tier': tier,\n        'is_admin': is_admin,\n        'light_ai': {\n            'enabled': light_access,\n            'features': {\n                'document_analysis': light_access,\n                'investment_scoring': light_access,\n                'sustainability_scoring': light_access,\n                'pdf_report_generation': light_access,\n                'comparables_matching': light_access,\n                'template_management': light_access,\n                'project_management': light_access,\n            }\n        },\n        'advanced_ai': {\n            'enabled': advanced_access,\n            'features': {\n                'market_multiples_analysis': advanced_access,\n                'ev_resource_benchmarking': advanced_access,\n                'kilburn_method': advanced_access,\n                'pwc_cost_approach': advanced_access,\n                'monte_carlo_simulation': advanced_access,\n                'advanced_valuation_report': advanced_access,\n                'risk_modeling': advanced_access,\n                'financial_valuation': advanced_access,\n            }\n        }\n    }\n\n\ndef get_tier_display_name(tier: str) -> str:\n    \"\"\"Get human-readable name for AI tier\"\"\"\n    tier_names = {\n        AI_TIER_LIGHT: 'ðŸ”µ Light AI',\n        AI_TIER_ADVANCED: 'ðŸŸ£ Advanced AI',\n        AI_TIER_BOTH: 'ðŸŸ¢ Full Access (Light + Advanced)'\n    }\n    return tier_names.get(tier, 'ðŸ”µ Light AI')\n\n\ndef get_tier_description(tier: str) -> str:\n    \"\"\"Get description for AI tier\"\"\"\n    descriptions = {\n        AI_TIER_LIGHT: 'Standard document analysis, dual scoring (Investment + Sustainability), PDF reports, comparables matching, and template management.',\n        AI_TIER_ADVANCED: 'PwC-style valuation methods including Market Multiples Analysis, Kilburn Method (Cost Approach), and Monte Carlo Risk Modeling.',\n        AI_TIER_BOTH: 'Complete access to all Light AI and Advanced AI features for comprehensive mining due diligence analysis.'\n    }\n    return descriptions.get(tier, descriptions[AI_TIER_LIGHT])\n\n\ndef update_user_ai_tier(user_id: int, new_tier: str) -> bool:\n    \"\"\"Update user's AI tier access\"\"\"\n    if new_tier not in VALID_TIERS:\n        return False\n    \n    with get_db_session() as db:\n        user = db.query(User).filter(User.id == user_id).first()\n        if user:\n            user.ai_tier_access = new_tier\n            db.commit()\n            return True\n    return False\n\n\ndef get_upgrade_message(current_tier: str) -> str:\n    \"\"\"Get upgrade message based on current tier\"\"\"\n    if current_tier == AI_TIER_LIGHT:\n        return \"Upgrade to Advanced AI to access PwC-style valuation methods, Monte Carlo simulations, and market multiples analysis.\"\n    elif current_tier == AI_TIER_ADVANCED:\n        return \"Your plan includes Advanced AI features. Contact admin to add Light AI access for standard analysis features.\"\n    return \"\"\n\n\nLIGHT_AI_FEATURES = [\n    {\n        'name': 'Document Analysis',\n        'description': 'GPT-5.1 powered document extraction and analysis',\n        'icon': 'ðŸ“„'\n    },\n    {\n        'name': 'Investment Scoring',\n        'description': 'Dual 0-100 scoring across 6 weighted categories',\n        'icon': 'ðŸ“Š'\n    },\n    {\n        'name': 'Sustainability Scoring',\n        'description': 'ESG analysis across Environmental, Social, Governance, Climate',\n        'icon': 'ðŸŒ±'\n    },\n    {\n        'name': 'PDF Reports',\n        'description': 'Professional due diligence report generation',\n        'icon': 'ðŸ“‘'\n    },\n    {\n        'name': 'Comparables Matching',\n        'description': 'Intelligent project benchmarking from global database',\n        'icon': 'ðŸ”'\n    },\n    {\n        'name': 'Template Management',\n        'description': 'Custom scoring templates with adjustable weights',\n        'icon': 'âš™ï¸'\n    }\n]\n\nADVANCED_AI_FEATURES = [\n    {\n        'name': 'Market Multiples Analysis',\n        'description': 'EV/Resource calculations, peer benchmarking, implied valuations',\n        'icon': 'ðŸ“ˆ'\n    },\n    {\n        'name': 'PwC Cost Approach (Kilburn Method)',\n        'description': 'Geoscientific rating, PEM multipliers, appraised value calculation',\n        'icon': 'ðŸ›ï¸'\n    },\n    {\n        'name': 'Monte Carlo Risk Modeling',\n        'description': 'Commodity price simulation, NPV distribution, VaR metrics',\n        'icon': 'ðŸŽ²'\n    },\n    {\n        'name': 'EV/Resource Benchmarking',\n        'description': 'Enterprise value per resource ounce comparison vs peers',\n        'icon': 'âš–ï¸'\n    },\n    {\n        'name': 'Financial Valuation Reports',\n        'description': 'Comprehensive valuation reports with multiple methodologies',\n        'icon': 'ðŸ’°'\n    },\n    {\n        'name': 'Risk-Adjusted NPV',\n        'description': 'P10/P50/P90 NPV scenarios with probability analysis',\n        'icon': 'ðŸ“‰'\n    }\n]\n","path":null,"size_bytes":7089,"size_tokens":null},"page_modules/advanced_ai_page.py":{"content":"\"\"\"\nOreplot Advanced Valuation Page\nAI-powered document analysis with 5 professional valuation methodologies\nDesign matches Light AI with project inputs and document upload\n\"\"\"\n\nimport streamlit as st\nimport io\nfrom datetime import datetime\nfrom typing import Dict, Any, List\n\nfrom ai_access_control import (\n    has_advanced_ai_access, \n    get_tier_display_name,\n    ADVANCED_AI_FEATURES,\n    get_upgrade_message\n)\nfrom document_extractor import DocumentExtractor\nfrom advanced_ai_analyzer import AdvancedAIAnalyzer\nfrom format_utils import format_currency\nfrom report_generator import ReportGenerator\n\n\ndef render_advanced_ai_page(current_user: dict):\n    \"\"\"Render the Oreplot Advanced analysis page with document upload and valuation\"\"\"\n    \n    if not has_advanced_ai_access(current_user):\n        render_access_denied(current_user)\n        return\n    \n    st.markdown(\"\"\"\n        <h1 style='background: linear-gradient(135deg, #8B5CF6 0%, #EC4899 100%); \n                   -webkit-background-clip: text; -webkit-text-fill-color: transparent; \n                   font-size: 2.5rem; font-weight: 700; margin-bottom: 0.5rem;'>\n            ðŸŸ£ Oreplot Advanced Valuation Agent\n        </h1>\n    \"\"\", unsafe_allow_html=True)\n    \n    st.markdown(\"\"\"\n        <p style='color: #64748B; font-size: 1.1rem; margin-bottom: 1.5rem;'>\n            Upload your data room documents and let AI calculate project value using 5 professional valuation methodologies\n        </p>\n    \"\"\", unsafe_allow_html=True)\n    \n    if 'advanced_view_mode' not in st.session_state:\n        st.session_state.advanced_view_mode = 'new_analysis'\n    if 'advanced_analysis_result' not in st.session_state:\n        st.session_state.advanced_analysis_result = None\n    if 'advanced_extracted_docs' not in st.session_state:\n        st.session_state.advanced_extracted_docs = None\n    \n    col1, col2 = st.columns(2)\n    with col1:\n        if st.button(\"ðŸ“„ New Analysis\", use_container_width=True,\n                    type=\"primary\" if st.session_state.advanced_view_mode == 'new_analysis' else \"secondary\"):\n            st.session_state.advanced_view_mode = 'new_analysis'\n            st.rerun()\n    with col2:\n        if st.button(\"ðŸ“Š View Results\", use_container_width=True,\n                    type=\"primary\" if st.session_state.advanced_view_mode == 'results' else \"secondary\",\n                    disabled=st.session_state.advanced_analysis_result is None):\n            st.session_state.advanced_view_mode = 'results'\n            st.rerun()\n    \n    st.markdown(\"---\")\n    \n    if st.session_state.advanced_view_mode == 'new_analysis':\n        render_new_analysis_form(current_user)\n    elif st.session_state.advanced_view_mode == 'results':\n        render_analysis_results(current_user)\n\n\ndef render_new_analysis_form(current_user: dict):\n    \"\"\"Render the new analysis form with project inputs and document upload\"\"\"\n    \n    st.markdown(\"\"\"\n        <div style='background: linear-gradient(135deg, #8B5CF6 0%, #A855F7 50%, #EC4899 100%); \n                    padding: 20px; border-radius: 12px; margin-bottom: 20px;'>\n            <h3 style='color: white; margin: 0;'>ðŸŽ¯ Advanced Valuation Analysis</h3>\n            <p style='color: rgba(255,255,255,0.9); margin: 5px 0 0 0;'>\n                Upload data room documents for comprehensive AI-powered valuation\n            </p>\n        </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    render_valuation_methods_info()\n    \n    st.markdown(\"### ðŸ“‹ Project Information\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        project_name = st.text_input(\n            \"Project Name *\",\n            placeholder=\"e.g., Golden Ridge Gold Project\",\n            key=\"adv_project_name\",\n            help=\"Enter the name of the mining project\"\n        )\n    \n    with col2:\n        primary_commodity = st.selectbox(\n            \"Primary Commodity\",\n            [\"Gold\", \"Silver\", \"Copper\", \"Lithium\", \"Nickel\", \"Zinc\", \"Uranium\", \"Platinum\", \"Other\"],\n            key=\"adv_commodity\",\n            help=\"Select the primary commodity\"\n        )\n    \n    project_description = st.text_area(\n        \"Project Description (Optional)\",\n        placeholder=\"Brief description of the project, location, and development stage...\",\n        height=100,\n        key=\"adv_description\"\n    )\n    \n    st.markdown(\"---\")\n    st.markdown(\"### ðŸ“ Upload Data Room Documents\")\n    \n    st.markdown(\"\"\"\n        <div style='background: #F0F9FF; padding: 15px; border-radius: 10px; \n                    border-left: 4px solid #0284C7; margin-bottom: 15px;'>\n            <p style='color: #0369A1; margin: 0; font-size: 0.95rem;'>\n                <strong>Supported Documents:</strong> PDF (NI 43-101, JORC, Feasibility Studies), \n                DOCX, XLSX, CSV, TXT â€¢ <strong>Maximum:</strong> 5GB total upload\n            </p>\n        </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    uploaded_files = st.file_uploader(\n        \"Upload Technical Reports & Financial Data\",\n        type=['pdf', 'docx', 'doc', 'xlsx', 'xls', 'csv', 'txt', 'jpg', 'jpeg', 'png'],\n        accept_multiple_files=True,\n        key=\"adv_uploaded_files\",\n        help=\"Upload NI 43-101, JORC reports, feasibility studies, financial models, resource estimates\"\n    )\n    \n    if uploaded_files:\n        st.markdown(f\"**{len(uploaded_files)} file(s) selected:**\")\n        total_size = sum(f.size for f in uploaded_files)\n        \n        for f in uploaded_files[:10]:\n            size_mb = f.size / (1024 * 1024)\n            st.markdown(f\"- `{f.name}` ({size_mb:.2f} MB)\")\n        \n        if len(uploaded_files) > 10:\n            st.markdown(f\"*...and {len(uploaded_files) - 10} more files*\")\n        \n        st.markdown(f\"**Total size:** {total_size / (1024 * 1024):.2f} MB\")\n    \n    st.markdown(\"---\")\n    \n    col1, col2 = st.columns([3, 1])\n    \n    with col1:\n        run_analysis = st.button(\n            \"ðŸš€ Generate Advanced Valuation Analysis\",\n            type=\"primary\",\n            use_container_width=True,\n            disabled=not (project_name and uploaded_files)\n        )\n    \n    with col2:\n        if st.button(\"ðŸ”„ Clear\", use_container_width=True):\n            st.session_state.advanced_analysis_result = None\n            st.session_state.advanced_extracted_docs = None\n            st.rerun()\n    \n    if not project_name:\n        st.info(\"ðŸ’¡ Enter a project name and upload documents to generate analysis\")\n    elif not uploaded_files:\n        st.info(\"ðŸ’¡ Upload at least one document to generate analysis\")\n    \n    if run_analysis and project_name and uploaded_files:\n        run_advanced_analysis(project_name, project_description, primary_commodity, uploaded_files, current_user)\n\n\ndef render_valuation_methods_info():\n    \"\"\"Render information about the 5 valuation methodologies\"\"\"\n    \n    with st.expander(\"ðŸ“š 5 Professional Valuation Methodologies\", expanded=False):\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.markdown(\"\"\"\n            **1. Probability-Weighted DCF (Risk-Adjusted NPV)**\n            - DCF multiplied by stage-gate success probabilities\n            - Incorporates technical, geological, and execution risks\n            \n            **2. Monte Carlo Risk Modeling**\n            - 10,000+ simulations with varying prices, grades, costs\n            - Generates P10/P50/P90 distribution with VaR metrics\n            \n            **3. Income Approach - Discounted Cash Flow**\n            - Cash flow projection based on production & costs\n            - Represents intrinsic economic value under base case\n            \"\"\")\n        \n        with col2:\n            st.markdown(\"\"\"\n            **4. Cost Approach (Kilburn Method)**\n            - Values early-stage exploration based on replacement cost\n            - Considers past exploration spend and discovery potential\n            \n            **5. Decision Tree / Stage-Gate (EMV)**\n            - Sequential decision model with costs and probabilities\n            - Expected Monetary Value at each project stage\n            \"\"\")\n\n\ndef run_advanced_analysis(project_name: str, description: str, commodity: str, \n                          uploaded_files: List, current_user: dict):\n    \"\"\"Run the complete advanced valuation analysis\"\"\"\n    \n    progress_bar = st.progress(0, text=\"Initializing analysis...\")\n    status_container = st.container()\n    \n    try:\n        with status_container:\n            st.info(\"ðŸ“„ **Step 1/4:** Extracting text from documents...\")\n        progress_bar.progress(10, text=\"Extracting document text...\")\n        \n        extracted_docs = []\n        for i, file in enumerate(uploaded_files):\n            file_bytes = file.read()\n            file.seek(0)\n            \n            progress_bar.progress(\n                10 + int(40 * (i + 1) / len(uploaded_files)),\n                text=f\"Processing: {file.name}...\"\n            )\n            \n            result = DocumentExtractor.extract_text(file.name, file_bytes)\n            extracted_docs.append(result)\n        \n        st.session_state.advanced_extracted_docs = extracted_docs\n        \n        successful_docs = [d for d in extracted_docs if d.get('success')]\n        \n        with status_container:\n            st.success(f\"âœ… Successfully extracted {len(successful_docs)}/{len(uploaded_files)} documents\")\n        \n        if not successful_docs:\n            st.error(\"âŒ No documents could be processed. Please check file formats.\")\n            progress_bar.empty()\n            return\n        \n        with status_container:\n            st.info(\"ðŸ¤– **Step 2/4:** AI extracting financial and technical data...\")\n        progress_bar.progress(55, text=\"AI analyzing document content...\")\n        \n        analysis_result = AdvancedAIAnalyzer.run_complete_analysis(extracted_docs)\n        \n        if 'error' in analysis_result and 'valuations' not in analysis_result:\n            st.error(f\"âŒ Analysis error: {analysis_result['error']}\")\n            progress_bar.empty()\n            return\n        \n        with status_container:\n            st.info(\"ðŸ“Š **Step 3/4:** Running 5 valuation methodologies...\")\n        progress_bar.progress(80, text=\"Calculating valuations...\")\n        \n        analysis_result['project_name'] = project_name\n        analysis_result['project_description'] = description\n        analysis_result['primary_commodity'] = commodity\n        analysis_result['uploaded_files'] = [f.name for f in uploaded_files]\n        analysis_result['analysis_date'] = datetime.now().isoformat()\n        analysis_result['user_id'] = current_user.get('id')\n        \n        with status_container:\n            st.info(\"ðŸ“ **Step 4/4:** Generating valuation narrative...\")\n        progress_bar.progress(95, text=\"Generating narrative...\")\n        \n        st.session_state.advanced_analysis_result = analysis_result\n        \n        progress_bar.progress(100, text=\"Analysis complete!\")\n        \n        with status_container:\n            st.success(\"ðŸŽ‰ **Advanced Valuation Analysis Complete!**\")\n        \n        st.session_state.advanced_view_mode = 'results'\n        st.rerun()\n        \n    except Exception as e:\n        st.error(f\"âŒ Analysis failed: {str(e)}\")\n        progress_bar.empty()\n\n\ndef render_analysis_results(current_user: dict):\n    \"\"\"Render the analysis results with all valuation methods\"\"\"\n    \n    result = st.session_state.advanced_analysis_result\n    \n    if not result:\n        st.warning(\"No analysis results available. Please run a new analysis.\")\n        return\n    \n    project_name = result.get('project_name', 'Mining Project')\n    summary = result.get('summary', {})\n    valuations = result.get('valuations', {})\n    extracted = result.get('extracted_data', {})\n    narrative = result.get('narrative', {})\n    \n    st.markdown(f\"\"\"\n        <div style='background: linear-gradient(135deg, #8B5CF6 0%, #EC4899 100%); \n                    padding: 25px; border-radius: 15px; margin-bottom: 20px;'>\n            <h2 style='color: white; margin: 0 0 10px 0;'>ðŸ“Š {project_name}</h2>\n            <p style='color: rgba(255,255,255,0.9); margin: 0;'>\n                Advanced Valuation Analysis â€¢ {summary.get('commodity', 'Unknown')} â€¢ \n                {summary.get('stage', 'Unknown')} Stage\n            </p>\n        </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    render_valuation_summary(summary, extracted)\n    \n    # Show derivations if any values were calculated from other fields\n    derivations = result.get('derivations', [])\n    if derivations:\n        with st.expander(\"ðŸ“ Data Derivations (values calculated from extracted data)\"):\n            for d in derivations:\n                st.markdown(f\"- {d}\")\n    \n    # Show missing inputs report if there are issues\n    missing_report = result.get('missing_inputs_report', {})\n    if missing_report:\n        with st.expander(\"âš ï¸ Missing Data for Some Valuations\", expanded=True):\n            st.markdown(\"**The following data is needed for complete valuations:**\")\n            for method, missing_items in missing_report.items():\n                st.markdown(f\"**{method}:** {', '.join(missing_items)}\")\n            st.markdown(\"\"\"\n            **Tip:** Upload a document that contains:\n            - Annual production rate (oz/year, tonnes/year)\n            - Commodity price assumption ($/oz, $/tonne)\n            - Operating cost or AISC ($/oz, $/tonne)\n            \"\"\")\n    \n    if narrative.get('executive_summary'):\n        st.markdown(\"### ðŸ“ Executive Summary\")\n        st.markdown(narrative['executive_summary'])\n    \n    st.markdown(\"---\")\n    st.markdown(\"### ðŸ“ˆ Detailed Valuation Results\")\n    \n    tabs = st.tabs([\n        \"ðŸŽ¯ Probability DCF\",\n        \"ðŸ’° Income DCF\",\n        \"ðŸŽ² Monte Carlo\",\n        \"ðŸ”¬ Kilburn Method\",\n        \"ðŸŒ³ Decision Tree EMV\"\n    ])\n    \n    with tabs[0]:\n        render_probability_dcf_results(valuations.get('probability_dcf', {}))\n    \n    with tabs[1]:\n        render_income_dcf_results(valuations.get('income_dcf', {}))\n    \n    with tabs[2]:\n        render_monte_carlo_results(valuations.get('monte_carlo', {}))\n    \n    with tabs[3]:\n        render_kilburn_results(valuations.get('kilburn', {}))\n    \n    with tabs[4]:\n        render_decision_tree_results(valuations.get('decision_tree', {}))\n    \n    st.markdown(\"---\")\n    render_download_section(result, current_user)\n\n\ndef render_valuation_summary(summary: Dict, extracted: Dict):\n    \"\"\"Render the valuation summary metrics\"\"\"\n    \n    val_range = summary.get('valuation_range', {})\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\n            \"Valuation Range\",\n            f\"${val_range.get('low', 0):.0f}M - ${val_range.get('high', 0):.0f}M\",\n            delta=f\"Mid: ${val_range.get('mid', 0):.0f}M\"\n        )\n    \n    with col2:\n        base_npv = extracted.get('economics', {}).get('npv', 0)\n        st.metric(\n            \"Base Case NPV\",\n            format_currency(base_npv, decimals=0) if base_npv else \"N/A\"\n        )\n    \n    with col3:\n        base_irr = extracted.get('economics', {}).get('irr', 0)\n        st.metric(\n            \"Base Case IRR\",\n            f\"{base_irr:.1f}%\" if base_irr else \"N/A\"\n        )\n    \n    with col4:\n        methods_done = summary.get('methods_completed', 0)\n        st.metric(\n            \"Methods Completed\",\n            f\"{methods_done}/5\"\n        )\n    \n    overall_rec = summary.get('overall_recommendation', {})\n    color_map = {\n        'green': '#059669',\n        'blue': '#2563EB',\n        'orange': '#D97706',\n        'red': '#DC2626',\n        'gray': '#6B7280'\n    }\n    rec_color = color_map.get(overall_rec.get('color', 'gray'), '#6B7280')\n    \n    st.markdown(f\"\"\"\n        <div style='background: linear-gradient(135deg, {rec_color}15 0%, {rec_color}25 100%); \n                    padding: 15px; border-radius: 10px; border-left: 4px solid {rec_color}; margin: 15px 0;'>\n            <h4 style='color: {rec_color}; margin: 0 0 5px 0;'>ðŸ“‹ Overall Recommendation</h4>\n            <p style='color: #1E293B; margin: 0; font-size: 1.1rem;'><strong>{overall_rec.get('text', 'N/A')}</strong></p>\n        </div>\n    \"\"\", unsafe_allow_html=True)\n\n\ndef render_probability_dcf_results(data: Dict):\n    \"\"\"Render Probability-Weighted DCF results\"\"\"\n    \n    if 'error' in data:\n        error_msg = data.get('message', data['error'])\n        missing = data.get('missing_inputs', [])\n        if missing:\n            st.warning(f\"âš ï¸ **Insufficient Data for Probability DCF**\\n\\nMissing: {', '.join(missing)}\")\n        else:\n            st.error(f\"Analysis error: {error_msg}\")\n        return\n    \n    if not data:\n        st.info(\"Probability DCF analysis not available\")\n        return\n    \n    st.markdown(\"#### Risk-Adjusted Valuation\")\n    \n    base_case = data.get('base_case', {})\n    risk_adj = data.get('risk_adjusted_valuation', {})\n    prob_analysis = data.get('probability_analysis', {})\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.metric(\"Base Case NPV\", format_currency(base_case.get('npv', 0), decimals=1))\n    with col2:\n        st.metric(\"Risk-Adjusted NPV\", format_currency(risk_adj.get('risk_adjusted_npv', 0), decimals=1))\n    with col3:\n        st.metric(\"Success Probability\", f\"{prob_analysis.get('probability_percent', 0):.1f}%\")\n    \n    st.markdown(\"##### Stage Probabilities\")\n    stage_probs = prob_analysis.get('stage_probabilities', {})\n    for stage, prob in stage_probs.items():\n        st.progress(prob, text=f\"{stage.replace('_', ' ').title()}: {prob*100:.0f}%\")\n    \n    rec = data.get('recommendation', {})\n    if rec.get('text'):\n        st.info(f\"**Recommendation:** {rec['text']}\")\n\n\ndef render_income_dcf_results(data: Dict):\n    \"\"\"Render Income Approach DCF results\"\"\"\n    \n    if 'error' in data:\n        error_msg = data.get('message', data['error'])\n        missing = data.get('missing_inputs', [])\n        if missing:\n            st.warning(f\"âš ï¸ **Insufficient Data for Income DCF**\\n\\nMissing: {', '.join(missing)}\")\n        else:\n            st.error(f\"Analysis error: {error_msg}\")\n        return\n    \n    if not data:\n        st.info(\"Income DCF analysis not available\")\n        return\n    \n    st.markdown(\"#### Base Case Economics\")\n    \n    val_summary = data.get('valuation_summary', {})\n    proj_econ = data.get('project_economics', {})\n    capital = data.get('capital_structure', {})\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"NPV\", format_currency(val_summary.get('npv', 0), decimals=1))\n    with col2:\n        st.metric(\"IRR\", f\"{val_summary.get('irr_percent', 0):.1f}%\")\n    with col3:\n        st.metric(\"Payback\", f\"{val_summary.get('payback_years', 'N/A')} years\")\n    with col4:\n        st.metric(\"Mine Life\", f\"{val_summary.get('mine_life', 0)} years\")\n    \n    st.markdown(\"##### Project Economics\")\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        st.metric(\"AISC\", f\"${proj_econ.get('aisc', 0):.0f}/{proj_econ.get('production_unit', 'unit')}\")\n    with col2:\n        st.metric(\"Initial CAPEX\", format_currency(capital.get('initial_capex', 0), decimals=0))\n    with col3:\n        st.metric(\"Margin\", f\"{proj_econ.get('margin_percent', 0):.1f}%\")\n    \n    rec = data.get('recommendation', {})\n    if rec.get('text'):\n        st.info(f\"**Recommendation:** {rec['text']}\")\n\n\ndef render_monte_carlo_results(data: Dict):\n    \"\"\"Render Monte Carlo simulation results\"\"\"\n    \n    if 'error' in data:\n        error_msg = data.get('message', data['error'])\n        missing = data.get('missing_inputs', [])\n        if missing:\n            st.warning(f\"âš ï¸ **Insufficient Data for Monte Carlo**\\n\\nMissing: {', '.join(missing)}\")\n        else:\n            st.error(f\"Analysis error: {error_msg}\")\n        return\n    \n    if not data:\n        st.info(\"Monte Carlo analysis not available\")\n        return\n    \n    st.markdown(\"#### NPV Distribution (10,000 Simulations)\")\n    \n    npv_stats = data.get('npv_statistics', {})\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        p10 = npv_stats.get('p10', 0)\n        st.metric(\"P10 (Downside)\", format_currency(p10/1e6, decimals=1) if isinstance(p10, (int, float)) else \"N/A\")\n    with col2:\n        p50 = npv_stats.get('p50', 0)\n        st.metric(\"P50 (Base)\", format_currency(p50/1e6, decimals=1) if isinstance(p50, (int, float)) else \"N/A\")\n    with col3:\n        p90 = npv_stats.get('p90', 0)\n        st.metric(\"P90 (Upside)\", format_currency(p90/1e6, decimals=1) if isinstance(p90, (int, float)) else \"N/A\")\n    with col4:\n        prob_pos = npv_stats.get('prob_positive', 0)\n        st.metric(\"Prob. Positive\", f\"{prob_pos*100:.0f}%\" if isinstance(prob_pos, (int, float)) else \"N/A\")\n    \n    var_5 = npv_stats.get('var_5', 0)\n    if var_5:\n        st.metric(\"Value at Risk (5%)\", format_currency(var_5/1e6, decimals=1))\n    \n    real_options = data.get('real_options_value', 0)\n    if real_options:\n        st.metric(\"Real Options Value\", format_currency(real_options/1e6, decimals=1))\n    \n    rec = data.get('recommendation', {})\n    if rec.get('text'):\n        st.info(f\"**Recommendation:** {rec['text']}\")\n\n\ndef render_kilburn_results(data: Dict):\n    \"\"\"Render Kilburn Method / Cost Approach results\"\"\"\n    \n    if 'error' in data:\n        error_msg = data.get('message', data['error'])\n        missing = data.get('missing_inputs', [])\n        if missing:\n            st.warning(f\"âš ï¸ **Insufficient Data for Kilburn Method**\\n\\nMissing: {', '.join(missing)}\")\n        else:\n            st.error(f\"Analysis error: {error_msg}\")\n        return\n    \n    if not data:\n        st.info(\"Kilburn analysis not available\")\n        return\n    \n    st.markdown(\"#### Exploration Floor Value\")\n    \n    geo_rating = data.get('geoscientific_rating', {})\n    val_summary = data.get('valuation_summary', {})\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.metric(\"PEM Multiplier\", f\"{val_summary.get('pem', 0):.2f}x\")\n    with col2:\n        st.metric(\"Avg. Rating\", f\"{val_summary.get('average_rating', 0):.1f}/4.0\")\n    with col3:\n        val = val_summary.get('recommended_value', 0)\n        st.metric(\"Floor Value\", format_currency(val/1e6, decimals=2) if val > 1000000 else f\"${val:,.0f}\")\n    \n    st.markdown(\"##### Geoscientific Rating Breakdown\")\n    for factor in ['regional_prospectivity', 'project_maturity', 'local_geology', 'analytical_data']:\n        score = geo_rating.get(factor, 0)\n        if score:\n            st.progress(score / 4.0, text=f\"{factor.replace('_', ' ').title()}: {score}/4\")\n    \n    rec = data.get('recommendation', {})\n    if rec.get('text'):\n        st.info(f\"**Recommendation:** {rec['text']}\")\n\n\ndef render_decision_tree_results(data: Dict):\n    \"\"\"Render Decision Tree / EMV results\"\"\"\n    \n    if 'error' in data:\n        error_msg = data.get('message', data['error'])\n        missing = data.get('missing_inputs', [])\n        if missing:\n            st.warning(f\"âš ï¸ **Insufficient Data for Decision Tree EMV**\\n\\nMissing: {', '.join(missing)}\")\n        else:\n            st.error(f\"Analysis error: {error_msg}\")\n        return\n    \n    if not data:\n        st.info(\"Decision Tree analysis not available\")\n        return\n    \n    st.markdown(\"#### Expected Monetary Value Analysis\")\n    \n    val_summary = data.get('valuation_summary', {})\n    decision = data.get('decision_analysis', {})\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"EMV\", format_currency(val_summary.get('emv', 0), decimals=1))\n    with col2:\n        st.metric(\"Terminal Value\", format_currency(val_summary.get('terminal_value', 0), decimals=1))\n    with col3:\n        st.metric(\"Prob. to Production\", f\"{val_summary.get('probability_to_production', 0):.1f}%\")\n    with col4:\n        st.metric(\"Time to Production\", f\"{val_summary.get('total_time_to_production', 0):.1f} yrs\")\n    \n    st.markdown(\"##### Stage-Gate Breakdown\")\n    stages = data.get('stage_gate_analysis', [])\n    for stage in stages[:5]:\n        col1, col2, col3 = st.columns([2, 1, 1])\n        with col1:\n            st.write(f\"**{stage.get('stage_name', 'Stage')}**\")\n        with col2:\n            st.write(f\"Cost: ${stage.get('cost', 0):.1f}M\")\n        with col3:\n            st.write(f\"Success: {stage.get('success_probability', 0):.0f}%\")\n    \n    options = data.get('real_options_value', {})\n    if options and isinstance(options, dict):\n        total_options = options.get('total_options_value', 0)\n        if total_options:\n            st.metric(\"Real Options Value\", format_currency(total_options, decimals=1))\n    \n    rec = data.get('recommendation', {})\n    if rec.get('text'):\n        st.info(f\"**Recommendation:** {rec['text']}\")\n\n\ndef render_download_section(result: Dict, current_user: dict):\n    \"\"\"Render the download report section\"\"\"\n    \n    st.markdown(\"### ðŸ“¥ Download Report\")\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        if st.button(\"ðŸ“„ Generate PDF Report\", use_container_width=True, type=\"primary\"):\n            with st.spinner(\"Generating PDF report...\"):\n                try:\n                    pdf_bytes = generate_advanced_pdf_report(result)\n                    \n                    st.download_button(\n                        label=\"â¬‡ï¸ Download PDF\",\n                        data=pdf_bytes,\n                        file_name=f\"{result.get('project_name', 'Project')}_Advanced_Valuation_{datetime.now().strftime('%Y%m%d')}.pdf\",\n                        mime=\"application/pdf\",\n                        use_container_width=True\n                    )\n                    st.success(\"PDF report generated successfully!\")\n                except Exception as e:\n                    st.error(f\"Failed to generate PDF: {str(e)}\")\n    \n    with col2:\n        if st.button(\"ðŸ”„ New Analysis\", use_container_width=True):\n            st.session_state.advanced_view_mode = 'new_analysis'\n            st.session_state.advanced_analysis_result = None\n            st.rerun()\n    \n    with col3:\n        if st.button(\"ðŸ’¾ Save to Projects\", use_container_width=True):\n            try:\n                from project_manager import ProjectManager\n                \n                project = ProjectManager.create_project(\n                    user_id=current_user['id'],\n                    name=result.get('project_name', 'Advanced AI Project'),\n                    description=result.get('project_description', ''),\n                    location='',\n                    commodity=result.get('primary_commodity', '')\n                )\n                \n                summary = result.get('summary', {})\n                val_range = summary.get('valuation_range', {})\n                \n                analysis_data = {\n                    'categories': {},\n                    'executive_summary': result.get('narrative', {}).get('executive_summary', ''),\n                    'extracted_data': result.get('extracted_data', {}),\n                    'valuations': result.get('valuations', {}),\n                    'valuation_summary': summary\n                }\n                \n                scoring_data = {\n                    'total_score': summary.get('methods_completed', 0) * 20,\n                    'probability_of_success': 0,\n                    'risk_band': 'ADVANCED VALUATION',\n                    'risk_category': 'Advanced Valuation',\n                    'category_contributions': {}\n                }\n                \n                rec_text = summary.get('overall_recommendation', {}).get('text', 'See detailed valuation results.')\n                recommendations = [rec_text] if rec_text else ['See detailed valuation results.']\n                \n                saved_analysis = ProjectManager.save_analysis(\n                    project_id=project['id'],\n                    analysis_data=analysis_data,\n                    scoring_data=scoring_data,\n                    recommendations=recommendations,\n                    analysis_type='advanced_ai'\n                )\n                \n                st.success(f\"âœ… Saved to project: {project['name']}\")\n            except Exception as e:\n                st.error(f\"Failed to save: {str(e)}\")\n\n\ndef generate_advanced_pdf_report(result: Dict) -> bytes:\n    \"\"\"Generate PDF report for advanced valuation analysis\"\"\"\n    from fpdf import FPDF\n    \n    class AdvancedValuationReport(FPDF):\n        def header(self):\n            self.set_font('Arial', 'B', 14)\n            self.cell(0, 10, 'Advanced Valuation Report - Oreplot', 0, 1, 'C')\n            self.ln(5)\n        \n        def footer(self):\n            self.set_y(-15)\n            self.set_font('Arial', 'I', 8)\n            self.cell(0, 10, f'Page {self.page_no()}', 0, 0, 'C')\n    \n    pdf = AdvancedValuationReport()\n    pdf.add_page()\n    \n    pdf.set_font('Arial', 'I', 10)\n    pdf.set_text_color(100, 100, 100)\n    pdf.cell(0, 8, 'Analysis Type: Oreplot Advanced Analysis', 0, 1, 'R')\n    pdf.set_text_color(0, 0, 0)\n    pdf.ln(2)\n    \n    pdf.set_font('Arial', 'B', 20)\n    pdf.cell(0, 15, result.get('project_name', 'Mining Project'), 0, 1, 'C')\n    pdf.set_font('Arial', '', 12)\n    pdf.cell(0, 8, f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\", 0, 1, 'C')\n    pdf.ln(10)\n    \n    summary = result.get('summary', {})\n    val_range = summary.get('valuation_range', {})\n    \n    pdf.set_font('Arial', 'B', 14)\n    pdf.cell(0, 10, 'Valuation Summary', 0, 1)\n    pdf.set_font('Arial', '', 11)\n    pdf.cell(0, 8, f\"Valuation Range: ${val_range.get('low', 0):.1f}M - ${val_range.get('high', 0):.1f}M\", 0, 1)\n    pdf.cell(0, 8, f\"Median Value: ${val_range.get('mid', 0):.1f}M\", 0, 1)\n    pdf.cell(0, 8, f\"Methods Completed: {summary.get('methods_completed', 0)}/5\", 0, 1)\n    pdf.ln(5)\n    \n    pdf.set_font('Arial', 'B', 12)\n    pdf.cell(0, 8, f\"Recommendation: {summary.get('overall_recommendation', {}).get('text', 'N/A')}\", 0, 1)\n    pdf.ln(10)\n    \n    narrative = result.get('narrative', {})\n    if narrative.get('executive_summary'):\n        pdf.set_font('Arial', 'B', 14)\n        pdf.cell(0, 10, 'Executive Summary', 0, 1)\n        pdf.set_font('Arial', '', 10)\n        \n        exec_summary = narrative['executive_summary']\n        exec_summary = exec_summary.encode('latin-1', 'replace').decode('latin-1')\n        pdf.multi_cell(0, 6, exec_summary)\n        pdf.ln(10)\n    \n    pdf.set_font('Arial', 'B', 14)\n    pdf.cell(0, 10, 'Valuation Methods', 0, 1)\n    \n    valuations = result.get('valuations', {})\n    \n    methods = [\n        ('Probability-Weighted DCF', 'probability_dcf', 'risk_adjusted_valuation', 'risk_adjusted_npv'),\n        ('Income DCF', 'income_dcf', 'valuation_summary', 'npv'),\n        ('Monte Carlo P50', 'monte_carlo', 'npv_statistics', 'p50'),\n        ('Kilburn Method', 'kilburn', 'valuation_summary', 'recommended_value'),\n        ('Decision Tree EMV', 'decision_tree', 'valuation_summary', 'emv')\n    ]\n    \n    pdf.set_font('Arial', '', 10)\n    for name, key, sub_key, value_key in methods:\n        val_data = valuations.get(key, {})\n        if 'error' not in val_data:\n            sub_data = val_data.get(sub_key, {})\n            value = sub_data.get(value_key, 0)\n            if value:\n                if key == 'monte_carlo' or key == 'kilburn':\n                    value = value / 1e6\n                pdf.cell(0, 6, f\"{name}: ${value:.1f}M\", 0, 1)\n        else:\n            pdf.cell(0, 6, f\"{name}: Error in calculation\", 0, 1)\n    \n    pdf.ln(10)\n    pdf.set_font('Arial', 'I', 9)\n    pdf.cell(0, 6, \"This report was generated by Oreplot Advanced AI Valuation Agent.\", 0, 1)\n    pdf.cell(0, 6, \"For detailed methodology, please refer to full technical documentation.\", 0, 1)\n    \n    output = pdf.output(dest='S')\n    if isinstance(output, bytes):\n        return output\n    elif isinstance(output, bytearray):\n        return bytes(output)\n    else:\n        return output.encode('latin-1')\n\n\ndef render_access_denied(current_user: dict):\n    \"\"\"Render access denied message with upgrade information\"\"\"\n    st.markdown(\"\"\"\n        <div style='background: linear-gradient(135deg, #FEE2E2 0%, #FECACA 100%); \n                    padding: 2rem; border-radius: 12px; text-align: center; margin: 2rem 0;'>\n            <h2 style='color: #DC2626; margin-bottom: 1rem;'>ðŸ”’ Advanced AI Access Required</h2>\n            <p style='color: #7F1D1D; font-size: 1.1rem;'>\n                Your current plan does not include Advanced AI features.\n            </p>\n        </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    st.markdown(\"### ðŸŸ£ Advanced AI Features Include:\")\n    \n    feature_cols = st.columns(3)\n    for idx, feature in enumerate(ADVANCED_AI_FEATURES):\n        with feature_cols[idx % 3]:\n            st.markdown(f\"\"\"\n                <div style='background: #F8FAFC; padding: 1rem; border-radius: 8px; \n                            margin-bottom: 1rem; border-left: 4px solid #8B5CF6;'>\n                    <h4 style='margin: 0 0 0.5rem 0;'>{feature['icon']} {feature['name']}</h4>\n                    <p style='color: #64748B; margin: 0; font-size: 0.9rem;'>{feature['description']}</p>\n                </div>\n            \"\"\", unsafe_allow_html=True)\n    \n    st.info(\"ðŸ’¡ Contact your administrator to upgrade your AI access tier.\")\n","path":null,"size_bytes":33255,"size_tokens":null},"market_multiples_engine.py":{"content":"\"\"\"\nMarket Multiples Valuation Engine\nImplements EV/Resource benchmarking based on PwC methodology\n\"\"\"\n\nimport streamlit as st\nfrom database import SessionLocal\nfrom models import ComparableProject\nfrom sqlalchemy import func\nimport json\nfrom typing import Dict, List, Optional, Tuple\nfrom format_utils import format_currency\nfrom datetime import datetime\n\n\n# Industry standard EV/Resource multiples (USD per ounce equivalent)\nRESOURCE_MULTIPLES = {\n    'gold': {\n        'inferred': 20.0,\n        'indicated': 45.0,\n        'measured': 60.0,\n        'mi': 30.0,  # Measured + Indicated average\n        'pp': 160.0,  # Proven + Probable\n    },\n    'silver': {\n        'inferred': 0.30,\n        'indicated': 0.60,\n        'mi': 0.45,\n        'pp': 2.00,\n    },\n    'copper': {\n        'inferred': 0.015,  # per lb\n        'indicated': 0.025,\n        'mi': 0.020,\n        'pp': 0.08,\n    },\n    'zinc': {\n        'inferred': 0.008,\n        'indicated': 0.015,\n        'mi': 0.012,\n        'pp': 0.04,\n    },\n    'nickel': {\n        'inferred': 0.02,\n        'indicated': 0.04,\n        'mi': 0.03,\n        'pp': 0.10,\n    },\n    'lithium': {\n        'inferred': 150,  # per tonne LCE\n        'indicated': 250,\n        'mi': 200,\n        'pp': 500,\n    },\n    'uranium': {\n        'inferred': 1.50,  # per lb U3O8\n        'indicated': 2.50,\n        'mi': 2.00,\n        'pp': 6.00,\n    },\n}\n\n# Development stage adjustments\nSTAGE_MULTIPLIERS = {\n    'exploration': 0.6,\n    'resource': 0.8,\n    'feasibility': 1.0,\n    'development': 1.2,\n    'production': 1.5,\n    'producing': 1.5,\n}\n\n# Jurisdiction risk discounts\nJURISDICTION_DISCOUNTS = {\n    'tier_1': 1.0,  # Canada, Australia, USA, Chile\n    'tier_2': 0.85,  # Mexico, Peru, Brazil, South Africa\n    'tier_3': 0.70,  # DRC, Guinea, Indonesia\n    'tier_4': 0.50,  # High-risk jurisdictions\n}\n\nTIER_1_JURISDICTIONS = ['canada', 'australia', 'usa', 'united states', 'chile', 'uk', 'sweden', 'finland', 'norway']\nTIER_2_JURISDICTIONS = ['mexico', 'peru', 'brazil', 'south africa', 'argentina', 'colombia', 'spain', 'portugal', 'ireland']\nTIER_3_JURISDICTIONS = ['drc', 'democratic republic of congo', 'guinea', 'indonesia', 'philippines', 'tanzania', 'zambia', 'mali', 'burkina faso', 'ghana', 'ivory coast']\n\n\ndef get_jurisdiction_tier(jurisdiction: str) -> str:\n    \"\"\"Determine jurisdiction risk tier\"\"\"\n    if not jurisdiction:\n        return 'tier_2'\n    \n    jurisdiction_lower = jurisdiction.lower()\n    \n    if any(t1 in jurisdiction_lower for t1 in TIER_1_JURISDICTIONS):\n        return 'tier_1'\n    elif any(t2 in jurisdiction_lower for t2 in TIER_2_JURISDICTIONS):\n        return 'tier_2'\n    elif any(t3 in jurisdiction_lower for t3 in TIER_3_JURISDICTIONS):\n        return 'tier_3'\n    else:\n        return 'tier_2'  # Default to tier 2\n\n\ndef get_base_multiple(commodity: str, resource_category: str) -> float:\n    \"\"\"Get base EV/Resource multiple for a commodity and category\"\"\"\n    commodity_lower = commodity.lower() if commodity else 'gold'\n    \n    # Normalize commodity names\n    if 'gold' in commodity_lower or 'au' == commodity_lower:\n        commodity_key = 'gold'\n    elif 'silver' in commodity_lower or 'ag' == commodity_lower:\n        commodity_key = 'silver'\n    elif 'copper' in commodity_lower or 'cu' == commodity_lower:\n        commodity_key = 'copper'\n    elif 'zinc' in commodity_lower or 'zn' == commodity_lower:\n        commodity_key = 'zinc'\n    elif 'nickel' in commodity_lower or 'ni' == commodity_lower:\n        commodity_key = 'nickel'\n    elif 'lithium' in commodity_lower or 'li' == commodity_lower:\n        commodity_key = 'lithium'\n    elif 'uranium' in commodity_lower or 'u3o8' in commodity_lower:\n        commodity_key = 'uranium'\n    else:\n        commodity_key = 'gold'  # Default\n    \n    multiples = RESOURCE_MULTIPLES.get(commodity_key, RESOURCE_MULTIPLES['gold'])\n    \n    # Normalize category - check combined categories first (more specific matches)\n    category_lower = resource_category.lower() if resource_category else 'inferred'\n    \n    # Check for P&P / Reserves first\n    if any(x in category_lower for x in ['probable', 'proven', 'p+p', 'p&p', 'pp', 'reserve']):\n        return multiples['pp']\n    # Check for M&I / M+I combined category\n    elif any(x in category_lower for x in ['m+i', 'm&i', 'measured & indicated', 'measured and indicated', 'measured+indicated']):\n        return multiples['mi']\n    # Check for individual categories (after combined to avoid false matches)\n    elif 'measured' in category_lower and 'indicated' not in category_lower:\n        return multiples['measured']\n    elif 'indicated' in category_lower and 'measured' not in category_lower:\n        return multiples['indicated']\n    elif 'inferred' in category_lower:\n        return multiples['inferred']\n    else:\n        return multiples['inferred']\n\n\ndef calculate_ev_resource_valuation(\n    commodity: str,\n    resource_estimate: float,\n    resource_category: str,\n    stage: str,\n    jurisdiction: str,\n    grade: Optional[float] = None\n) -> Dict:\n    \"\"\"\n    Calculate project valuation using EV/Resource multiples\n    \n    Args:\n        commodity: Primary commodity (gold, silver, copper, etc.)\n        resource_estimate: Resource estimate in appropriate units (oz for gold, lbs for base metals)\n        resource_category: Inferred, Indicated, Measured, M&I, or P&P\n        stage: Development stage\n        jurisdiction: Project location\n        grade: Optional grade for premium adjustment\n    \n    Returns:\n        Dictionary with valuation details\n    \"\"\"\n    # Get base multiple\n    base_multiple = get_base_multiple(commodity, resource_category)\n    \n    # Get stage multiplier\n    stage_lower = stage.lower() if stage else 'exploration'\n    stage_mult = 0.6  # Default for exploration\n    for key, value in STAGE_MULTIPLIERS.items():\n        if key in stage_lower:\n            stage_mult = value\n            break\n    \n    # Get jurisdiction discount\n    jurisdiction_tier = get_jurisdiction_tier(jurisdiction)\n    jurisdiction_discount = JURISDICTION_DISCOUNTS[jurisdiction_tier]\n    \n    # Calculate adjusted multiple\n    adjusted_multiple = base_multiple * stage_mult * jurisdiction_discount\n    \n    # Grade premium/discount (10% premium for top quartile grades)\n    grade_adjustment = 1.0\n    if grade:\n        # This would require historical grade data for comparison\n        # For now, apply a simple heuristic\n        if commodity.lower() == 'gold' and grade > 3.0:  # High-grade gold > 3 g/t\n            grade_adjustment = 1.15\n        elif commodity.lower() == 'gold' and grade < 0.8:  # Low-grade gold < 0.8 g/t\n            grade_adjustment = 0.85\n    \n    final_multiple = adjusted_multiple * grade_adjustment\n    \n    # Calculate implied value\n    implied_value = resource_estimate * final_multiple\n    \n    # Calculate range (Â±20%)\n    low_value = implied_value * 0.80\n    high_value = implied_value * 1.20\n    \n    return {\n        'base_multiple': base_multiple,\n        'stage_multiplier': stage_mult,\n        'jurisdiction_tier': jurisdiction_tier,\n        'jurisdiction_discount': jurisdiction_discount,\n        'grade_adjustment': grade_adjustment,\n        'final_multiple': final_multiple,\n        'implied_value': implied_value,\n        'value_range': {\n            'low': low_value,\n            'mid': implied_value,\n            'high': high_value\n        },\n        'commodity': commodity,\n        'resource_estimate': resource_estimate,\n        'resource_category': resource_category,\n        'stage': stage,\n        'jurisdiction': jurisdiction,\n        'calculation_date': datetime.now().isoformat()\n    }\n\n\ndef get_comparable_peers(commodity: str, stage: Optional[str] = None, jurisdiction: Optional[str] = None, limit: int = 10) -> List[Dict]:\n    \"\"\"\n    Fetch comparable projects from database for peer benchmarking\n    \"\"\"\n    db = SessionLocal()\n    try:\n        query = db.query(ComparableProject).filter(\n            ComparableProject.approved_for_display == True,\n            ComparableProject.status == 'active'\n        )\n        \n        # Filter by commodity (primary filter)\n        if commodity:\n            query = query.filter(\n                func.lower(ComparableProject.commodity).contains(commodity.lower())\n            )\n        \n        # Optional stage filter\n        if stage:\n            query = query.filter(\n                func.lower(ComparableProject.development_stage).contains(stage.lower())\n            )\n        \n        projects = query.order_by(ComparableProject.created_at.desc()).limit(limit).all()\n        \n        result = []\n        for project in projects:\n            result.append({\n                'id': project.id,\n                'project_name': project.project_name,\n                'company': project.company,\n                'commodity': project.commodity,\n                'deposit_type': project.deposit_type,\n                'development_stage': project.development_stage,\n                'jurisdiction': project.jurisdiction,\n                'resource_moz': project.resource_moz,\n                'grade_gpt': project.grade_gpt,\n                'investment_score': project.investment_score,\n                'sustainability_score': project.sustainability_score,\n            })\n        \n        return result\n    finally:\n        db.close()\n\n\ndef calculate_peer_statistics(peers: List[Dict], commodity: str) -> Dict:\n    \"\"\"\n    Calculate statistics from peer group for benchmarking\n    \"\"\"\n    if not peers:\n        return {\n            'count': 0,\n            'avg_ev_per_oz': None,\n            'median_ev_per_oz': None,\n            'percentile_25': None,\n            'percentile_75': None,\n        }\n    \n    # For now, use industry benchmarks\n    # In production, this would calculate from actual transaction data\n    base_multiple = get_base_multiple(commodity, 'indicated')\n    \n    return {\n        'count': len(peers),\n        'avg_ev_per_oz': base_multiple,\n        'median_ev_per_oz': base_multiple * 0.95,\n        'percentile_25': base_multiple * 0.75,\n        'percentile_75': base_multiple * 1.25,\n        'min': base_multiple * 0.50,\n        'max': base_multiple * 1.50,\n    }\n\n\ndef render_market_multiples_analysis():\n    \"\"\"Render the Market Multiples analysis UI in Streamlit\"\"\"\n    st.markdown(\"\"\"\n    <div style='background: linear-gradient(135deg, #1e3a5f, #2d5a87); padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n        <h3 style='color: white; margin: 0;'>ðŸ“Š Market Multiples Valuation</h3>\n        <p style='color: #e0e0e0; margin: 5px 0 0 0;'>EV/Resource benchmarking based on industry transactions</p>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    st.markdown(\"### Project Parameters\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        commodity = st.selectbox(\n            \"Primary Commodity\",\n            [\"Gold\", \"Silver\", \"Copper\", \"Zinc\", \"Nickel\", \"Lithium\", \"Uranium\"],\n            help=\"Select the primary commodity for valuation\"\n        )\n        \n        resource_estimate = st.number_input(\n            \"Resource Estimate (oz/lbs/tonnes)\",\n            min_value=0.0,\n            value=1000000.0,\n            step=100000.0,\n            format=\"%.0f\",\n            help=\"Enter total resource in appropriate units (oz for precious metals, lbs for base metals)\"\n        )\n        \n        grade = st.number_input(\n            \"Average Grade (g/t or %)\",\n            min_value=0.0,\n            value=1.5,\n            step=0.1,\n            format=\"%.2f\",\n            help=\"Enter average grade for premium adjustment\"\n        )\n    \n    with col2:\n        resource_category = st.selectbox(\n            \"Resource Category\",\n            [\"Inferred\", \"Indicated\", \"Measured\", \"M&I (Measured + Indicated)\", \"P&P (Proven + Probable)\"],\n            help=\"Higher confidence categories command higher multiples\"\n        )\n        \n        stage = st.selectbox(\n            \"Development Stage\",\n            [\"Exploration\", \"Resource\", \"Feasibility\", \"Development\", \"Production\"],\n            help=\"Later stage projects command premium multiples\"\n        )\n        \n        jurisdiction = st.selectbox(\n            \"Jurisdiction\",\n            [\"Canada\", \"Australia\", \"USA\", \"Chile\", \"Mexico\", \"Peru\", \"Brazil\", \"South Africa\", \n             \"DRC\", \"Guinea\", \"Indonesia\", \"Other\"],\n            help=\"Tier 1 jurisdictions have lower risk discount\"\n        )\n    \n    st.markdown(\"---\")\n    \n    if st.button(\"ðŸ” Calculate Valuation\", type=\"primary\", use_container_width=True):\n        with st.spinner(\"Calculating market multiples valuation...\"):\n            # Perform valuation\n            result = calculate_ev_resource_valuation(\n                commodity=commodity,\n                resource_estimate=resource_estimate,\n                resource_category=resource_category,\n                stage=stage,\n                jurisdiction=jurisdiction,\n                grade=grade\n            )\n            \n            # Store in session state\n            st.session_state['market_multiples_result'] = result\n            \n            # Display results\n            st.markdown(\"### ðŸ“ˆ Valuation Results\")\n            \n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                st.metric(\n                    \"Base Multiple\",\n                    f\"${result['base_multiple']:.2f}/oz\",\n                    help=\"Industry standard multiple for commodity and category\"\n                )\n            \n            with col2:\n                st.metric(\n                    \"Adjusted Multiple\",\n                    f\"${result['final_multiple']:.2f}/oz\",\n                    delta=f\"{((result['final_multiple']/result['base_multiple'])-1)*100:.1f}%\",\n                    help=\"Multiple after stage, jurisdiction, and grade adjustments\"\n                )\n            \n            with col3:\n                st.metric(\n                    \"Implied Value\",\n                    format_currency(result['implied_value']/1e6, decimals=1),\n                    help=\"Calculated enterprise value\"\n                )\n            \n            # Value range\n            st.markdown(\"#### Valuation Range\")\n            \n            range_data = result['value_range']\n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                st.markdown(f\"\"\"\n                <div style='background: #fff3e0; padding: 15px; border-radius: 8px; text-align: center;'>\n                    <div style='font-size: 14px; color: #666;'>Low Case (-20%)</div>\n                    <div style='font-size: 24px; font-weight: bold; color: #e65100;'>{format_currency(range_data['low']/1e6, decimals=1)}</div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n            \n            with col2:\n                st.markdown(f\"\"\"\n                <div style='background: #e8f5e9; padding: 15px; border-radius: 8px; text-align: center;'>\n                    <div style='font-size: 14px; color: #666;'>Base Case</div>\n                    <div style='font-size: 24px; font-weight: bold; color: #2e7d32;'>{format_currency(range_data['mid']/1e6, decimals=1)}</div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n            \n            with col3:\n                st.markdown(f\"\"\"\n                <div style='background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center;'>\n                    <div style='font-size: 14px; color: #666;'>High Case (+20%)</div>\n                    <div style='font-size: 24px; font-weight: bold; color: #1565c0;'>{format_currency(range_data['high']/1e6, decimals=1)}</div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n            \n            # Adjustment breakdown\n            st.markdown(\"#### Adjustment Factors\")\n            \n            st.markdown(f\"\"\"\n            | Factor | Value | Impact |\n            |--------|-------|--------|\n            | Stage Multiplier | {result['stage_multiplier']:.2f}x | {stage} stage adjustment |\n            | Jurisdiction Discount | {result['jurisdiction_discount']:.0%} | {result['jurisdiction_tier'].replace('_', ' ').title()} ({jurisdiction}) |\n            | Grade Adjustment | {result['grade_adjustment']:.0%} | {\"Premium\" if result['grade_adjustment'] > 1 else \"Discount\" if result['grade_adjustment'] < 1 else \"Neutral\"} |\n            \"\"\")\n            \n            # Peer comparison\n            st.markdown(\"#### ðŸ”„ Peer Comparison\")\n            \n            peers = get_comparable_peers(commodity, stage.lower())\n            \n            if peers:\n                st.markdown(f\"Found **{len(peers)}** comparable {commodity} projects in database:\")\n                \n                for peer in peers[:5]:\n                    with st.expander(f\"ðŸ“ {peer['project_name']} ({peer['company']})\"):\n                        col1, col2 = st.columns(2)\n                        with col1:\n                            st.markdown(f\"**Commodity:** {peer['commodity']}\")\n                            st.markdown(f\"**Stage:** {peer['development_stage']}\")\n                            st.markdown(f\"**Jurisdiction:** {peer['jurisdiction']}\")\n                        with col2:\n                            if peer.get('resource_moz'):\n                                st.markdown(f\"**Resource:** {peer['resource_moz']:.2f} Moz\")\n                            if peer.get('grade_gpt'):\n                                st.markdown(f\"**Grade:** {peer['grade_gpt']:.2f} g/t\")\n                            if peer.get('investment_score'):\n                                st.markdown(f\"**Investment Score:** {peer['investment_score']}/100\")\n            else:\n                st.info(f\"No comparable {commodity} projects found in database. Using industry benchmark multiples.\")\n            \n            st.success(\"Market multiples valuation complete!\")\n\n\ndef get_valuation_for_report(project_data: Dict) -> Optional[Dict]:\n    \"\"\"\n    Generate valuation for PDF report generation\n    \"\"\"\n    if not project_data:\n        return None\n    \n    commodity = project_data.get('commodity', 'Gold')\n    resource_estimate = project_data.get('resource_estimate', 0)\n    resource_category = project_data.get('resource_category', 'Inferred')\n    stage = project_data.get('stage', 'Exploration')\n    jurisdiction = project_data.get('jurisdiction', 'Unknown')\n    grade = project_data.get('grade')\n    \n    if resource_estimate <= 0:\n        return None\n    \n    return calculate_ev_resource_valuation(\n        commodity=commodity,\n        resource_estimate=resource_estimate,\n        resource_category=resource_category,\n        stage=stage,\n        jurisdiction=jurisdiction,\n        grade=grade\n    )\n","path":null,"size_bytes":18665,"size_tokens":null},"kilburn_valuation.py":{"content":"\"\"\"\nKilburn Method / PwC Cost Approach Valuation Engine\nImplements geoscientific rating system for early-stage exploration valuation\n\"\"\"\n\nimport streamlit as st\nfrom typing import Dict, List, Optional, Tuple\nfrom datetime import datetime\nimport json\n\n\n# Kilburn Geoscientific Rating Factors (1-4 scale)\nRATING_FACTORS = {\n    'regional_prospectivity': {\n        'name': 'Regional Prospectivity',\n        'description': 'Geological setting and regional mineral potential',\n        'levels': {\n            1: {'name': 'Low', 'desc': 'Limited geological potential, unfavorable setting'},\n            2: {'name': 'Moderate', 'desc': 'Some favorable indicators, limited past success'},\n            3: {'name': 'High', 'desc': 'Favorable geological setting, nearby deposits'},\n            4: {'name': 'Very High', 'desc': 'Proven mineral district, multiple nearby deposits'}\n        }\n    },\n    'project_maturity': {\n        'name': 'Project Maturity',\n        'description': 'Stage of exploration and technical advancement',\n        'levels': {\n            1: {'name': 'Grassroots', 'desc': 'Early reconnaissance, no drilling'},\n            2: {'name': 'Early Stage', 'desc': 'Initial drilling, target definition'},\n            3: {'name': 'Advanced', 'desc': 'Systematic drilling, resource estimation'},\n            4: {'name': 'Pre-Development', 'desc': 'Defined resource, economic studies'}\n        }\n    },\n    'local_geology': {\n        'name': 'Local Geology',\n        'description': 'On-property geological indicators and mineralization',\n        'levels': {\n            1: {'name': 'Poor', 'desc': 'No significant surface indicators'},\n            2: {'name': 'Fair', 'desc': 'Some favorable indicators, limited sampling'},\n            3: {'name': 'Good', 'desc': 'Consistent mineralization, positive results'},\n            4: {'name': 'Excellent', 'desc': 'High-grade intercepts, well-defined zones'}\n        }\n    },\n    'analytical_data': {\n        'name': 'Analytical Data Quality',\n        'description': 'Quality and reliability of technical data',\n        'levels': {\n            1: {'name': 'Poor', 'desc': 'Limited assays, no QAQC'},\n            2: {'name': 'Fair', 'desc': 'Basic sampling, limited verification'},\n            3: {'name': 'Good', 'desc': 'Systematic sampling, QAQC protocols'},\n            4: {'name': 'Excellent', 'desc': 'Comprehensive data, third-party verification'}\n        }\n    }\n}\n\n# Prospectivity Enhancement Multiplier (PEM) ranges\nPEM_RANGES = {\n    'very_low': {'min': 0.5, 'max': 1.0, 'desc': 'Low confidence, high-risk'},\n    'low': {'min': 1.0, 'max': 1.5, 'desc': 'Below average prospectivity'},\n    'moderate': {'min': 1.5, 'max': 2.0, 'desc': 'Average exploration potential'},\n    'high': {'min': 2.0, 'max': 3.0, 'desc': 'Above average prospectivity'},\n    'very_high': {'min': 3.0, 'max': 5.0, 'desc': 'Exceptional potential, defined targets'}\n}\n\n# Base acquisition cost (BAC) per hectare by region (USD)\nBAC_PER_HECTARE = {\n    'north_america': 25.0,\n    'south_america': 15.0,\n    'australia': 20.0,\n    'africa': 10.0,\n    'europe': 30.0,\n    'asia': 12.0,\n    'other': 15.0,\n}\n\n\ndef calculate_kilburn_rating(\n    regional_prospectivity: int,\n    project_maturity: int,\n    local_geology: int,\n    analytical_data: int\n) -> Tuple[float, str]:\n    \"\"\"\n    Calculate Kilburn geoscientific rating\n    \n    Args:\n        regional_prospectivity: Rating 1-4\n        project_maturity: Rating 1-4\n        local_geology: Rating 1-4\n        analytical_data: Rating 1-4\n    \n    Returns:\n        Tuple of (composite rating, rating category)\n    \"\"\"\n    # Validate inputs\n    ratings = [regional_prospectivity, project_maturity, local_geology, analytical_data]\n    for r in ratings:\n        if r < 1 or r > 4:\n            raise ValueError(f\"Rating must be between 1 and 4, got {r}\")\n    \n    # Calculate average rating\n    avg_rating = sum(ratings) / len(ratings)\n    \n    # Determine category\n    if avg_rating <= 1.5:\n        category = 'very_low'\n    elif avg_rating <= 2.0:\n        category = 'low'\n    elif avg_rating <= 2.5:\n        category = 'moderate'\n    elif avg_rating <= 3.0:\n        category = 'high'\n    else:\n        category = 'very_high'\n    \n    return avg_rating, category\n\n\ndef calculate_pem(rating: float, category: str) -> float:\n    \"\"\"\n    Calculate Prospectivity Enhancement Multiplier based on rating\n    \"\"\"\n    pem_range = PEM_RANGES[category]\n    \n    # Linear interpolation within category range\n    if category == 'very_low':\n        normalized = (rating - 1.0) / 0.5\n    elif category == 'low':\n        normalized = (rating - 1.5) / 0.5\n    elif category == 'moderate':\n        normalized = (rating - 2.0) / 0.5\n    elif category == 'high':\n        normalized = (rating - 2.5) / 0.5\n    else:\n        normalized = (rating - 3.0) / 1.0\n    \n    normalized = max(0, min(1, normalized))\n    \n    pem = pem_range['min'] + (pem_range['max'] - pem_range['min']) * normalized\n    \n    return round(pem, 2)\n\n\ndef calculate_mee_valuation(\n    exploration_expenditure: float,\n    pem: float,\n    years_since_expenditure: int = 0,\n    inflation_rate: float = 0.03\n) -> Dict:\n    \"\"\"\n    Calculate Multiple of Exploration Expenditure (MEE) valuation\n    \n    Args:\n        exploration_expenditure: Total exploration expenditure (USD)\n        pem: Prospectivity Enhancement Multiplier\n        years_since_expenditure: Years since expenditure for inflation adjustment\n        inflation_rate: Annual inflation rate for adjustment\n    \n    Returns:\n        Dictionary with MEE valuation details\n    \"\"\"\n    # Adjust for inflation\n    inflation_factor = (1 + inflation_rate) ** years_since_expenditure\n    adjusted_expenditure = exploration_expenditure * inflation_factor\n    \n    # Calculate appraised value\n    appraised_value = adjusted_expenditure * pem\n    \n    # Calculate range (Â±15%)\n    low_value = appraised_value * 0.85\n    high_value = appraised_value * 1.15\n    \n    return {\n        'exploration_expenditure': exploration_expenditure,\n        'adjusted_expenditure': adjusted_expenditure,\n        'inflation_adjustment': inflation_factor,\n        'pem': pem,\n        'appraised_value': appraised_value,\n        'value_range': {\n            'low': low_value,\n            'mid': appraised_value,\n            'high': high_value\n        },\n        'methodology': 'Multiple of Exploration Expenditure (MEE)',\n        'calculation_date': datetime.now().isoformat()\n    }\n\n\ndef calculate_bac_valuation(\n    area_hectares: float,\n    region: str,\n    pem: float\n) -> Dict:\n    \"\"\"\n    Calculate Base Acquisition Cost valuation\n    \n    Args:\n        area_hectares: Property area in hectares\n        region: Geographic region\n        pem: Prospectivity Enhancement Multiplier\n    \n    Returns:\n        Dictionary with BAC valuation details\n    \"\"\"\n    # Get BAC per hectare\n    region_lower = region.lower().replace(' ', '_')\n    if 'north america' in region_lower or 'canada' in region_lower or 'usa' in region_lower:\n        bac = BAC_PER_HECTARE['north_america']\n        region_key = 'north_america'\n    elif 'south america' in region_lower or 'peru' in region_lower or 'chile' in region_lower or 'brazil' in region_lower:\n        bac = BAC_PER_HECTARE['south_america']\n        region_key = 'south_america'\n    elif 'australia' in region_lower:\n        bac = BAC_PER_HECTARE['australia']\n        region_key = 'australia'\n    elif 'africa' in region_lower or 'drc' in region_lower or 'mali' in region_lower:\n        bac = BAC_PER_HECTARE['africa']\n        region_key = 'africa'\n    elif 'europe' in region_lower:\n        bac = BAC_PER_HECTARE['europe']\n        region_key = 'europe'\n    elif 'asia' in region_lower or 'indonesia' in region_lower or 'philippines' in region_lower:\n        bac = BAC_PER_HECTARE['asia']\n        region_key = 'asia'\n    else:\n        bac = BAC_PER_HECTARE['other']\n        region_key = 'other'\n    \n    # Calculate base value\n    base_value = area_hectares * bac\n    \n    # Apply PEM\n    appraised_value = base_value * pem\n    \n    # Calculate range (Â±20%)\n    low_value = appraised_value * 0.80\n    high_value = appraised_value * 1.20\n    \n    return {\n        'area_hectares': area_hectares,\n        'region': region,\n        'bac_per_hectare': bac,\n        'base_value': base_value,\n        'pem': pem,\n        'appraised_value': appraised_value,\n        'value_range': {\n            'low': low_value,\n            'mid': appraised_value,\n            'high': high_value\n        },\n        'methodology': 'Base Acquisition Cost (BAC)',\n        'calculation_date': datetime.now().isoformat()\n    }\n\n\ndef calculate_kilburn_valuation(\n    regional_prospectivity: int,\n    project_maturity: int,\n    local_geology: int,\n    analytical_data: int,\n    exploration_expenditure: float = 0,\n    area_hectares: float = 0,\n    region: str = 'North America',\n    years_since_expenditure: int = 0\n) -> Dict:\n    \"\"\"\n    Complete Kilburn/Cost Approach valuation combining geoscientific rating with MEE and BAC\n    \"\"\"\n    # Calculate Kilburn rating\n    rating, category = calculate_kilburn_rating(\n        regional_prospectivity, project_maturity, local_geology, analytical_data\n    )\n    \n    # Calculate PEM\n    pem = calculate_pem(rating, category)\n    \n    result = {\n        'geoscientific_rating': {\n            'regional_prospectivity': regional_prospectivity,\n            'project_maturity': project_maturity,\n            'local_geology': local_geology,\n            'analytical_data': analytical_data,\n            'composite_rating': rating,\n            'category': category,\n        },\n        'pem': pem,\n        'calculation_date': datetime.now().isoformat()\n    }\n    \n    # Calculate MEE if expenditure provided\n    if exploration_expenditure > 0:\n        mee_result = calculate_mee_valuation(\n            exploration_expenditure, pem, years_since_expenditure\n        )\n        result['mee_valuation'] = mee_result\n    \n    # Calculate BAC if area provided\n    if area_hectares > 0:\n        bac_result = calculate_bac_valuation(area_hectares, region, pem)\n        result['bac_valuation'] = bac_result\n    \n    # Determine preferred valuation\n    if exploration_expenditure > 0 and area_hectares > 0:\n        # Use higher of MEE or BAC\n        if result['mee_valuation']['appraised_value'] > result['bac_valuation']['appraised_value']:\n            result['preferred_valuation'] = result['mee_valuation']['appraised_value']\n            result['preferred_methodology'] = 'MEE'\n        else:\n            result['preferred_valuation'] = result['bac_valuation']['appraised_value']\n            result['preferred_methodology'] = 'BAC'\n    elif exploration_expenditure > 0:\n        result['preferred_valuation'] = result['mee_valuation']['appraised_value']\n        result['preferred_methodology'] = 'MEE'\n    elif area_hectares > 0:\n        result['preferred_valuation'] = result['bac_valuation']['appraised_value']\n        result['preferred_methodology'] = 'BAC'\n    else:\n        result['preferred_valuation'] = None\n        result['preferred_methodology'] = None\n    \n    return result\n\n\ndef generate_kilburn_from_extraction(extracted_data: Dict) -> Dict:\n    \"\"\"\n    Generate Kilburn valuation from AI-extracted document data\n    \n    Args:\n        extracted_data: Data extracted from documents by GPT-5.1\n    \n    Returns:\n        Complete Kilburn/Cost Approach valuation\n    \"\"\"\n    project_info = extracted_data.get('project_info', {})\n    exploration = extracted_data.get('exploration', {})\n    \n    regional = exploration.get('regional_prospectivity', 0)\n    if regional == 0 or regional is None:\n        stage = project_info.get('development_stage', '').lower()\n        if 'production' in stage or 'construction' in stage:\n            regional = 4\n        elif 'feasibility' in stage or 'permitted' in stage:\n            regional = 3\n        elif 'advanced' in stage or 'pre_feasibility' in stage:\n            regional = 3\n        elif 'exploration' in stage:\n            regional = 2\n        else:\n            regional = 2\n    regional = max(1, min(4, int(regional)))\n    \n    maturity = exploration.get('project_maturity_score', 0)\n    if maturity == 0 or maturity is None:\n        stage = project_info.get('development_stage', '').lower()\n        if 'production' in stage:\n            maturity = 4\n        elif 'construction' in stage or 'permitted' in stage:\n            maturity = 4\n        elif 'feasibility' in stage:\n            maturity = 3\n        elif 'pre_feasibility' in stage or 'pfs' in stage:\n            maturity = 3\n        elif 'advanced' in stage:\n            maturity = 2\n        else:\n            maturity = 2\n    maturity = max(1, min(4, int(maturity)))\n    \n    geology = exploration.get('local_geology_score', 0)\n    if geology == 0 or geology is None:\n        resources = extracted_data.get('resources', {})\n        total_mi = resources.get('total_mi_contained_metal', 0)\n        if total_mi > 0:\n            geology = 3\n        else:\n            geology = 2\n    geology = max(1, min(4, int(geology)))\n    \n    data_quality = exploration.get('analytical_data_quality', 0)\n    if data_quality == 0 or data_quality is None:\n        report_type = extracted_data.get('data_quality', {}).get('report_type', '')\n        if '43-101' in report_type or 'JORC' in report_type:\n            data_quality = 3\n        else:\n            data_quality = 2\n    data_quality = max(1, min(4, int(data_quality)))\n    \n    exploration_spend = exploration.get('historical_exploration_spend', 0)\n    if exploration_spend == 0:\n        drill_meters = exploration.get('drill_meters_completed', 0)\n        exploration_spend = drill_meters * 0.0003\n    \n    area_km2 = project_info.get('property_area_km2', 0)\n    area_hectares = area_km2 * 100 if area_km2 > 0 else 1000\n    \n    location = project_info.get('location', 'Unknown')\n    jurisdiction = project_info.get('jurisdiction', 'Tier 2')\n    if 'canada' in location.lower() or 'australia' in location.lower() or 'usa' in location.lower():\n        region = 'North America'\n    elif 'chile' in location.lower() or 'peru' in location.lower() or 'mexico' in location.lower():\n        region = 'South America'\n    elif 'africa' in location.lower():\n        region = 'Africa'\n    else:\n        region = 'North America'\n    \n    result = calculate_kilburn_valuation(\n        regional_prospectivity=regional,\n        project_maturity=maturity,\n        local_geology=geology,\n        analytical_data=data_quality,\n        exploration_expenditure=exploration_spend,\n        area_hectares=area_hectares,\n        region=region,\n        years_since_expenditure=0\n    )\n    \n    preferred_value = result.get('preferred_valuation', 0)\n    composite_rating = result.get('geoscientific_rating', {}).get('composite_rating', 0)\n    if preferred_value and preferred_value > 0:\n        if composite_rating >= 3.0:\n            recommendation = \"Strong Floor Value - Exploration upside significant\"\n            color = \"green\"\n        elif composite_rating >= 2.5:\n            recommendation = \"Solid Floor Value - Additional work warranted\"\n            color = \"blue\"\n        else:\n            recommendation = \"Low Floor Value - High exploration risk\"\n            color = \"orange\"\n    else:\n        recommendation = \"Insufficient Data - Cannot calculate floor value\"\n        color = \"red\"\n    \n    result['recommendation'] = {\n        'text': recommendation,\n        'color': color\n    }\n    \n    result['valuation_summary'] = {\n        'recommended_value': preferred_value if preferred_value else 0,\n        'methodology': result.get('preferred_methodology', 'Unknown'),\n        'pem': result.get('pem', 1.0),\n        'average_rating': result.get('geoscientific_rating', {}).get('composite_rating', 0)\n    }\n    \n    return result\n\n\ndef render_kilburn_analysis():\n    \"\"\"Render the Kilburn/Cost Approach analysis UI in Streamlit\"\"\"\n    st.markdown(\"\"\"\n    <div style='background: linear-gradient(135deg, #4a148c, #7b1fa2); padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n        <h3 style='color: white; margin: 0;'>ðŸ”¬ PwC Cost Approach Valuation</h3>\n        <p style='color: #e0e0e0; margin: 5px 0 0 0;'>Kilburn geoscientific rating with MEE methodology</p>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    st.markdown(\"\"\"\n    The Cost Approach is the preferred methodology for early-stage exploration properties \n    where DCF analysis is not appropriate. It uses geoscientific rating to adjust historical \n    exploration expenditure.\n    \"\"\")\n    \n    st.markdown(\"### Geoscientific Rating Factors\")\n    st.markdown(\"Rate each factor from 1 (Low) to 4 (Very High)\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.markdown(\"#### Regional Prospectivity\")\n        regional_prospectivity = st.slider(\n            \"Geological setting and regional mineral potential\",\n            min_value=1, max_value=4, value=2,\n            help=\"1=Limited potential | 2=Some indicators | 3=Favorable setting | 4=Proven district\"\n        )\n        \n        st.markdown(f\"**Selected:** {RATING_FACTORS['regional_prospectivity']['levels'][regional_prospectivity]['name']}\")\n        st.caption(RATING_FACTORS['regional_prospectivity']['levels'][regional_prospectivity]['desc'])\n        \n        st.markdown(\"---\")\n        \n        st.markdown(\"#### Project Maturity\")\n        project_maturity = st.slider(\n            \"Stage of exploration and technical advancement\",\n            min_value=1, max_value=4, value=2,\n            help=\"1=Grassroots | 2=Early Stage | 3=Advanced | 4=Pre-Development\"\n        )\n        \n        st.markdown(f\"**Selected:** {RATING_FACTORS['project_maturity']['levels'][project_maturity]['name']}\")\n        st.caption(RATING_FACTORS['project_maturity']['levels'][project_maturity]['desc'])\n    \n    with col2:\n        st.markdown(\"#### Local Geology\")\n        local_geology = st.slider(\n            \"On-property geological indicators\",\n            min_value=1, max_value=4, value=2,\n            help=\"1=Poor indicators | 2=Fair | 3=Good | 4=Excellent mineralization\"\n        )\n        \n        st.markdown(f\"**Selected:** {RATING_FACTORS['local_geology']['levels'][local_geology]['name']}\")\n        st.caption(RATING_FACTORS['local_geology']['levels'][local_geology]['desc'])\n        \n        st.markdown(\"---\")\n        \n        st.markdown(\"#### Analytical Data Quality\")\n        analytical_data = st.slider(\n            \"Quality and reliability of technical data\",\n            min_value=1, max_value=4, value=2,\n            help=\"1=Poor QAQC | 2=Fair | 3=Good protocols | 4=Excellent verification\"\n        )\n        \n        st.markdown(f\"**Selected:** {RATING_FACTORS['analytical_data']['levels'][analytical_data]['name']}\")\n        st.caption(RATING_FACTORS['analytical_data']['levels'][analytical_data]['desc'])\n    \n    st.markdown(\"---\")\n    st.markdown(\"### Cost Inputs\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        exploration_expenditure = st.number_input(\n            \"Historical Exploration Expenditure (USD)\",\n            min_value=0.0,\n            value=5000000.0,\n            step=500000.0,\n            format=\"%.0f\",\n            help=\"Total exploration spending to date\"\n        )\n        \n        years_since = st.number_input(\n            \"Years Since Expenditure\",\n            min_value=0,\n            max_value=20,\n            value=2,\n            help=\"For inflation adjustment\"\n        )\n    \n    with col2:\n        area_hectares = st.number_input(\n            \"Property Area (hectares)\",\n            min_value=0.0,\n            value=10000.0,\n            step=1000.0,\n            format=\"%.0f\",\n            help=\"Total area of mineral claims\"\n        )\n        \n        region = st.selectbox(\n            \"Geographic Region\",\n            [\"North America\", \"South America\", \"Australia\", \"Africa\", \"Europe\", \"Asia\"],\n            help=\"Region affects base acquisition cost\"\n        )\n    \n    st.markdown(\"---\")\n    \n    if st.button(\"ðŸ”¬ Calculate Kilburn Valuation\", type=\"primary\", use_container_width=True):\n        with st.spinner(\"Calculating cost approach valuation...\"):\n            # Calculate valuation\n            result = calculate_kilburn_valuation(\n                regional_prospectivity=regional_prospectivity,\n                project_maturity=project_maturity,\n                local_geology=local_geology,\n                analytical_data=analytical_data,\n                exploration_expenditure=exploration_expenditure,\n                area_hectares=area_hectares,\n                region=region,\n                years_since_expenditure=years_since\n            )\n            \n            # Store in session state\n            st.session_state['kilburn_result'] = result\n            \n            # Display results\n            st.markdown(\"### ðŸ“Š Valuation Results\")\n            \n            # Geoscientific rating summary\n            rating_data = result['geoscientific_rating']\n            \n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                st.metric(\n                    \"Composite Rating\",\n                    f\"{rating_data['composite_rating']:.2f}/4.00\",\n                    help=\"Average of all four factors\"\n                )\n            \n            with col2:\n                st.metric(\n                    \"Rating Category\",\n                    rating_data['category'].replace('_', ' ').title(),\n                    help=\"Overall prospectivity category\"\n                )\n            \n            with col3:\n                st.metric(\n                    \"PEM Multiplier\",\n                    f\"{result['pem']:.2f}x\",\n                    help=\"Prospectivity Enhancement Multiplier\"\n                )\n            \n            # Rating breakdown\n            st.markdown(\"#### Rating Factor Breakdown\")\n            \n            factor_cols = st.columns(4)\n            factors = [\n                ('regional_prospectivity', 'ðŸŒ Regional', rating_data['regional_prospectivity']),\n                ('project_maturity', 'ðŸ“ˆ Maturity', rating_data['project_maturity']),\n                ('local_geology', 'â›°ï¸ Local', rating_data['local_geology']),\n                ('analytical_data', 'ðŸ“Š Data', rating_data['analytical_data'])\n            ]\n            \n            for i, (key, label, value) in enumerate(factors):\n                with factor_cols[i]:\n                    color = ['#ef5350', '#ff9800', '#4caf50', '#2196f3'][value - 1]\n                    st.markdown(f\"\"\"\n                    <div style='background: {color}; padding: 10px; border-radius: 8px; text-align: center; color: white;'>\n                        <div style='font-size: 12px;'>{label}</div>\n                        <div style='font-size: 24px; font-weight: bold;'>{value}/4</div>\n                    </div>\n                    \"\"\", unsafe_allow_html=True)\n            \n            st.markdown(\"---\")\n            \n            # MEE Valuation\n            if 'mee_valuation' in result:\n                mee = result['mee_valuation']\n                st.markdown(\"#### MEE (Multiple of Exploration Expenditure)\")\n                \n                col1, col2, col3 = st.columns(3)\n                with col1:\n                    st.markdown(f\"**Expenditure:** ${mee['exploration_expenditure']:,.0f}\")\n                    st.markdown(f\"**Inflation Adj:** {mee['inflation_adjustment']:.2f}x\")\n                with col2:\n                    st.markdown(f\"**Adjusted Base:** ${mee['adjusted_expenditure']:,.0f}\")\n                    st.markdown(f\"**PEM Applied:** {mee['pem']:.2f}x\")\n                with col3:\n                    st.markdown(f\"**Appraised Value:** ${mee['appraised_value']:,.0f}\")\n                    st.markdown(f\"**Range:** ${mee['value_range']['low']:,.0f} - ${mee['value_range']['high']:,.0f}\")\n            \n            # BAC Valuation\n            if 'bac_valuation' in result:\n                bac = result['bac_valuation']\n                st.markdown(\"#### BAC (Base Acquisition Cost)\")\n                \n                col1, col2, col3 = st.columns(3)\n                with col1:\n                    st.markdown(f\"**Area:** {bac['area_hectares']:,.0f} hectares\")\n                    st.markdown(f\"**BAC/ha:** ${bac['bac_per_hectare']:.2f}\")\n                with col2:\n                    st.markdown(f\"**Base Value:** ${bac['base_value']:,.0f}\")\n                    st.markdown(f\"**PEM Applied:** {bac['pem']:.2f}x\")\n                with col3:\n                    st.markdown(f\"**Appraised Value:** ${bac['appraised_value']:,.0f}\")\n                    st.markdown(f\"**Range:** ${bac['value_range']['low']:,.0f} - ${bac['value_range']['high']:,.0f}\")\n            \n            # Preferred Valuation\n            if result.get('preferred_valuation'):\n                st.markdown(\"---\")\n                st.markdown(\"#### ðŸŽ¯ Recommended Valuation\")\n                \n                st.markdown(f\"\"\"\n                <div style='background: linear-gradient(135deg, #1b5e20, #388e3c); padding: 20px; border-radius: 10px; text-align: center;'>\n                    <div style='color: #a5d6a7; font-size: 14px;'>Preferred Methodology: {result['preferred_methodology']}</div>\n                    <div style='color: white; font-size: 32px; font-weight: bold;'>${result['preferred_valuation']:,.0f}</div>\n                    <div style='color: #a5d6a7; font-size: 12px;'>Based on higher of MEE or BAC approach</div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n            \n            st.success(\"Cost approach valuation complete!\")\n\n\ndef get_kilburn_for_report(\n    ratings: Dict,\n    exploration_expenditure: float = 0,\n    area_hectares: float = 0,\n    region: str = 'North America'\n) -> Optional[Dict]:\n    \"\"\"\n    Generate Kilburn valuation for PDF report\n    \"\"\"\n    if not ratings:\n        return None\n    \n    return calculate_kilburn_valuation(\n        regional_prospectivity=ratings.get('regional_prospectivity', 2),\n        project_maturity=ratings.get('project_maturity', 2),\n        local_geology=ratings.get('local_geology', 2),\n        analytical_data=ratings.get('analytical_data', 2),\n        exploration_expenditure=exploration_expenditure,\n        area_hectares=area_hectares,\n        region=region\n    )\n","path":null,"size_bytes":26208,"size_tokens":null},"monte_carlo_engine.py":{"content":"\"\"\"\nMonte Carlo Risk Modeling Engine\nImplements probabilistic NPV analysis with commodity price simulation\n\"\"\"\n\nimport streamlit as st\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\nfrom datetime import datetime\nimport json\nfrom format_utils import format_currency\n\n\n# Default simulation parameters\nDEFAULT_SIMULATIONS = 10000\nDEFAULT_PROJECT_LIFE = 15  # years\n\n# Commodity price volatility (annual standard deviation)\nCOMMODITY_VOLATILITY = {\n    'gold': 0.15,      # 15% annual volatility\n    'silver': 0.25,    # 25% annual volatility\n    'copper': 0.20,    # 20% annual volatility\n    'zinc': 0.22,      # 22% annual volatility\n    'nickel': 0.28,    # 28% annual volatility\n    'lithium': 0.35,   # 35% annual volatility (high volatility commodity)\n    'uranium': 0.30,   # 30% annual volatility\n}\n\n# Current spot prices (USD) - would be fetched from API in production\nDEFAULT_SPOT_PRICES = {\n    'gold': 2000.0,     # $/oz\n    'silver': 25.0,     # $/oz\n    'copper': 4.00,     # $/lb\n    'zinc': 1.20,       # $/lb\n    'nickel': 8.50,     # $/lb\n    'lithium': 25000,   # $/tonne (spodumene concentrate)\n    'uranium': 65.0,    # $/lb U3O8\n}\n\n\ndef simulate_price_paths(\n    spot_price: float,\n    volatility: float,\n    years: int,\n    num_simulations: int,\n    drift: float = 0.0,\n    mean_reversion: bool = True,\n    reversion_speed: float = 0.15,\n    seed: int = None\n) -> np.ndarray:\n    \"\"\"\n    Simulate commodity price paths using Geometric Brownian Motion\n    with optional mean reversion (Ornstein-Uhlenbeck process)\n    \n    Args:\n        spot_price: Current spot price\n        volatility: Annual volatility (std dev)\n        years: Project life in years\n        num_simulations: Number of Monte Carlo simulations\n        drift: Expected annual drift (default 0 for commodities)\n        mean_reversion: Whether to apply mean reversion\n        reversion_speed: Speed of mean reversion\n        seed: Optional random seed for reproducibility (None for random)\n    \n    Returns:\n        numpy array of simulated price paths (num_simulations x years)\n    \"\"\"\n    dt = 1.0  # Annual time step\n    \n    # Initialize price array\n    prices = np.zeros((num_simulations, years))\n    prices[:, 0] = spot_price\n    \n    # Use a random state object instead of global seed for proper randomness\n    rng = np.random.RandomState(seed)\n    random_shocks = rng.normal(0, 1, (num_simulations, years - 1))\n    \n    for t in range(1, years):\n        if mean_reversion:\n            # Mean-reverting GBM (exponential form to ensure positive prices)\n            # Log-Ornstein-Uhlenbeck: d(log S) = theta*(log(mu) - log(S))dt + sigma*dW\n            log_mean = np.log(spot_price)\n            log_prices = np.log(np.maximum(prices[:, t-1], spot_price * 0.01))\n            log_prices_new = log_prices + reversion_speed * (log_mean - log_prices) * dt + \\\n                            volatility * np.sqrt(dt) * random_shocks[:, t-1]\n            prices[:, t] = np.exp(log_prices_new)\n        else:\n            # Standard GBM (log-normal, always positive)\n            prices[:, t] = prices[:, t-1] * np.exp(\n                (drift - 0.5 * volatility**2) * dt + \n                volatility * np.sqrt(dt) * random_shocks[:, t-1]\n            )\n    \n    # Floor at 10% of spot price to prevent extreme outliers\n    prices = np.maximum(prices, spot_price * 0.1)\n    \n    return prices\n\n\ndef calculate_npv_distribution(\n    annual_production: float,\n    unit_cost: float,\n    initial_capex: float,\n    price_paths: np.ndarray,\n    discount_rate: float = 0.08,\n    royalty_rate: float = 0.03,\n    tax_rate: float = 0.25\n) -> Dict:\n    \"\"\"\n    Calculate NPV distribution from simulated price paths\n    \n    Args:\n        annual_production: Annual production (oz, lbs, tonnes)\n        unit_cost: All-in sustaining cost per unit\n        initial_capex: Initial capital expenditure\n        price_paths: Simulated commodity price paths\n        discount_rate: Project discount rate\n        royalty_rate: Royalty rate on revenue\n        tax_rate: Corporate tax rate\n    \n    Returns:\n        Dictionary with NPV distribution statistics\n    \"\"\"\n    num_simulations, years = price_paths.shape\n    \n    # Calculate annual cash flows for each simulation\n    npvs = np.zeros(num_simulations)\n    \n    for sim in range(num_simulations):\n        cash_flows = []\n        cumulative_cf = -initial_capex\n        \n        for year in range(years):\n            price = price_paths[sim, year]\n            revenue = annual_production * price\n            royalty = revenue * royalty_rate\n            net_revenue = revenue - royalty\n            operating_cost = annual_production * unit_cost\n            ebitda = net_revenue - operating_cost\n            \n            # Apply tax if positive\n            if ebitda > 0:\n                tax = ebitda * tax_rate\n                net_cash_flow = ebitda - tax\n            else:\n                net_cash_flow = ebitda\n            \n            cash_flows.append(net_cash_flow)\n        \n        # Calculate NPV\n        discount_factors = [(1 + discount_rate) ** -(t+1) for t in range(years)]\n        npv = -initial_capex + sum(cf * df for cf, df in zip(cash_flows, discount_factors))\n        npvs[sim] = npv\n    \n    # Calculate statistics\n    npv_mean = np.mean(npvs)\n    npv_std = np.std(npvs)\n    npv_median = np.median(npvs)\n    npv_p10 = np.percentile(npvs, 10)\n    npv_p25 = np.percentile(npvs, 25)\n    npv_p75 = np.percentile(npvs, 75)\n    npv_p90 = np.percentile(npvs, 90)\n    \n    # Value at Risk (VaR) - 5% worst case\n    var_5 = np.percentile(npvs, 5)\n    \n    # Probability of positive NPV\n    prob_positive = np.mean(npvs > 0)\n    \n    # Probability of exceeding 10% IRR threshold (simplified)\n    breakeven_npv = initial_capex * 0.1\n    prob_exceed_hurdle = np.mean(npvs > breakeven_npv)\n    \n    return {\n        'mean': npv_mean,\n        'std': npv_std,\n        'median': npv_median,\n        'p50': npv_median,\n        'p10': npv_p10,\n        'p25': npv_p25,\n        'p75': npv_p75,\n        'p90': npv_p90,\n        'var_5': var_5,\n        'prob_positive': prob_positive,\n        'prob_exceed_hurdle': prob_exceed_hurdle,\n        'npv_distribution': npvs.tolist(),\n        'num_simulations': num_simulations\n    }\n\n\ndef run_monte_carlo_simulation(\n    commodity: str,\n    spot_price: float,\n    annual_production: float,\n    unit_cost: float,\n    initial_capex: float,\n    project_life: int = DEFAULT_PROJECT_LIFE,\n    num_simulations: int = DEFAULT_SIMULATIONS,\n    discount_rate: float = 0.08,\n    royalty_rate: float = 0.03,\n    tax_rate: float = 0.25,\n    custom_volatility: Optional[float] = None\n) -> Dict:\n    \"\"\"\n    Run full Monte Carlo simulation for project valuation\n    \"\"\"\n    # Get volatility\n    commodity_lower = commodity.lower()\n    if custom_volatility:\n        volatility = custom_volatility\n    else:\n        volatility = COMMODITY_VOLATILITY.get(commodity_lower, 0.20)\n    \n    # Simulate price paths\n    price_paths = simulate_price_paths(\n        spot_price=spot_price,\n        volatility=volatility,\n        years=project_life,\n        num_simulations=num_simulations\n    )\n    \n    # Calculate NPV distribution\n    npv_stats = calculate_npv_distribution(\n        annual_production=annual_production,\n        unit_cost=unit_cost,\n        initial_capex=initial_capex,\n        price_paths=price_paths,\n        discount_rate=discount_rate,\n        royalty_rate=royalty_rate,\n        tax_rate=tax_rate\n    )\n    \n    # Calculate price statistics\n    final_prices = price_paths[:, -1]\n    price_stats = {\n        'initial': spot_price,\n        'mean_final': np.mean(final_prices),\n        'std_final': np.std(final_prices),\n        'p10_final': np.percentile(final_prices, 10),\n        'p90_final': np.percentile(final_prices, 90),\n    }\n    \n    # Real options value (simplified - premium over static NPV)\n    # Real options typically add 30-50% value vs static DCF\n    static_npv = npv_stats['mean']\n    option_premium = abs(static_npv) * 0.35 if static_npv > 0 else abs(static_npv) * 0.20\n    real_options_value = static_npv + option_premium\n    \n    npv_mean = npv_stats['mean']\n    npv_p50 = npv_stats['p50']\n    \n    if npv_mean > 0 and npv_stats['prob_positive'] > 0.70:\n        recommendation = \"Favorable Risk Profile - High probability of positive returns\"\n        color = \"green\"\n    elif npv_mean > 0 and npv_stats['prob_positive'] > 0.50:\n        recommendation = \"Moderate Risk - Positive expected value with significant downside\"\n        color = \"blue\"\n    elif npv_stats['prob_positive'] > 0.30:\n        recommendation = \"High Risk - Substantial probability of loss\"\n        color = \"orange\"\n    else:\n        recommendation = \"Very High Risk - Majority of scenarios show losses\"\n        color = \"red\"\n    \n    return {\n        'commodity': commodity,\n        'input_parameters': {\n            'spot_price': spot_price,\n            'annual_production': annual_production,\n            'unit_cost': unit_cost,\n            'initial_capex': initial_capex,\n            'project_life': project_life,\n            'discount_rate': discount_rate,\n            'volatility': volatility,\n            'num_simulations': num_simulations\n        },\n        'npv_statistics': npv_stats,\n        'price_statistics': price_stats,\n        'real_options_value': real_options_value,\n        'option_premium_pct': (real_options_value / max(static_npv, 1) - 1) * 100 if static_npv > 0 else 0,\n        'calculation_date': datetime.now().isoformat(),\n        'recommendation': {\n            'text': recommendation,\n            'color': color\n        }\n    }\n\n\ndef run_full_monte_carlo_analysis(\n    commodity: str,\n    annual_production: float,\n    unit_cost: float,\n    initial_capex: float,\n    spot_price: float = None,\n    years: int = 15,\n    discount_rate: float = 0.08,\n    num_simulations: int = 10000\n) -> Dict:\n    \"\"\"\n    Run Monte Carlo analysis with automatic spot price detection\n    Used by Advanced AI analyzer\n    \"\"\"\n    commodity_lower = commodity.lower() if commodity else 'gold'\n    \n    if spot_price is None or spot_price <= 0:\n        spot_price = DEFAULT_SPOT_PRICES.get(commodity_lower, 2000.0)\n    \n    return run_monte_carlo_simulation(\n        commodity=commodity,\n        spot_price=spot_price,\n        annual_production=annual_production,\n        unit_cost=unit_cost,\n        initial_capex=initial_capex,\n        project_life=years,\n        num_simulations=num_simulations,\n        discount_rate=discount_rate\n    )\n\n\ndef calculate_sensitivity_tornado(\n    base_params: Dict,\n    variation_pct: float = 0.20\n) -> Dict:\n    \"\"\"\n    Calculate sensitivity analysis (tornado diagram data)\n    \"\"\"\n    sensitivities = {}\n    \n    # Parameters to vary\n    params_to_vary = [\n        ('spot_price', 'Commodity Price'),\n        ('unit_cost', 'Operating Cost'),\n        ('initial_capex', 'Capital Cost'),\n        ('annual_production', 'Production Rate'),\n        ('discount_rate', 'Discount Rate')\n    ]\n    \n    # Run base case\n    base_result = run_monte_carlo_simulation(**base_params, num_simulations=1000)\n    base_npv = base_result['npv_statistics']['mean']\n    \n    for param_key, param_name in params_to_vary:\n        base_value = base_params[param_key]\n        \n        # Low case\n        low_params = base_params.copy()\n        if param_key in ['unit_cost', 'initial_capex', 'discount_rate']:\n            low_params[param_key] = base_value * (1 + variation_pct)  # Higher is worse\n        else:\n            low_params[param_key] = base_value * (1 - variation_pct)\n        \n        low_result = run_monte_carlo_simulation(**low_params, num_simulations=1000)\n        low_npv = low_result['npv_statistics']['mean']\n        \n        # High case\n        high_params = base_params.copy()\n        if param_key in ['unit_cost', 'initial_capex', 'discount_rate']:\n            high_params[param_key] = base_value * (1 - variation_pct)  # Lower is better\n        else:\n            high_params[param_key] = base_value * (1 + variation_pct)\n        \n        high_result = run_monte_carlo_simulation(**high_params, num_simulations=1000)\n        high_npv = high_result['npv_statistics']['mean']\n        \n        sensitivities[param_name] = {\n            'low_npv': low_npv,\n            'base_npv': base_npv,\n            'high_npv': high_npv,\n            'low_delta': low_npv - base_npv,\n            'high_delta': high_npv - base_npv,\n            'range': high_npv - low_npv\n        }\n    \n    return {\n        'base_npv': base_npv,\n        'sensitivities': sensitivities,\n        'variation_pct': variation_pct\n    }\n\n\ndef render_monte_carlo_analysis():\n    \"\"\"Render the Monte Carlo Risk Modeling UI in Streamlit\"\"\"\n    st.markdown(\"\"\"\n    <div style='background: linear-gradient(135deg, #b71c1c, #e53935); padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n        <h3 style='color: white; margin: 0;'>ðŸŽ² Monte Carlo Risk Modeling</h3>\n        <p style='color: #ffcdd2; margin: 5px 0 0 0;'>Probabilistic NPV analysis with real options value</p>\n    </div>\n    \"\"\", unsafe_allow_html=True)\n    \n    st.markdown(\"\"\"\n    Monte Carlo simulation models uncertainty in commodity prices and project economics \n    to generate a probability distribution of project NPV outcomes. This provides \n    a more realistic assessment of project risk than static DCF analysis.\n    \"\"\")\n    \n    st.markdown(\"### Project Parameters\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        commodity = st.selectbox(\n            \"Primary Commodity\",\n            [\"Gold\", \"Silver\", \"Copper\", \"Zinc\", \"Nickel\", \"Lithium\", \"Uranium\"],\n            help=\"Select commodity for volatility assumptions\"\n        )\n        \n        # Get default spot price\n        commodity_lower = commodity.lower()\n        default_price = DEFAULT_SPOT_PRICES.get(commodity_lower, 2000)\n        \n        spot_price = st.number_input(\n            \"Current Spot Price (USD)\",\n            min_value=0.1,\n            value=float(default_price),\n            step=float(default_price * 0.05),\n            format=\"%.2f\",\n            help=\"Current commodity price\"\n        )\n        \n        annual_production = st.number_input(\n            \"Annual Production (units)\",\n            min_value=0.0,\n            value=100000.0,\n            step=10000.0,\n            format=\"%.0f\",\n            help=\"Annual production in oz (precious) or lbs/tonnes (base metals)\"\n        )\n        \n        project_life = st.slider(\n            \"Project Life (years)\",\n            min_value=5,\n            max_value=30,\n            value=15,\n            help=\"Expected mine life\"\n        )\n    \n    with col2:\n        unit_cost = st.number_input(\n            \"All-in Sustaining Cost (USD/unit)\",\n            min_value=0.0,\n            value=float(default_price * 0.6),\n            step=float(default_price * 0.05),\n            format=\"%.2f\",\n            help=\"Total operating cost per unit produced\"\n        )\n        \n        initial_capex = st.number_input(\n            \"Initial Capital (USD)\",\n            min_value=0.0,\n            value=500000000.0,\n            step=50000000.0,\n            format=\"%.0f\",\n            help=\"Initial capital expenditure to build the mine\"\n        )\n        \n        discount_rate = st.slider(\n            \"Discount Rate (%)\",\n            min_value=5.0,\n            max_value=15.0,\n            value=8.0,\n            step=0.5,\n            help=\"Risk-adjusted discount rate\"\n        ) / 100\n        \n        num_simulations = st.select_slider(\n            \"Number of Simulations\",\n            options=[1000, 5000, 10000, 25000, 50000],\n            value=10000,\n            help=\"More simulations = more accurate but slower\"\n        )\n    \n    st.markdown(\"---\")\n    \n    with st.expander(\"âš™ï¸ Advanced Parameters\"):\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            royalty_rate = st.slider(\n                \"Royalty Rate (%)\",\n                min_value=0.0,\n                max_value=10.0,\n                value=3.0,\n                step=0.5\n            ) / 100\n            \n            custom_volatility = st.checkbox(\"Use Custom Volatility\")\n            if custom_volatility:\n                volatility = st.slider(\n                    \"Annual Price Volatility (%)\",\n                    min_value=5.0,\n                    max_value=50.0,\n                    value=COMMODITY_VOLATILITY.get(commodity_lower, 0.20) * 100,\n                    step=1.0\n                ) / 100\n            else:\n                volatility = None\n        \n        with col2:\n            tax_rate = st.slider(\n                \"Corporate Tax Rate (%)\",\n                min_value=0.0,\n                max_value=40.0,\n                value=25.0,\n                step=1.0\n            ) / 100\n    \n    st.markdown(\"---\")\n    \n    if st.button(\"ðŸŽ² Run Monte Carlo Simulation\", type=\"primary\", use_container_width=True):\n        with st.spinner(f\"Running {num_simulations:,} simulations...\"):\n            # Run simulation\n            result = run_monte_carlo_simulation(\n                commodity=commodity,\n                spot_price=spot_price,\n                annual_production=annual_production,\n                unit_cost=unit_cost,\n                initial_capex=initial_capex,\n                project_life=project_life,\n                num_simulations=num_simulations,\n                discount_rate=discount_rate,\n                royalty_rate=royalty_rate,\n                tax_rate=tax_rate,\n                custom_volatility=volatility\n            )\n            \n            # Store in session state\n            st.session_state['monte_carlo_result'] = result\n            \n            # Display results\n            st.markdown(\"### ðŸ“Š Simulation Results\")\n            \n            npv_stats = result['npv_statistics']\n            \n            # Key metrics\n            col1, col2, col3, col4 = st.columns(4)\n            \n            with col1:\n                st.metric(\n                    \"Mean NPV\",\n                    format_currency(npv_stats['mean']/1e6, decimals=1),\n                    help=\"Expected (average) NPV\"\n                )\n            \n            with col2:\n                st.metric(\n                    \"Median NPV\",\n                    format_currency(npv_stats['median']/1e6, decimals=1),\n                    help=\"50th percentile NPV\"\n                )\n            \n            with col3:\n                prob_color = \"normal\" if npv_stats['prob_positive'] >= 0.7 else \"off\"\n                st.metric(\n                    \"P(NPV > 0)\",\n                    f\"{npv_stats['prob_positive']*100:.1f}%\",\n                    help=\"Probability of positive NPV\"\n                )\n            \n            with col4:\n                st.metric(\n                    \"VaR (5%)\",\n                    format_currency(npv_stats['var_5']/1e6, decimals=1),\n                    help=\"5th percentile - worst 5% of outcomes\"\n                )\n            \n            st.markdown(\"---\")\n            \n            # NPV Distribution\n            st.markdown(\"#### NPV Probability Distribution\")\n            \n            # Create histogram data\n            npv_values = np.array(npv_stats['npv_distribution'])\n            \n            # Display percentile table\n            col1, col2 = st.columns(2)\n            \n            with col1:\n                st.markdown(\"\"\"\n                | Percentile | NPV |\n                |------------|-----|\n                | P10 (Downside) | ${:,.0f}M |\n                | P25 | ${:,.0f}M |\n                | P50 (Median) | ${:,.0f}M |\n                | P75 | ${:,.0f}M |\n                | P90 (Upside) | ${:,.0f}M |\n                \"\"\".format(\n                    npv_stats['p10']/1e6,\n                    npv_stats['p25']/1e6,\n                    npv_stats['median']/1e6,\n                    npv_stats['p75']/1e6,\n                    npv_stats['p90']/1e6\n                ))\n            \n            with col2:\n                # Risk metrics\n                st.markdown(\"#### Risk Metrics\")\n                \n                st.markdown(f\"**Standard Deviation:** {format_currency(npv_stats['std']/1e6, decimals=1)}\")\n                st.markdown(f\"**Coefficient of Variation:** {npv_stats['std']/max(abs(npv_stats['mean']), 1):.2f}\")\n                st.markdown(f\"**P90/P10 Range:** {format_currency((npv_stats['p90']-npv_stats['p10'])/1e6, decimals=1)}\")\n                st.markdown(f\"**Probability > Hurdle:** {npv_stats['prob_exceed_hurdle']*100:.1f}%\")\n            \n            st.markdown(\"---\")\n            \n            # Real Options Value\n            st.markdown(\"#### ðŸ’Ž Real Options Value\")\n            \n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                st.markdown(f\"\"\"\n                <div style='background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center;'>\n                    <div style='font-size: 14px; color: #666;'>Static DCF NPV</div>\n                    <div style='font-size: 24px; font-weight: bold; color: #1565c0;'>{format_currency(npv_stats['mean']/1e6, decimals=1)}</div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n            \n            with col2:\n                st.markdown(f\"\"\"\n                <div style='background: #fff3e0; padding: 15px; border-radius: 8px; text-align: center;'>\n                    <div style='font-size: 14px; color: #666;'>Option Premium</div>\n                    <div style='font-size: 24px; font-weight: bold; color: #e65100;'>+{result['option_premium_pct']:.0f}%</div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n            \n            with col3:\n                st.markdown(f\"\"\"\n                <div style='background: #e8f5e9; padding: 15px; border-radius: 8px; text-align: center;'>\n                    <div style='font-size: 14px; color: #666;'>Real Options Value</div>\n                    <div style='font-size: 24px; font-weight: bold; color: #2e7d32;'>{format_currency(result['real_options_value']/1e6, decimals=1)}</div>\n                </div>\n                \"\"\", unsafe_allow_html=True)\n            \n            st.info(\"\"\"\n            **Real Options Premium:** Mining projects have embedded optionality (ability to defer, \n            expand, or abandon) that adds value beyond static NPV. The premium typically ranges \n            from 30-50% for development-stage projects with price volatility.\n            \"\"\")\n            \n            # Price Statistics\n            st.markdown(\"#### ðŸ“ˆ Price Path Statistics\")\n            \n            price_stats = result['price_statistics']\n            \n            col1, col2 = st.columns(2)\n            \n            with col1:\n                st.markdown(f\"**Initial Price:** ${price_stats['initial']:,.2f}\")\n                st.markdown(f\"**Mean Final Price:** ${price_stats['mean_final']:,.2f}\")\n            \n            with col2:\n                st.markdown(f\"**P10 Final Price:** ${price_stats['p10_final']:,.2f}\")\n                st.markdown(f\"**P90 Final Price:** ${price_stats['p90_final']:,.2f}\")\n            \n            st.success(f\"Monte Carlo simulation complete! ({num_simulations:,} iterations)\")\n\n\ndef get_monte_carlo_for_report(\n    commodity: str,\n    spot_price: float,\n    annual_production: float,\n    unit_cost: float,\n    initial_capex: float,\n    project_life: int = 15\n) -> Optional[Dict]:\n    \"\"\"\n    Generate Monte Carlo results for PDF report\n    \"\"\"\n    if not all([spot_price, annual_production, unit_cost, initial_capex]):\n        return None\n    \n    return run_monte_carlo_simulation(\n        commodity=commodity,\n        spot_price=spot_price,\n        annual_production=annual_production,\n        unit_cost=unit_cost,\n        initial_capex=initial_capex,\n        project_life=project_life,\n        num_simulations=5000  # Reduced for report generation speed\n    )\n","path":null,"size_bytes":23815,"size_tokens":null},"probability_dcf_engine.py":{"content":"\"\"\"\nProbability-Weighted DCF Engine (Risk-Adjusted NPV)\nA valuation method where the DCF result is multiplied by explicit probabilities \nof success at each project stage (exploration, permitting, financing, construction).\n\"\"\"\n\nimport numpy as np\nfrom typing import Dict, Any, Optional, List\n\nSTAGE_SUCCESS_PROBABILITIES = {\n    'grassroots': {\n        'exploration_success': 0.10,\n        'resource_definition': 0.30,\n        'permitting_approval': 0.60,\n        'financing_secured': 0.50,\n        'construction_complete': 0.85,\n        'production_ramp': 0.90\n    },\n    'early_exploration': {\n        'exploration_success': 0.25,\n        'resource_definition': 0.45,\n        'permitting_approval': 0.65,\n        'financing_secured': 0.55,\n        'construction_complete': 0.85,\n        'production_ramp': 0.90\n    },\n    'advanced_exploration': {\n        'exploration_success': 0.50,\n        'resource_definition': 0.65,\n        'permitting_approval': 0.70,\n        'financing_secured': 0.60,\n        'construction_complete': 0.85,\n        'production_ramp': 0.92\n    },\n    'pre_feasibility': {\n        'exploration_success': 0.80,\n        'resource_definition': 0.85,\n        'permitting_approval': 0.75,\n        'financing_secured': 0.65,\n        'construction_complete': 0.88,\n        'production_ramp': 0.93\n    },\n    'feasibility': {\n        'exploration_success': 0.90,\n        'resource_definition': 0.92,\n        'permitting_approval': 0.80,\n        'financing_secured': 0.70,\n        'construction_complete': 0.90,\n        'production_ramp': 0.95\n    },\n    'permitted': {\n        'exploration_success': 1.00,\n        'resource_definition': 1.00,\n        'permitting_approval': 0.95,\n        'financing_secured': 0.75,\n        'construction_complete': 0.92,\n        'production_ramp': 0.95\n    },\n    'construction': {\n        'exploration_success': 1.00,\n        'resource_definition': 1.00,\n        'permitting_approval': 1.00,\n        'financing_secured': 0.90,\n        'construction_complete': 0.93,\n        'production_ramp': 0.96\n    },\n    'production': {\n        'exploration_success': 1.00,\n        'resource_definition': 1.00,\n        'permitting_approval': 1.00,\n        'financing_secured': 1.00,\n        'construction_complete': 1.00,\n        'production_ramp': 0.97\n    }\n}\n\nRISK_FACTOR_ADJUSTMENTS = {\n    'jurisdiction': {\n        'tier_1': 1.0,\n        'tier_2': 0.90,\n        'tier_3': 0.75,\n        'tier_4': 0.55\n    },\n    'commodity_risk': {\n        'gold': 1.0,\n        'silver': 0.95,\n        'copper': 0.95,\n        'lithium': 0.85,\n        'nickel': 0.90,\n        'zinc': 0.92,\n        'uranium': 0.80,\n        'rare_earth': 0.75\n    },\n    'technical_complexity': {\n        'simple': 1.0,\n        'moderate': 0.92,\n        'complex': 0.80,\n        'highly_complex': 0.65\n    }\n}\n\n\ndef calculate_stage_probability(\n    current_stage: str,\n    jurisdiction_tier: str = 'tier_2',\n    commodity: str = 'gold',\n    technical_complexity: str = 'moderate'\n) -> Dict[str, Any]:\n    \"\"\"\n    Calculate cumulative probability of reaching production\n    \n    Args:\n        current_stage: Current project development stage\n        jurisdiction_tier: Jurisdiction risk tier\n        commodity: Primary commodity\n        technical_complexity: Technical complexity level\n    \n    Returns:\n        Dictionary with stage probabilities and cumulative probability\n    \"\"\"\n    stage_key = current_stage.lower().replace(' ', '_').replace('-', '_')\n    if stage_key not in STAGE_SUCCESS_PROBABILITIES:\n        stage_key = 'early_exploration'\n    \n    base_probs = STAGE_SUCCESS_PROBABILITIES[stage_key].copy()\n    \n    jur_adj = RISK_FACTOR_ADJUSTMENTS['jurisdiction'].get(jurisdiction_tier, 0.85)\n    comm_adj = RISK_FACTOR_ADJUSTMENTS['commodity_risk'].get(commodity.lower(), 0.90)\n    tech_adj = RISK_FACTOR_ADJUSTMENTS['technical_complexity'].get(technical_complexity, 0.90)\n    \n    adjusted_probs = {}\n    cumulative_prob = 1.0\n    \n    for stage, prob in base_probs.items():\n        adjusted_prob = prob * jur_adj * comm_adj * tech_adj\n        adjusted_prob = min(adjusted_prob, 0.99)\n        adjusted_probs[stage] = round(adjusted_prob, 3)\n        cumulative_prob *= adjusted_prob\n    \n    return {\n        'stage_probabilities': adjusted_probs,\n        'cumulative_probability': round(cumulative_prob, 4),\n        'risk_adjustments': {\n            'jurisdiction': jur_adj,\n            'commodity': comm_adj,\n            'technical': tech_adj,\n            'combined': round(jur_adj * comm_adj * tech_adj, 3)\n        }\n    }\n\n\ndef calculate_probability_weighted_dcf(\n    base_npv: float,\n    base_irr: float,\n    current_stage: str,\n    project_life_years: int = 15,\n    discount_rate: float = 0.08,\n    annual_cash_flows: List[float] = None,\n    initial_capex: float = None,\n    annual_revenue: float = None,\n    annual_opex: float = None,\n    jurisdiction_tier: str = 'tier_2',\n    commodity: str = 'gold',\n    technical_complexity: str = 'moderate'\n) -> Dict[str, Any]:\n    \"\"\"\n    Calculate probability-weighted DCF valuation\n    \n    Args:\n        base_npv: Base case NPV ($ millions)\n        base_irr: Base case IRR (decimal)\n        current_stage: Current project stage\n        project_life_years: Project life in years\n        discount_rate: Discount rate (decimal)\n        annual_cash_flows: Optional list of annual cash flows\n        initial_capex: Initial capital expenditure ($ millions)\n        annual_revenue: Annual revenue ($ millions)\n        annual_opex: Annual operating costs ($ millions)\n        jurisdiction_tier: Jurisdiction risk tier\n        commodity: Primary commodity\n        technical_complexity: Technical complexity level\n    \n    Returns:\n        Comprehensive probability-weighted DCF analysis\n    \"\"\"\n    prob_analysis = calculate_stage_probability(\n        current_stage, \n        jurisdiction_tier, \n        commodity, \n        technical_complexity\n    )\n    \n    cumulative_prob = prob_analysis['cumulative_probability']\n    \n    risk_adjusted_npv = base_npv * cumulative_prob\n    \n    if annual_cash_flows is None and all([initial_capex, annual_revenue, annual_opex]):\n        annual_cash_flow = annual_revenue - annual_opex\n        annual_cash_flows = [-initial_capex] + [annual_cash_flow] * project_life_years\n    \n    npv_sensitivity = {}\n    if base_npv != 0:\n        for adj in [-0.20, -0.10, 0.10, 0.20]:\n            adjusted_npv = base_npv * (1 + adj)\n            risk_adj_npv = adjusted_npv * cumulative_prob\n            npv_sensitivity[f\"{adj*100:+.0f}%\"] = {\n                'base_npv': round(adjusted_npv, 2),\n                'risk_adjusted_npv': round(risk_adj_npv, 2)\n            }\n    \n    if risk_adjusted_npv > base_npv * 0.5:\n        recommendation = \"Strong Buy - High probability of value realization\"\n        color = \"green\"\n    elif risk_adjusted_npv > base_npv * 0.25:\n        recommendation = \"Buy - Moderate probability-adjusted upside\"\n        color = \"blue\"\n    elif risk_adjusted_npv > base_npv * 0.10:\n        recommendation = \"Hold - Significant execution risk embedded\"\n        color = \"orange\"\n    else:\n        recommendation = \"High Risk - Consider only with portfolio diversification\"\n        color = \"red\"\n    \n    return {\n        'base_case': {\n            'npv': round(base_npv, 2),\n            'irr': round(base_irr * 100, 2),\n            'project_life': project_life_years,\n            'discount_rate': round(discount_rate * 100, 2)\n        },\n        'probability_analysis': {\n            'current_stage': current_stage,\n            'stage_probabilities': prob_analysis['stage_probabilities'],\n            'cumulative_probability': cumulative_prob,\n            'probability_percent': round(cumulative_prob * 100, 2),\n            'risk_adjustments': prob_analysis['risk_adjustments']\n        },\n        'risk_adjusted_valuation': {\n            'risk_adjusted_npv': round(risk_adjusted_npv, 2),\n            'npv_discount_from_base': round((1 - cumulative_prob) * 100, 2),\n            'risk_adjusted_irr': round(base_irr * cumulative_prob * 100, 2)\n        },\n        'sensitivity_analysis': npv_sensitivity,\n        'recommendation': {\n            'text': recommendation,\n            'color': color\n        },\n        'methodology_notes': [\n            \"Probability-weighted DCF applies stage-gate success probabilities to base case NPV\",\n            f\"Current stage '{current_stage}' implies {cumulative_prob*100:.1f}% probability of full value realization\",\n            f\"Jurisdiction ({jurisdiction_tier}), commodity ({commodity}), and technical factors adjust base probabilities\",\n            \"Risk-adjusted NPV represents expected value considering all development risks\"\n        ]\n    }\n\n\ndef safe_float(value, default: float = 0.0) -> float:\n    \"\"\"Safely convert value to float with default\"\"\"\n    if value is None:\n        return default\n    try:\n        return float(value)\n    except (TypeError, ValueError):\n        return default\n\n\ndef safe_int(value, default: int = 0) -> int:\n    \"\"\"Safely convert value to int with default\"\"\"\n    if value is None:\n        return default\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return default\n\n\ndef generate_probability_dcf_from_extraction(extracted_data: Dict[str, Any], income_dcf_result: Dict[str, Any] = None) -> Dict[str, Any]:\n    \"\"\"\n    Generate Probability-Weighted DCF analysis from AI-extracted document data\n    \n    IMPORTANT: Uses calculated NPV from Income DCF engine, NOT document-reported NPV\n    \n    Args:\n        extracted_data: Data extracted from documents by GPT-5.1\n        income_dcf_result: Pre-calculated Income DCF result (used for base NPV/IRR)\n    \n    Returns:\n        Complete probability-weighted DCF valuation\n    \"\"\"\n    if not extracted_data:\n        return {\n            'error': 'insufficient_data',\n            'message': 'No data available for probability-weighted DCF calculation'\n        }\n    \n    economics = extracted_data.get('economics', {}) or {}\n    project_info = extracted_data.get('project_info', {}) or {}\n    production = extracted_data.get('production', {}) or {}\n    \n    # CRITICAL: Use calculated NPV from Income DCF, NOT document-reported NPV\n    # Document-reported NPVs can be unrealistic or from different assumptions\n    \n    # If Income DCF returned an error, propagate it - don't fabricate values\n    if income_dcf_result and 'error' in income_dcf_result:\n        return {\n            'error': 'insufficient_data',\n            'message': f\"Cannot calculate probability-weighted DCF: Income DCF failed - {income_dcf_result.get('message', 'insufficient data')}\",\n            'upstream_error': income_dcf_result.get('error'),\n            'missing_inputs': income_dcf_result.get('missing_inputs', [])\n        }\n    \n    if income_dcf_result and 'valuation_summary' in income_dcf_result:\n        base_npv = income_dcf_result['valuation_summary'].get('npv', 0)\n        base_irr = income_dcf_result['valuation_summary'].get('irr_percent', 0) / 100\n    else:\n        # No Income DCF result - try to calculate from first principles with STRICT validation\n        annual_prod = safe_float(production.get('annual_production') or economics.get('annual_production'), 0)\n        commodity_price = safe_float(economics.get('commodity_price'), 0)\n        aisc = safe_float(economics.get('aisc') or economics.get('all_in_sustaining_cost') or economics.get('operating_cost'), 0)\n        \n        # STRICT: Require ALL THREE inputs - no fabrication\n        missing_inputs = []\n        if annual_prod <= 0:\n            missing_inputs.append('annual_production')\n        if commodity_price <= 0:\n            missing_inputs.append('commodity_price')\n        if aisc <= 0:\n            missing_inputs.append('operating_cost')\n        \n        if len(missing_inputs) > 0:\n            return {\n                'error': 'insufficient_data',\n                'message': f'Cannot calculate probability-weighted DCF: missing {\", \".join(missing_inputs)}',\n                'missing_inputs': missing_inputs\n            }\n        \n        # Only calculate if we have all inputs\n        mine_life = safe_int(economics.get('mine_life'), 15) or 15\n        capex = safe_float(economics.get('initial_capex'), 0)\n        annual_margin = annual_prod * (commodity_price - aisc)\n        annual_margin_millions = annual_margin / 1_000_000\n        base_npv = annual_margin_millions * mine_life * 0.6 - capex\n        base_irr = 0.15 if base_npv > 0 else 0.05\n    \n    if base_npv <= 0:\n        return {\n            'error': 'insufficient_data',\n            'message': 'Cannot calculate probability-weighted DCF: calculated NPV is zero or negative',\n            'base_npv_calculated': base_npv\n        }\n    \n    current_stage = project_info.get('development_stage') or 'early_exploration'\n    project_life_raw = safe_int(economics.get('mine_life'), 15)\n    project_life = project_life_raw if project_life_raw > 0 else 15\n    raw_discount = safe_float(economics.get('discount_rate'), 8)\n    discount_rate = raw_discount / 100 if raw_discount > 1 else raw_discount if raw_discount > 0 else 0.08\n    \n    jurisdiction = project_info.get('jurisdiction', 'tier_2')\n    if isinstance(jurisdiction, str):\n        jur_lower = jurisdiction.lower()\n        if any(t in jur_lower for t in ['canada', 'australia', 'usa', 'tier 1', 'tier1']):\n            jurisdiction_tier = 'tier_1'\n        elif any(t in jur_lower for t in ['chile', 'peru', 'mexico', 'tier 2', 'tier2']):\n            jurisdiction_tier = 'tier_2'\n        elif any(t in jur_lower for t in ['tier 3', 'tier3', 'africa', 'asia']):\n            jurisdiction_tier = 'tier_3'\n        else:\n            jurisdiction_tier = 'tier_2'\n    else:\n        jurisdiction_tier = 'tier_2'\n    \n    commodity = project_info.get('primary_commodity', 'gold').lower()\n    \n    technical = project_info.get('technical_complexity') or 'moderate'\n    if isinstance(technical, str):\n        tech_lower = technical.lower()\n        if 'simple' in tech_lower or 'straightforward' in tech_lower:\n            technical_complexity = 'simple'\n        elif 'complex' in tech_lower or 'difficult' in tech_lower:\n            technical_complexity = 'complex'\n        elif 'highly' in tech_lower:\n            technical_complexity = 'highly_complex'\n        else:\n            technical_complexity = 'moderate'\n    else:\n        technical_complexity = 'moderate'\n    \n    capex = safe_float(economics.get('initial_capex'), 200)\n    revenue = safe_float(economics.get('annual_revenue'), 100)\n    opex = safe_float(economics.get('annual_opex'), 50)\n    \n    return calculate_probability_weighted_dcf(\n        base_npv=base_npv,\n        base_irr=base_irr,\n        current_stage=current_stage,\n        project_life_years=project_life,\n        discount_rate=discount_rate,\n        initial_capex=capex,\n        annual_revenue=revenue,\n        annual_opex=opex,\n        jurisdiction_tier=jurisdiction_tier,\n        commodity=commodity,\n        technical_complexity=technical_complexity\n    )\n","path":null,"size_bytes":15006,"size_tokens":null},"income_dcf_engine.py":{"content":"\"\"\"\nIncome Approach - Discounted Cash Flow (DCF) Engine\nA cash-flow projection model based on production schedules, operating costs, \ncapital costs, and commodity forecasts, discounted to present value.\n\"\"\"\n\nimport numpy as np\nfrom typing import Dict, Any, List, Optional\n\nDEFAULT_COMMODITY_PRICES = {\n    'gold': 2000.0,\n    'silver': 25.0,\n    'copper': 4.0,\n    'zinc': 1.20,\n    'nickel': 8.50,\n    'lithium': 25000.0,\n    'uranium': 65.0,\n    'platinum': 950.0,\n    'palladium': 1050.0\n}\n\nCOMMODITY_UNITS = {\n    'gold': 'oz',\n    'silver': 'oz',\n    'copper': 'lb',\n    'zinc': 'lb',\n    'nickel': 'lb',\n    'lithium': 'tonne',\n    'uranium': 'lb',\n    'platinum': 'oz',\n    'palladium': 'oz'\n}\n\n\ndef calculate_dcf_valuation(\n    commodity: str,\n    annual_production: float,\n    commodity_price: float = None,\n    mine_life_years: int = 15,\n    initial_capex: float = 0,\n    sustaining_capex_annual: float = 0,\n    all_in_sustaining_cost: float = None,\n    operating_cost_per_unit: float = None,\n    royalty_rate: float = 0.03,\n    tax_rate: float = 0.25,\n    discount_rate: float = 0.08,\n    production_ramp_years: int = 2,\n    closure_cost: float = 0,\n    working_capital: float = 0,\n    price_escalation: float = 0.02\n) -> Dict[str, Any]:\n    \"\"\"\n    Calculate comprehensive DCF valuation for a mining project\n    \n    Args:\n        commodity: Primary commodity\n        annual_production: Steady-state annual production (oz, lbs, tonnes)\n        commodity_price: Commodity price (uses default if None)\n        mine_life_years: Mine life in years\n        initial_capex: Initial capital expenditure ($ millions)\n        sustaining_capex_annual: Annual sustaining capital ($ millions)\n        all_in_sustaining_cost: AISC per unit (overrides operating cost if provided)\n        operating_cost_per_unit: Operating cost per unit\n        royalty_rate: Royalty rate (decimal)\n        tax_rate: Corporate tax rate (decimal)\n        discount_rate: Project discount rate (decimal)\n        production_ramp_years: Years to reach full production\n        closure_cost: Mine closure cost ($ millions)\n        working_capital: Net working capital requirement ($ millions)\n        price_escalation: Annual price escalation rate (decimal)\n    \n    Returns:\n        Comprehensive DCF valuation results\n    \"\"\"\n    commodity_lower = commodity.lower() if commodity else 'gold'\n    if commodity_price is None:\n        commodity_price = DEFAULT_COMMODITY_PRICES.get(commodity_lower, 2000.0)\n    \n    if all_in_sustaining_cost is not None:\n        operating_cost = all_in_sustaining_cost\n    elif operating_cost_per_unit is not None:\n        operating_cost = operating_cost_per_unit + (sustaining_capex_annual * 1_000_000 / annual_production if annual_production > 0 else 0)\n    else:\n        operating_cost = commodity_price * 0.65\n    \n    years = list(range(-1, mine_life_years + 1))\n    production_profile = []\n    price_profile = []\n    revenue_profile = []\n    operating_cost_profile = []\n    royalty_profile = []\n    ebitda_profile = []\n    sustaining_capex_profile = []\n    tax_profile = []\n    fcf_profile = []\n    \n    cumulative_production = 0\n    cumulative_revenue = 0\n    cumulative_fcf = 0\n    \n    for year in years:\n        if year == -1:\n            production = 0\n            price = commodity_price\n            revenue = 0\n            opex = 0\n            royalty = 0\n            ebitda = 0\n            sustaining = 0\n            tax = 0\n            fcf = -(initial_capex * 1_000_000) - (working_capital * 1_000_000)\n        elif year == 0:\n            production = 0\n            price = commodity_price\n            revenue = 0\n            opex = 0\n            royalty = 0\n            ebitda = 0\n            sustaining = 0\n            tax = 0\n            fcf = -(initial_capex * 0.5 * 1_000_000) if initial_capex > 0 else 0\n        elif year <= production_ramp_years:\n            ramp_factor = year / production_ramp_years\n            production = annual_production * ramp_factor\n            price = commodity_price * ((1 + price_escalation) ** year)\n            revenue = production * price\n            opex = production * operating_cost\n            royalty = revenue * royalty_rate\n            ebitda = revenue - opex - royalty\n            sustaining = sustaining_capex_annual * ramp_factor * 1_000_000\n            taxable_income = max(0, ebitda - sustaining)\n            tax = taxable_income * tax_rate\n            fcf = ebitda - sustaining - tax\n            cumulative_production += production\n            cumulative_revenue += revenue\n        elif year == mine_life_years:\n            production = annual_production\n            price = commodity_price * ((1 + price_escalation) ** year)\n            revenue = production * price\n            opex = production * operating_cost\n            royalty = revenue * royalty_rate\n            ebitda = revenue - opex - royalty\n            sustaining = sustaining_capex_annual * 1_000_000\n            taxable_income = max(0, ebitda - sustaining)\n            tax = taxable_income * tax_rate\n            fcf = ebitda - sustaining - tax - (closure_cost * 1_000_000) + (working_capital * 1_000_000)\n            cumulative_production += production\n            cumulative_revenue += revenue\n        else:\n            production = annual_production\n            price = commodity_price * ((1 + price_escalation) ** year)\n            revenue = production * price\n            opex = production * operating_cost\n            royalty = revenue * royalty_rate\n            ebitda = revenue - opex - royalty\n            sustaining = sustaining_capex_annual * 1_000_000\n            taxable_income = max(0, ebitda - sustaining)\n            tax = taxable_income * tax_rate\n            fcf = ebitda - sustaining - tax\n            cumulative_production += production\n            cumulative_revenue += revenue\n        \n        cumulative_fcf += fcf\n        \n        production_profile.append(production)\n        price_profile.append(price)\n        revenue_profile.append(revenue / 1_000_000 if revenue else 0)\n        operating_cost_profile.append(opex / 1_000_000 if opex else 0)\n        royalty_profile.append(royalty / 1_000_000 if royalty else 0)\n        ebitda_profile.append(ebitda / 1_000_000 if ebitda else 0)\n        sustaining_capex_profile.append(sustaining / 1_000_000 if sustaining else 0)\n        tax_profile.append(tax / 1_000_000 if tax else 0)\n        fcf_profile.append(fcf / 1_000_000)\n    \n    npv = 0\n    for i, fcf in enumerate(fcf_profile):\n        year = years[i]\n        if year >= 0:\n            npv += fcf / ((1 + discount_rate) ** year)\n        else:\n            npv += fcf / ((1 + discount_rate) ** (year))\n    \n    def calculate_irr(cash_flows):\n        if not cash_flows or all(cf == 0 for cf in cash_flows):\n            return 0\n        \n        for rate in np.arange(-0.50, 2.0, 0.001):\n            npv_test = 0\n            for i, cf in enumerate(cash_flows):\n                if (1 + rate) != 0:\n                    npv_test += cf / ((1 + rate) ** i)\n            if abs(npv_test) < 0.1:\n                return rate\n        return 0\n    \n    irr = calculate_irr(fcf_profile)\n    \n    payback_year = None\n    cumulative = 0\n    for i, fcf in enumerate(fcf_profile):\n        cumulative += fcf\n        if cumulative >= 0 and payback_year is None:\n            payback_year = years[i]\n    \n    total_capex = initial_capex + (sustaining_capex_annual * (mine_life_years - production_ramp_years))\n    total_opex = sum(operating_cost_profile)\n    total_revenue = sum(revenue_profile)\n    \n    if npv > 0:\n        if irr >= 0.25:\n            recommendation = \"Strong Investment - Excellent returns exceed hurdle rates\"\n            color = \"green\"\n        elif irr >= 0.15:\n            recommendation = \"Positive Investment - Solid risk-adjusted returns\"\n            color = \"blue\"\n        else:\n            recommendation = \"Marginal - Returns may not justify risk\"\n            color = \"orange\"\n    else:\n        recommendation = \"Not Economic - NPV is negative at current assumptions\"\n        color = \"red\"\n    \n    sensitivity = {}\n    for price_change in [-0.20, -0.10, 0.10, 0.20]:\n        test_price = commodity_price * (1 + price_change)\n        test_npv = npv * (1 + price_change * 2.5)\n        sensitivity[f\"Price {price_change*100:+.0f}%\"] = round(test_npv, 2)\n    \n    for cost_change in [-0.20, -0.10, 0.10, 0.20]:\n        test_npv = npv * (1 - cost_change * 1.5)\n        sensitivity[f\"OPEX {cost_change*100:+.0f}%\"] = round(test_npv, 2)\n    \n    return {\n        'valuation_summary': {\n            'npv': round(npv, 2),\n            'irr_percent': round(irr * 100, 2),\n            'payback_years': payback_year,\n            'discount_rate': round(discount_rate * 100, 2),\n            'mine_life': mine_life_years\n        },\n        'project_economics': {\n            'commodity': commodity,\n            'commodity_price': round(commodity_price, 2),\n            'annual_production': round(annual_production, 0),\n            'production_unit': COMMODITY_UNITS.get(commodity_lower, 'units'),\n            'aisc': round(operating_cost, 2),\n            'margin_per_unit': round(commodity_price - operating_cost, 2),\n            'margin_percent': round((commodity_price - operating_cost) / commodity_price * 100, 2) if commodity_price > 0 else 0\n        },\n        'capital_structure': {\n            'initial_capex': round(initial_capex, 2),\n            'sustaining_capex_annual': round(sustaining_capex_annual, 2),\n            'total_capex': round(total_capex, 2),\n            'working_capital': round(working_capital, 2),\n            'closure_cost': round(closure_cost, 2)\n        },\n        'cash_flow_metrics': {\n            'total_revenue': round(total_revenue, 2),\n            'total_opex': round(total_opex, 2),\n            'total_ebitda': round(sum(ebitda_profile), 2),\n            'total_free_cash_flow': round(cumulative_fcf / 1_000_000, 2),\n            'average_annual_fcf': round(np.mean([f for f in fcf_profile if f > 0]), 2) if any(f > 0 for f in fcf_profile) else 0\n        },\n        'production_summary': {\n            'cumulative_production': round(cumulative_production, 0),\n            'ramp_up_years': production_ramp_years,\n            'steady_state_years': mine_life_years - production_ramp_years\n        },\n        'sensitivity_analysis': sensitivity,\n        'recommendation': {\n            'text': recommendation,\n            'color': color\n        },\n        'cash_flow_schedule': {\n            'years': years,\n            'production': [round(p, 0) for p in production_profile],\n            'revenue': [round(r, 2) for r in revenue_profile],\n            'ebitda': [round(e, 2) for e in ebitda_profile],\n            'free_cash_flow': [round(f, 2) for f in fcf_profile]\n        },\n        'methodology_notes': [\n            f\"Base case DCF at {discount_rate*100:.0f}% discount rate over {mine_life_years} year mine life\",\n            f\"Production ramp-up over {production_ramp_years} years to steady state\",\n            f\"AISC of ${operating_cost:.2f}/{COMMODITY_UNITS.get(commodity_lower, 'unit')} vs ${commodity_price:.2f} commodity price\",\n            \"Tax rate applied after sustaining capital deductions\"\n        ]\n    }\n\n\ndef safe_float(value, default: float = 0.0) -> float:\n    \"\"\"Safely convert value to float with default\"\"\"\n    if value is None:\n        return default\n    try:\n        return float(value)\n    except (TypeError, ValueError):\n        return default\n\n\ndef safe_int(value, default: int = 0) -> int:\n    \"\"\"Safely convert value to int with default\"\"\"\n    if value is None:\n        return default\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return default\n\n\ndef generate_dcf_from_extraction(extracted_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Generate Income DCF analysis from AI-extracted document data\n    \n    Args:\n        extracted_data: Data extracted from documents by GPT-5.1\n    \n    Returns:\n        Complete DCF valuation or insufficient data message\n    \"\"\"\n    if not extracted_data:\n        return {\n            'error': 'insufficient_data',\n            'message': 'No data available for DCF calculation',\n            'missing_inputs': ['all data']\n        }\n    \n    economics = extracted_data.get('economics', {}) or {}\n    project_info = extracted_data.get('project_info', {}) or {}\n    production = extracted_data.get('production', {}) or {}\n    \n    commodity = project_info.get('primary_commodity') or 'gold'\n    \n    # Extract and validate critical inputs - ALL THREE are required for valid DCF\n    annual_prod_raw = safe_float(production.get('annual_production') or economics.get('annual_production'), 0)\n    commodity_price_raw = safe_float(economics.get('commodity_price'), 0)\n    aisc_raw = safe_float(economics.get('aisc') or economics.get('all_in_sustaining_cost'), 0)\n    opex_raw = safe_float(economics.get('operating_cost'), 0)\n    initial_capex_raw = safe_float(economics.get('initial_capex'), 0)\n    \n    # Track missing critical data\n    missing_inputs = []\n    if annual_prod_raw <= 0:\n        missing_inputs.append('annual_production')\n    if commodity_price_raw <= 0:\n        missing_inputs.append('commodity_price')\n    if aisc_raw <= 0 and opex_raw <= 0:\n        missing_inputs.append('operating_cost/AISC')\n    \n    # CRITICAL: Require ALL core inputs (production, price, cost) for valid DCF\n    # Without these three, any calculation is fabricated and misleading\n    if len(missing_inputs) > 0:\n        return {\n            'error': 'insufficient_data',\n            'message': f'Cannot calculate DCF: missing {\", \".join(missing_inputs)}. All three core inputs (annual production, commodity price, and operating cost/AISC) are required for a valid valuation.',\n            'missing_inputs': missing_inputs,\n            'valuation_summary': {\n                'npv': 0,\n                'irr_percent': 0,\n                'payback_years': -1,\n                'discount_rate': 8,\n                'mine_life': 0\n            }\n        }\n    \n    # Use validated values with sensible defaults only when we have enough data\n    annual_prod = annual_prod_raw if annual_prod_raw > 0 else 100000\n    commodity_price = commodity_price_raw if commodity_price_raw > 0 else None\n    aisc = aisc_raw if aisc_raw > 0 else None\n    opex = opex_raw if opex_raw > 0 else None\n    \n    mine_life_raw = safe_int(economics.get('mine_life'), 15)\n    mine_life = mine_life_raw if mine_life_raw > 0 else 15\n    \n    initial_capex = initial_capex_raw if initial_capex_raw > 0 else 200\n    sustaining_capex = safe_float(economics.get('sustaining_capex'), 10)\n    \n    raw_royalty = safe_float(economics.get('royalty_rate'), 3)\n    royalty = raw_royalty / 100 if raw_royalty > 1 else raw_royalty if raw_royalty > 0 else 0.03\n    \n    raw_tax = safe_float(economics.get('tax_rate'), 25)\n    tax = raw_tax / 100 if raw_tax > 1 else raw_tax if raw_tax > 0 else 0.25\n    \n    raw_discount = safe_float(economics.get('discount_rate'), 8)\n    discount = raw_discount / 100 if raw_discount > 1 else raw_discount if raw_discount > 0 else 0.08\n    \n    result = calculate_dcf_valuation(\n        commodity=commodity,\n        annual_production=annual_prod,\n        commodity_price=commodity_price,\n        mine_life_years=mine_life,\n        initial_capex=initial_capex,\n        sustaining_capex_annual=sustaining_capex,\n        all_in_sustaining_cost=aisc,\n        operating_cost_per_unit=opex,\n        royalty_rate=royalty,\n        tax_rate=tax,\n        discount_rate=discount,\n        production_ramp_years=2,\n        closure_cost=economics.get('closure_cost', 20),\n        working_capital=economics.get('working_capital', 15)\n    )\n    \n    # Add data quality indicators\n    result['data_quality'] = {\n        'inputs_used': 'extracted' if len(missing_inputs) == 0 else 'partial',\n        'missing_inputs': missing_inputs if missing_inputs else None,\n        'confidence': 'high' if len(missing_inputs) == 0 else 'medium' if len(missing_inputs) <= 1 else 'low'\n    }\n    \n    return result\n","path":null,"size_bytes":16039,"size_tokens":null},"advanced_ai_analyzer.py":{"content":"\"\"\"\nAdvanced AI Valuation Analyzer\nHybrid AI System:\n- GPT-5.1: Document extraction and financial data parsing (superior vision & JSON)\n- Claude Opus 4.5: Investment narratives and strategic analysis (superior reasoning)\nRuns all 5 professional valuation methodologies\n\"\"\"\n\nimport os\nimport json\nfrom typing import Dict, List, Any\nfrom openai import OpenAI\nfrom anthropic import Anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception\nfrom format_utils import format_currency\n\nfrom probability_dcf_engine import generate_probability_dcf_from_extraction\nfrom income_dcf_engine import generate_dcf_from_extraction\nfrom monte_carlo_engine import run_full_monte_carlo_analysis\nfrom kilburn_valuation import generate_kilburn_from_extraction\nfrom decision_tree_emv_engine import generate_emv_from_extraction\n\nAI_INTEGRATIONS_OPENAI_API_KEY = os.environ.get(\"AI_INTEGRATIONS_OPENAI_API_KEY\")\nAI_INTEGRATIONS_OPENAI_BASE_URL = os.environ.get(\"AI_INTEGRATIONS_OPENAI_BASE_URL\")\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n\nAI_INTEGRATIONS_ANTHROPIC_API_KEY = os.environ.get(\"AI_INTEGRATIONS_ANTHROPIC_API_KEY\")\nAI_INTEGRATIONS_ANTHROPIC_BASE_URL = os.environ.get(\"AI_INTEGRATIONS_ANTHROPIC_BASE_URL\")\n\nopenai_client = None\nif AI_INTEGRATIONS_OPENAI_API_KEY:\n    openai_client = OpenAI(\n        api_key=AI_INTEGRATIONS_OPENAI_API_KEY,\n        base_url=AI_INTEGRATIONS_OPENAI_BASE_URL\n    )\nelif OPENAI_API_KEY:\n    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n\nanthropic_client = None\nif AI_INTEGRATIONS_ANTHROPIC_API_KEY:\n    anthropic_client = Anthropic(\n        api_key=AI_INTEGRATIONS_ANTHROPIC_API_KEY,\n        base_url=AI_INTEGRATIONS_ANTHROPIC_BASE_URL\n    )\n\n\ndef safe_float(value, default: float = 0.0) -> float:\n    \"\"\"Safely convert value to float with default\"\"\"\n    if value is None:\n        return default\n    try:\n        return float(value)\n    except (TypeError, ValueError):\n        return default\n\n\ndef safe_int(value, default: int = 0) -> int:\n    \"\"\"Safely convert value to int with default\"\"\"\n    if value is None:\n        return default\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return default\n\n\ndef normalize_extracted_data(extracted_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Normalize extracted data by deriving missing values from related fields.\n    This helps when the LLM uses different field names or when values need calculation.\n    \n    Returns the normalized data with a report of what was derived.\n    \"\"\"\n    if \"error\" in extracted_data:\n        return extracted_data\n    \n    economics = extracted_data.get('economics', {}) or {}\n    production = extracted_data.get('production', {}) or {}\n    resources = extracted_data.get('resources', {}) or {}\n    project_info = extracted_data.get('project_info', {}) or {}\n    \n    derivations = []\n    \n    # Try to derive annual_production from various sources\n    annual_prod = safe_float(production.get('annual_production'), 0)\n    if annual_prod <= 0:\n        # Try life_of_mine_production / mine_life\n        lom_prod = safe_float(production.get('life_of_mine_production'), 0)\n        mine_life = safe_float(economics.get('mine_life'), 0)\n        if lom_prod > 0 and mine_life > 0:\n            annual_prod = lom_prod / mine_life\n            production['annual_production'] = annual_prod\n            derivations.append(f\"Derived annual_production ({annual_prod:.0f}) from life_of_mine_production / mine_life\")\n    \n    if annual_prod <= 0:\n        # Try throughput * recovery * 365 for gold (approximate)\n        throughput_tpd = safe_float(production.get('throughput_tpd'), 0)\n        recovery = safe_float(production.get('recovery_rate'), 0)\n        total_mi_grade = safe_float(resources.get('total_mi_grade'), 0)\n        if throughput_tpd > 0 and recovery > 0 and total_mi_grade > 0:\n            # For gold: throughput(t/day) * grade(g/t) * recovery * 365 / 31.1035 = oz/year\n            annual_prod = throughput_tpd * total_mi_grade * (recovery/100 if recovery > 1 else recovery) * 365 / 31.1035\n            production['annual_production'] = annual_prod\n            derivations.append(f\"Calculated annual_production ({annual_prod:.0f} oz) from throughput * grade * recovery\")\n    \n    # Try to derive commodity_price\n    commodity_price = safe_float(economics.get('commodity_price'), 0)\n    if commodity_price <= 0:\n        # Check commodity_price_assumption\n        price_assumption = safe_float(economics.get('commodity_price_assumption'), 0)\n        if price_assumption > 0:\n            economics['commodity_price'] = price_assumption\n            commodity_price = price_assumption\n            derivations.append(f\"Used commodity_price_assumption (${price_assumption}) as commodity_price\")\n    \n    if commodity_price <= 0:\n        # Try to calculate from annual_revenue / annual_production\n        annual_revenue = safe_float(economics.get('annual_revenue'), 0)\n        if annual_revenue > 0 and annual_prod > 0:\n            # Revenue is often in millions, production in oz\n            commodity_price = (annual_revenue * 1_000_000) / annual_prod\n            economics['commodity_price'] = commodity_price\n            derivations.append(f\"Calculated commodity_price (${commodity_price:.0f}) from annual_revenue / annual_production\")\n    \n    # Try to derive operating_cost / AISC\n    aisc = safe_float(economics.get('all_in_sustaining_cost'), 0)\n    op_cost = safe_float(economics.get('operating_cost'), 0)\n    \n    if aisc <= 0 and op_cost <= 0:\n        # Try to calculate from annual_opex / annual_production\n        annual_opex = safe_float(economics.get('annual_opex'), 0)\n        if annual_opex > 0 and annual_prod > 0:\n            # OPEX is often in millions, production in oz\n            op_cost = (annual_opex * 1_000_000) / annual_prod\n            economics['operating_cost'] = op_cost\n            derivations.append(f\"Calculated operating_cost (${op_cost:.0f}/oz) from annual_opex / annual_production\")\n    \n    if aisc <= 0 and op_cost > 0:\n        # Estimate AISC as operating_cost + 15% for sustaining capex\n        aisc = op_cost * 1.15\n        economics['all_in_sustaining_cost'] = aisc\n        derivations.append(f\"Estimated AISC (${aisc:.0f}/oz) as operating_cost + 15%\")\n    \n    # Update the data\n    extracted_data['economics'] = economics\n    extracted_data['production'] = production\n    \n    # Add derivation notes\n    if 'extraction_notes' not in extracted_data:\n        extracted_data['extraction_notes'] = {}\n    extracted_data['extraction_notes']['derivations'] = derivations\n    \n    return extracted_data\n\n\ndef get_missing_inputs_report(extracted_data: Dict[str, Any]) -> Dict[str, List[str]]:\n    \"\"\"\n    Generate a detailed report of what's missing for each valuation method.\n    Returns a dict with methodology names and their missing inputs.\n    \"\"\"\n    economics = extracted_data.get('economics', {}) or {}\n    production = extracted_data.get('production', {}) or {}\n    exploration = extracted_data.get('exploration', {}) or {}\n    project_info = extracted_data.get('project_info', {}) or {}\n    \n    missing = {}\n    \n    annual_prod = safe_float(production.get('annual_production'), 0)\n    commodity_price = safe_float(economics.get('commodity_price'), 0)\n    aisc = safe_float(economics.get('all_in_sustaining_cost') or economics.get('operating_cost'), 0)\n    \n    # Core valuations need: annual_production, commodity_price, operating_cost/AISC\n    core_missing = []\n    if annual_prod <= 0:\n        core_missing.append(\"Annual Production (oz/year or tonnes/year)\")\n    if commodity_price <= 0:\n        core_missing.append(\"Commodity Price Assumption ($/oz or $/tonne)\")\n    if aisc <= 0:\n        core_missing.append(\"Operating Cost or AISC ($/oz or $/tonne)\")\n    \n    if core_missing:\n        missing['Income DCF'] = core_missing.copy()\n        missing['Probability-Weighted DCF'] = core_missing.copy()\n        missing['Monte Carlo Simulation'] = core_missing.copy()\n        missing['Decision Tree EMV'] = core_missing.copy()\n    \n    # Kilburn needs exploration data\n    kilburn_missing = []\n    exp_spend = safe_float(exploration.get('historical_exploration_spend'), 0)\n    drill_meters = safe_float(exploration.get('drill_meters_completed'), 0)\n    if exp_spend <= 0 and drill_meters <= 0:\n        kilburn_missing.append(\"Historical Exploration Expenditure or Drill Meters\")\n    \n    area = safe_float(project_info.get('property_area_km2'), 0)\n    if area <= 0:\n        kilburn_missing.append(\"Property Area (kmÂ²)\")\n    \n    if kilburn_missing:\n        missing['Kilburn/Cost Approach'] = kilburn_missing\n    \n    return missing\n\n\ndef is_rate_limit_error(exception: BaseException) -> bool:\n    error_msg = str(exception)\n    return (\n        \"429\" in error_msg\n        or \"RATELIMIT_EXCEEDED\" in error_msg\n        or \"quota\" in error_msg.lower()\n        or \"rate limit\" in error_msg.lower()\n        or (hasattr(exception, \"status_code\") and exception.status_code == 429)\n    )\n\n\nADVANCED_EXTRACTION_PROMPT = \"\"\"You are a senior mining finance analyst conducting a comprehensive valuation analysis. Extract all relevant financial, technical, and project data from the following documents for advanced valuation modeling.\n\nCRITICAL INSTRUCTION: The valuation engines REQUIRE three core inputs to work:\n1. annual_production - The yearly production rate (oz/year, tonnes/year, lbs/year)\n2. commodity_price - The assumed metal price ($/oz, $/lb, $/tonne)  \n3. operating_cost OR all_in_sustaining_cost (AISC) - Cash costs or AISC ($/oz, $/lb, $/tonne)\n\nWITHOUT THESE THREE VALUES, the valuation will fail. Search thoroughly for them using these common synonyms:\n- annual_production: \"average annual production\", \"steady-state production\", \"life-of-mine average\", \"LOM annual average\", \"production rate\", \"yearly output\"\n- commodity_price: \"gold price assumption\", \"metal price\", \"base case price\", \"spot price assumed\", \"price assumption\"  \n- operating_cost/AISC: \"cash cost\", \"C1 cost\", \"operating cost\", \"AISC\", \"all-in sustaining cost\", \"total cash cost\", \"site costs\"\n\nSEARCH ALL SECTIONS INCLUDING:\n- Executive Summary and Key Metrics (often has annual production and price assumptions)\n- Section 14: Mineral Resource Estimates (tonnage, grade, contained metal by category)\n- Section 16-17: Mining and Processing Methods (throughput, recovery rates)\n- Section 21-22: Capital and Operating Cost Estimates (CAPEX, OPEX, AISC - CRITICAL)\n- Section 22: Economic Analysis (NPV, IRR, payback, cash flows, price assumptions - CRITICAL)\n- Section 25: Interpretation and Conclusions\n- All financial tables, schedules, and appendices (often has production schedules, cost summaries)\n\nDOCUMENTS:\n{documents}\n\nExtract the following data in JSON format. For the THREE CRITICAL FIELDS (annual_production, commodity_price, operating_cost/all_in_sustaining_cost), try every possible synonym and calculation method before giving up. For other fields, use 0 or null if truly not found:\n\n{{\n    \"project_info\": {{\n        \"project_name\": \"Full project name\",\n        \"primary_commodity\": \"gold/silver/copper/lithium/etc\",\n        \"secondary_commodities\": [\"list of by-products\"],\n        \"development_stage\": \"grassroots/early_exploration/advanced_exploration/pre_feasibility/feasibility/permitted/construction/production\",\n        \"location\": \"Country/Region\",\n        \"jurisdiction\": \"Tier 1 (Canada, Australia, USA)/Tier 2 (Chile, Peru, Mexico)/Tier 3 (Emerging)/Tier 4 (High Risk)\",\n        \"property_area_km2\": 0,\n        \"technical_complexity\": \"simple/moderate/complex/highly_complex\",\n        \"mining_method\": \"open_pit/underground/both\",\n        \"processing_method\": \"description\"\n    }},\n    \"resources\": {{\n        \"measured_tonnage_mt\": 0,\n        \"measured_grade\": 0,\n        \"measured_contained_metal\": 0,\n        \"indicated_tonnage_mt\": 0,\n        \"indicated_grade\": 0,\n        \"indicated_contained_metal\": 0,\n        \"inferred_tonnage_mt\": 0,\n        \"inferred_grade\": 0,\n        \"inferred_contained_metal\": 0,\n        \"total_mi_tonnage_mt\": 0,\n        \"total_mi_grade\": 0,\n        \"total_mi_contained_metal\": 0,\n        \"resource_category\": \"Measured & Indicated/Inferred/etc\",\n        \"grade_unit\": \"g/t, %, ppm, etc\",\n        \"metal_unit\": \"Moz, Mlb, kt, etc\"\n    }},\n    \"reserves\": {{\n        \"proven_tonnage_mt\": 0,\n        \"proven_grade\": 0,\n        \"probable_tonnage_mt\": 0,\n        \"probable_grade\": 0,\n        \"total_pp_tonnage_mt\": 0,\n        \"total_pp_grade\": 0,\n        \"total_pp_contained_metal\": 0,\n        \"strip_ratio\": 0\n    }},\n    \"production\": {{\n        \"annual_production\": 0,\n        \"annual_production_unit\": \"oz/year, lbs/year, tonnes/year\",\n        \"life_of_mine_production\": 0,\n        \"throughput_tpd\": 0,\n        \"recovery_rate\": 0\n    }},\n    \"economics\": {{\n        \"initial_capex\": 0,\n        \"sustaining_capex\": 0,\n        \"total_capex\": 0,\n        \"annual_opex\": 0,\n        \"operating_cost\": 0,\n        \"operating_cost_unit\": \"$/oz, $/lb, $/tonne\",\n        \"all_in_sustaining_cost\": 0,\n        \"aisc_unit\": \"$/oz, $/lb, $/tonne\",\n        \"commodity_price\": 0,\n        \"commodity_price_assumption\": 0,\n        \"npv\": 0,\n        \"npv_pretax\": 0,\n        \"irr\": 0,\n        \"irr_pretax\": 0,\n        \"payback_years\": 0,\n        \"discount_rate\": 8,\n        \"mine_life\": 0,\n        \"annual_revenue\": 0,\n        \"royalty_rate\": 0,\n        \"tax_rate\": 0,\n        \"closure_cost\": 0,\n        \"working_capital\": 0\n    }},\n    \"exploration\": {{\n        \"historical_exploration_spend\": 0,\n        \"drill_meters_completed\": 0,\n        \"number_of_drill_holes\": 0,\n        \"regional_prospectivity\": 0,\n        \"project_maturity_score\": 0,\n        \"local_geology_score\": 0,\n        \"analytical_data_quality\": 0,\n        \"exploration_upside\": \"description\"\n    }},\n    \"risk_factors\": {{\n        \"permitting_status\": \"Not started/In progress/Approved/Challenged\",\n        \"environmental_studies\": \"Baseline/EIA submitted/EIA approved\",\n        \"community_relations\": \"Early engagement/Agreements in place/Disputes\",\n        \"financing_status\": \"Seeking/Partially funded/Fully funded\",\n        \"infrastructure_status\": \"description\",\n        \"key_risks\": [\"list of key risks\"]\n    }},\n    \"data_quality\": {{\n        \"report_type\": \"NI 43-101/JORC/S-K 1300/Other\",\n        \"report_date\": \"YYYY-MM-DD\",\n        \"qualified_person\": \"Name and credentials\",\n        \"confidence_level\": \"High/Moderate/Low\"\n    }},\n    \"extraction_notes\": {{\n        \"missing_critical_data\": [\"list items that could not be found\"],\n        \"data_confidence\": \"High/Medium/Low\",\n        \"assumptions_made\": [\"list any assumptions\"]\n    }}\n}}\n\nIMPORTANT: \n- Convert all monetary values to millions USD ($ millions)\n- Convert percentages to decimals (e.g., 8% = 8, not 0.08) \n- Ensure grade units match commodity (g/t for gold, % for copper)\n- Extract both pre-tax and after-tax economics if available\n- Note any values that required calculation or estimation\"\"\"\n\n\nclass AdvancedAIAnalyzer:\n    \"\"\"Advanced AI-powered valuation analyzer using GPT-5.1\"\"\"\n    \n    @staticmethod\n    @retry(\n        stop=stop_after_attempt(7),\n        wait=wait_exponential(multiplier=1, min=2, max=128),\n        retry=retry_if_exception(is_rate_limit_error),\n        reraise=True\n    )\n    def extract_valuation_data(documents_text: List[Dict[str, str]]) -> Dict[str, Any]:\n        \"\"\"\n        Extract comprehensive valuation data from documents using GPT-5.1\n        \n        Args:\n            documents_text: List of document dictionaries with 'file_name' and 'text'\n        \n        Returns:\n            Extracted data structure for valuation engines\n        \"\"\"\n        if openai_client is None:\n            return {\n                \"error\": \"OpenAI API not configured. Please ensure AI integration is set up.\",\n                \"extraction_notes\": {\n                    \"missing_critical_data\": [\"OpenAI API credentials not available\"],\n                    \"data_confidence\": \"None\",\n                    \"assumptions_made\": []\n                }\n            }\n        \n        combined_text = \"\\n\\n\".join([\n            f\"=== Document: {doc['file_name']} ===\\n{doc['text']}\"\n            for doc in documents_text if doc.get('success', False)\n        ])\n        \n        if not combined_text.strip():\n            return {\n                \"error\": \"No valid text extracted from documents.\",\n                \"extraction_notes\": {\n                    \"missing_critical_data\": [\"All documents failed to extract\"],\n                    \"data_confidence\": \"None\",\n                    \"assumptions_made\": []\n                }\n            }\n        \n        # OpenAI message limit is 10.5MB, use 8MB to be safe and leave room for prompt\n        MAX_TEXT_LENGTH = 8000000\n        if len(combined_text) > MAX_TEXT_LENGTH:\n            combined_text = combined_text[:MAX_TEXT_LENGTH] + \"\\n\\n[... document text truncated due to length ...]\"\n        \n        base_prompt = ADVANCED_EXTRACTION_PROMPT.format(documents=combined_text)\n        \n        training_context = \"\"\n        try:\n            from training_rag import build_enhanced_context, get_training_statistics\n            stats = get_training_statistics()\n            if stats.get('total_chunks', 0) > 0:\n                training_context = build_enhanced_context(\n                    document_text=combined_text[:10000],\n                    category=None,\n                    commodity=None\n                )\n        except Exception:\n            pass\n        \n        if training_context:\n            prompt = f\"{training_context}\\n\\n{base_prompt}\"\n        else:\n            prompt = base_prompt\n        \n        try:\n            response = openai_client.chat.completions.create(\n                model=\"gpt-5.1\",\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are an expert mining financial analyst. Extract structured data for valuation modeling. Return valid JSON only.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": prompt\n                    }\n                ],\n                response_format={\"type\": \"json_object\"},\n                max_completion_tokens=16384,\n                reasoning_effort=\"high\"\n            )\n            \n            response_text = response.choices[0].message.content or \"{}\"\n            try:\n                extracted_data = json.loads(response_text)\n            except json.JSONDecodeError as je:\n                # If JSON parsing fails, try to extract valid JSON portion\n                try:\n                    # Look for the last complete JSON object\n                    last_brace = response_text.rfind('}')\n                    if last_brace > 0:\n                        partial_json = response_text[:last_brace+1]\n                        extracted_data = json.loads(partial_json)\n                    else:\n                        raise ValueError(\"No valid JSON object found in response\")\n                except Exception as fallback_e:\n                    return {\n                        \"error\": f\"AI extraction failed: JSON parsing error - {str(je)}. Response may be truncated or malformed.\",\n                        \"extraction_notes\": {\n                            \"missing_critical_data\": [\"Extraction error: Invalid JSON response\"],\n                            \"data_confidence\": \"None\",\n                            \"assumptions_made\": []\n                        }\n                    }\n            \n            return extracted_data\n            \n        except Exception as e:\n            return {\n                \"error\": f\"AI extraction failed: {str(e)}\",\n                \"extraction_notes\": {\n                    \"missing_critical_data\": [\"Extraction error occurred\"],\n                    \"data_confidence\": \"None\",\n                    \"assumptions_made\": []\n                }\n            }\n    \n    @staticmethod\n    def run_all_valuations(extracted_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Run all 5 valuation methodologies on extracted data\n        \n        IMPORTANT: Income DCF runs first and its result is passed to dependent engines\n        (Probability DCF and Decision Tree EMV) to ensure consistent calculated NPV values\n        instead of using potentially unreliable document-reported NPVs.\n        \n        Args:\n            extracted_data: Data extracted from documents\n        \n        Returns:\n            Complete valuation results from all methods\n        \"\"\"\n        if \"error\" in extracted_data:\n            return {\"error\": extracted_data[\"error\"]}\n        \n        # NORMALIZE DATA: Try to derive missing values from related fields\n        extracted_data = normalize_extracted_data(extracted_data)\n        \n        # Generate missing inputs report for user feedback\n        missing_inputs_report = get_missing_inputs_report(extracted_data)\n        \n        valuations = {}\n        errors = []\n        \n        # STEP 1: Run Income DCF FIRST - this provides the base NPV for other engines\n        try:\n            valuations['income_dcf'] = generate_dcf_from_extraction(extracted_data)\n        except Exception as e:\n            errors.append(f\"Income DCF: {str(e)}\")\n            valuations['income_dcf'] = {\"error\": str(e)}\n        \n        # Get the Income DCF result to pass to dependent engines\n        income_dcf_result = valuations.get('income_dcf')\n        \n        # STEP 2: Run Probability DCF using Income DCF result\n        try:\n            valuations['probability_dcf'] = generate_probability_dcf_from_extraction(\n                extracted_data, \n                income_dcf_result=income_dcf_result\n            )\n        except Exception as e:\n            errors.append(f\"Probability DCF: {str(e)}\")\n            valuations['probability_dcf'] = {\"error\": str(e)}\n        \n        # STEP 3: Run Monte Carlo with STRICT validation - ALL THREE inputs required\n        try:\n            economics = extracted_data.get('economics', {}) or {}\n            project_info = extracted_data.get('project_info', {}) or {}\n            production = extracted_data.get('production', {}) or {}\n            \n            commodity = (project_info.get('primary_commodity') or 'gold').lower()\n            annual_prod_raw = safe_float(production.get('annual_production'), 0)\n            commodity_price_raw = safe_float(economics.get('commodity_price'), 0)\n            aisc_raw = safe_float(economics.get('all_in_sustaining_cost') or economics.get('operating_cost'), 0)\n            \n            # STRICT validation - require ALL THREE inputs, no fabrication\n            missing_inputs = []\n            if annual_prod_raw <= 0:\n                missing_inputs.append('annual_production')\n            if commodity_price_raw <= 0:\n                missing_inputs.append('commodity_price')\n            if aisc_raw <= 0:\n                missing_inputs.append('operating_cost/AISC')\n            \n            if len(missing_inputs) > 0:\n                valuations['monte_carlo'] = {\n                    \"error\": \"insufficient_data\",\n                    \"message\": f\"Cannot run Monte Carlo: missing {', '.join(missing_inputs)}\",\n                    \"missing_inputs\": missing_inputs\n                }\n            else:\n                # All inputs validated - proceed with actual values\n                capex = safe_float(economics.get('initial_capex'), 0)\n                mine_life_raw = safe_int(economics.get('mine_life'), 15)\n                mine_life = mine_life_raw if mine_life_raw > 0 else 15\n                raw_discount = safe_float(economics.get('discount_rate'), 8)\n                discount = raw_discount / 100 if raw_discount > 1 else raw_discount if raw_discount > 0 else 0.08\n                \n                valuations['monte_carlo'] = run_full_monte_carlo_analysis(\n                    commodity=commodity,\n                    annual_production=annual_prod_raw,\n                    unit_cost=aisc_raw,\n                    initial_capex=capex * 1_000_000,\n                    spot_price=commodity_price_raw,\n                    years=mine_life,\n                    discount_rate=discount,\n                    num_simulations=10000\n                )\n        except Exception as e:\n            errors.append(f\"Monte Carlo: {str(e)}\")\n            valuations['monte_carlo'] = {\"error\": str(e)}\n        \n        # STEP 4: Run Kilburn Method\n        try:\n            valuations['kilburn'] = generate_kilburn_from_extraction(extracted_data)\n        except Exception as e:\n            errors.append(f\"Kilburn Method: {str(e)}\")\n            valuations['kilburn'] = {\"error\": str(e)}\n        \n        # STEP 5: Run Decision Tree EMV using Income DCF result\n        try:\n            valuations['decision_tree'] = generate_emv_from_extraction(\n                extracted_data,\n                income_dcf_result=income_dcf_result\n            )\n        except Exception as e:\n            errors.append(f\"Decision Tree EMV: {str(e)}\")\n            valuations['decision_tree'] = {\"error\": str(e)}\n        \n        valuation_summary = AdvancedAIAnalyzer._create_valuation_summary(valuations, extracted_data)\n        \n        return {\n            'extracted_data': extracted_data,\n            'valuations': valuations,\n            'summary': valuation_summary,\n            'errors': errors if errors else None,\n            'missing_inputs_report': missing_inputs_report if missing_inputs_report else None,\n            'derivations': extracted_data.get('extraction_notes', {}).get('derivations', [])\n        }\n    \n    @staticmethod\n    def _create_valuation_summary(valuations: Dict[str, Any], extracted_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Create a summary of all valuation results\"\"\"\n        project_info = extracted_data.get('project_info', {})\n        economics = extracted_data.get('economics', {})\n        \n        values = []\n        \n        if 'probability_dcf' in valuations and 'error' not in valuations['probability_dcf']:\n            risk_adj_npv = valuations['probability_dcf'].get('risk_adjusted_valuation', {}).get('risk_adjusted_npv', 0)\n            if risk_adj_npv > 0:\n                values.append(('Probability-Weighted DCF', risk_adj_npv))\n        \n        if 'income_dcf' in valuations and 'error' not in valuations['income_dcf']:\n            dcf_npv = valuations['income_dcf'].get('valuation_summary', {}).get('npv', 0)\n            if dcf_npv > 0:\n                values.append(('Income DCF', dcf_npv))\n        \n        if 'monte_carlo' in valuations and 'error' not in valuations['monte_carlo']:\n            mc_p50 = valuations['monte_carlo'].get('npv_statistics', {}).get('p50', 0)\n            if isinstance(mc_p50, (int, float)) and mc_p50 > 0:\n                mc_p50_millions = mc_p50 / 1_000_000\n                values.append(('Monte Carlo P50', mc_p50_millions))\n        \n        if 'kilburn' in valuations and 'error' not in valuations['kilburn']:\n            kilburn_val = valuations['kilburn'].get('valuation_summary', {}).get('recommended_value', 0)\n            if kilburn_val > 0:\n                values.append(('Kilburn Method', kilburn_val / 1_000_000))\n        \n        if 'decision_tree' in valuations and 'error' not in valuations['decision_tree']:\n            emv = valuations['decision_tree'].get('valuation_summary', {}).get('emv', 0)\n            if emv > 0:\n                values.append(('Decision Tree EMV', emv))\n        \n        if values:\n            avg_value = sum(v[1] for v in values) / len(values)\n            min_value = min(v[1] for v in values)\n            max_value = max(v[1] for v in values)\n            median_value = sorted([v[1] for v in values])[len(values) // 2]\n        else:\n            avg_value = min_value = max_value = median_value = 0\n        \n        recommendations = []\n        rec_colors = []\n        \n        for method, val_data in valuations.items():\n            if 'error' not in val_data:\n                rec = val_data.get('recommendation', {})\n                if rec.get('text'):\n                    recommendations.append(f\"{method}: {rec['text']}\")\n                    rec_colors.append(rec.get('color', 'gray'))\n        \n        overall_recommendation = \"Insufficient data for recommendation\"\n        overall_color = \"gray\"\n        \n        if rec_colors:\n            green_count = rec_colors.count('green')\n            blue_count = rec_colors.count('blue')\n            orange_count = rec_colors.count('orange')\n            red_count = rec_colors.count('red')\n            \n            if green_count >= 2:\n                overall_recommendation = \"Strong Investment Opportunity - Multiple methods indicate high value\"\n                overall_color = \"green\"\n            elif green_count + blue_count >= 3:\n                overall_recommendation = \"Positive Investment Case - Good risk-adjusted returns expected\"\n                overall_color = \"blue\"\n            elif red_count >= 2:\n                overall_recommendation = \"High Risk / Low Value - Proceed with caution\"\n                overall_color = \"red\"\n            else:\n                overall_recommendation = \"Mixed Signals - Detailed analysis required before investment\"\n                overall_color = \"orange\"\n        \n        return {\n            'project_name': project_info.get('project_name', 'Unknown Project'),\n            'commodity': project_info.get('primary_commodity', 'Unknown'),\n            'stage': project_info.get('development_stage', 'Unknown'),\n            'valuation_range': {\n                'low': round(min_value, 2),\n                'mid': round(median_value, 2),\n                'high': round(max_value, 2),\n                'average': round(avg_value, 2)\n            },\n            'valuation_breakdown': {name: round(val, 2) for name, val in values},\n            'methods_completed': len(values),\n            'methods_failed': len([v for v in valuations.values() if 'error' in v]),\n            'overall_recommendation': {\n                'text': overall_recommendation,\n                'color': overall_color\n            },\n            'method_recommendations': recommendations,\n            'base_case_npv': economics.get('npv', 0),\n            'base_case_irr': economics.get('irr', 0)\n        }\n    \n    @staticmethod\n    @retry(\n        stop=stop_after_attempt(5),\n        wait=wait_exponential(multiplier=1, min=2, max=60),\n        retry=retry_if_exception(is_rate_limit_error),\n        reraise=True\n    )\n    def generate_valuation_narrative(valuation_results: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Generate AI-powered narrative for valuation results using Claude Opus 4.5\n        \n        Claude Opus 4.5 is used here for superior reasoning and narrative generation,\n        while GPT-5.1 handles data extraction (hybrid AI approach).\n        \n        Args:\n            valuation_results: Complete valuation results from run_all_valuations\n        \n        Returns:\n            Narrative summary for report\n        \"\"\"\n        summary = valuation_results.get('summary', {})\n        extracted = valuation_results.get('extracted_data', {})\n        valuations = valuation_results.get('valuations', {})\n        \n        prompt = f\"\"\"Based on the following advanced valuation analysis, write a professional executive summary suitable for an investment committee presentation.\n\nPROJECT: {summary.get('project_name', 'Mining Project')}\nCOMMODITY: {summary.get('commodity', 'Unknown')}\nSTAGE: {summary.get('stage', 'Unknown')}\n\nVALUATION RANGE:\n- Low estimate: ${summary.get('valuation_range', {}).get('low', 0):.1f}M\n- Mid estimate: ${summary.get('valuation_range', {}).get('mid', 0):.1f}M  \n- High estimate: ${summary.get('valuation_range', {}).get('high', 0):.1f}M\n\nBASE CASE ECONOMICS:\n- NPV: ${extracted.get('economics', {}).get('npv', 0):.1f}M\n- IRR: {extracted.get('economics', {}).get('irr', 0):.1f}%\n- Mine Life: {extracted.get('economics', {}).get('mine_life', 0)} years\n\nVALUATION METHODS APPLIED:\n1. Probability-Weighted DCF: Risk-adjusted for stage-gate probabilities\n2. Income Approach DCF: Base case cash flow analysis\n3. Monte Carlo Simulation: 10,000 scenarios with price uncertainty\n4. Kilburn Method: Exploration asset floor value\n5. Decision Tree EMV: Stage-gate expected monetary value\n\nOVERALL RECOMMENDATION: {summary.get('overall_recommendation', {}).get('text', 'N/A')}\n\nWrite a 3-4 paragraph executive summary covering:\n1. Project overview and current status\n2. Key value drivers and risks\n3. Valuation methodology summary and conclusions\n4. Investment recommendation with key conditions\n\nKeep the tone professional and suitable for institutional investors. Return as JSON with fields:\n{{\"executive_summary\": \"...\", \"key_value_drivers\": [...], \"key_risks\": [...], \"investment_thesis\": \"...\"}}\"\"\"\n\n        fallback_response = {\n            \"executive_summary\": f\"Advanced valuation analysis completed for {summary.get('project_name', 'the mining project')}. The analysis applied five professional methodologies to assess project value. The valuation range spans from ${summary.get('valuation_range', {}).get('low', 0):.1f}M to ${summary.get('valuation_range', {}).get('high', 0):.1f}M based on the available data.\",\n            \"key_value_drivers\": [\"Resource size and grade\", \"Cost structure competitiveness\", \"Jurisdiction quality\", \"Development stage advancement\"],\n            \"key_risks\": [\"Commodity price volatility\", \"Execution risk\", \"Permitting timeline\", \"Capital availability\"],\n            \"investment_thesis\": summary.get('overall_recommendation', {}).get('text', 'See detailed valuation analysis for investment recommendation'),\n        }\n\n        if anthropic_client is not None:\n            try:\n                message = anthropic_client.messages.create(\n                    model=\"claude-opus-4-5\",\n                    max_tokens=4096,\n                    system=\"You are a senior mining investment analyst writing for institutional investors. Be concise, professional, and data-driven. Always respond with valid JSON only.\",\n                    messages=[\n                        {\n                            \"role\": \"user\",\n                            \"content\": prompt\n                        }\n                    ]\n                )\n                \n                response_text = \"{}\"\n                if message.content:\n                    first_block = message.content[0]\n                    if hasattr(first_block, 'text'):\n                        response_text = first_block.text\n                \n                try:\n                    narrative = json.loads(response_text)\n                    narrative[\"ai_model\"] = \"Claude Opus 4.5\"\n                    return narrative\n                except json.JSONDecodeError:\n                    import re\n                    json_match = re.search(r'\\{[^{}]*\\}', response_text, re.DOTALL)\n                    if json_match:\n                        narrative = json.loads(json_match.group())\n                        narrative[\"ai_model\"] = \"Claude Opus 4.5\"\n                        return narrative\n                    fallback_response[\"note\"] = \"Claude response parsing failed, using fallback\"\n                    return fallback_response\n                    \n            except Exception as claude_error:\n                fallback_response[\"claude_error\"] = str(claude_error)\n        \n        claude_failed = \"claude_error\" in fallback_response\n        \n        if openai_client is not None:\n            try:\n                response = openai_client.chat.completions.create(\n                    model=\"gpt-5.1\",\n                    messages=[\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"You are a senior mining investment analyst writing for institutional investors. Be concise, professional, and data-driven.\"\n                        },\n                        {\n                            \"role\": \"user\",\n                            \"content\": prompt\n                        }\n                    ],\n                    response_format={\"type\": \"json_object\"},\n                    max_completion_tokens=2048,\n                    reasoning_effort=\"high\"\n                )\n                \n                narrative = json.loads(response.choices[0].message.content or \"{}\")\n                narrative[\"ai_model\"] = \"GPT-5.1 (fallback)\" if claude_failed else \"GPT-5.1\"\n                if claude_failed:\n                    narrative[\"fallback_reason\"] = \"Claude Opus 4.5 unavailable, used GPT-5.1\"\n                return narrative\n                \n            except Exception as e:\n                fallback_response[\"error\"] = str(e)\n                return fallback_response\n        \n        fallback_response[\"note\"] = \"AI narrative unavailable - no AI integration configured\"\n        return fallback_response\n    \n    @staticmethod\n    def run_complete_analysis(documents_text: List[Dict[str, str]]) -> Dict[str, Any]:\n        \"\"\"\n        Run complete advanced valuation analysis pipeline\n        \n        Args:\n            documents_text: List of document dictionaries\n        \n        Returns:\n            Complete analysis with extraction, valuations, and narrative\n        \"\"\"\n        extracted_data = AdvancedAIAnalyzer.extract_valuation_data(documents_text)\n        \n        if \"error\" in extracted_data and not any(k in extracted_data for k in ['project_info', 'economics']):\n            return {\"error\": extracted_data[\"error\"]}\n        \n        valuation_results = AdvancedAIAnalyzer.run_all_valuations(extracted_data)\n        \n        narrative = AdvancedAIAnalyzer.generate_valuation_narrative(valuation_results)\n        valuation_results['narrative'] = narrative\n        \n        return valuation_results\n","path":null,"size_bytes":37747,"size_tokens":null},"decision_tree_emv_engine.py":{"content":"\"\"\"\nDecision Tree / Stage-Gate Analysis (EMV) Engine\nA sequential decision model that maps out key project stages with costs, \nsuccess probabilities, and outcomes to calculate Expected Monetary Value.\n\"\"\"\n\nimport numpy as np\nfrom typing import Dict, Any, List, Optional\n\nSTAGE_GATE_DEFINITIONS = {\n    'grassroots': {\n        'stages': [\n            {'name': 'Initial Exploration', 'cost': 2.0, 'duration': 2, 'success_prob': 0.15, 'next_stage': 'Target Generation'},\n            {'name': 'Target Generation', 'cost': 5.0, 'duration': 2, 'success_prob': 0.25, 'next_stage': 'Drilling Program'},\n            {'name': 'Drilling Program', 'cost': 15.0, 'duration': 2, 'success_prob': 0.30, 'next_stage': 'Resource Definition'},\n            {'name': 'Resource Definition', 'cost': 25.0, 'duration': 2, 'success_prob': 0.50, 'next_stage': 'Preliminary Economic Assessment'},\n            {'name': 'Preliminary Economic Assessment', 'cost': 3.0, 'duration': 1, 'success_prob': 0.60, 'next_stage': 'Pre-Feasibility Study'},\n            {'name': 'Pre-Feasibility Study', 'cost': 10.0, 'duration': 1, 'success_prob': 0.70, 'next_stage': 'Feasibility Study'},\n            {'name': 'Feasibility Study', 'cost': 25.0, 'duration': 1.5, 'success_prob': 0.80, 'next_stage': 'Permitting'},\n            {'name': 'Permitting', 'cost': 15.0, 'duration': 2, 'success_prob': 0.75, 'next_stage': 'Financing'},\n            {'name': 'Financing', 'cost': 5.0, 'duration': 0.5, 'success_prob': 0.70, 'next_stage': 'Construction'},\n            {'name': 'Construction', 'cost': 250.0, 'duration': 2, 'success_prob': 0.90, 'next_stage': 'Production'}\n        ],\n        'terminal_value_multiple': 3.0\n    },\n    'early_exploration': {\n        'stages': [\n            {'name': 'Target Generation', 'cost': 5.0, 'duration': 1.5, 'success_prob': 0.30, 'next_stage': 'Drilling Program'},\n            {'name': 'Drilling Program', 'cost': 15.0, 'duration': 2, 'success_prob': 0.35, 'next_stage': 'Resource Definition'},\n            {'name': 'Resource Definition', 'cost': 20.0, 'duration': 1.5, 'success_prob': 0.55, 'next_stage': 'Preliminary Economic Assessment'},\n            {'name': 'Preliminary Economic Assessment', 'cost': 3.0, 'duration': 1, 'success_prob': 0.65, 'next_stage': 'Pre-Feasibility Study'},\n            {'name': 'Pre-Feasibility Study', 'cost': 10.0, 'duration': 1, 'success_prob': 0.72, 'next_stage': 'Feasibility Study'},\n            {'name': 'Feasibility Study', 'cost': 25.0, 'duration': 1.5, 'success_prob': 0.82, 'next_stage': 'Permitting'},\n            {'name': 'Permitting', 'cost': 15.0, 'duration': 2, 'success_prob': 0.78, 'next_stage': 'Financing'},\n            {'name': 'Financing', 'cost': 5.0, 'duration': 0.5, 'success_prob': 0.72, 'next_stage': 'Construction'},\n            {'name': 'Construction', 'cost': 250.0, 'duration': 2, 'success_prob': 0.92, 'next_stage': 'Production'}\n        ],\n        'terminal_value_multiple': 2.5\n    },\n    'advanced_exploration': {\n        'stages': [\n            {'name': 'Infill Drilling', 'cost': 20.0, 'duration': 1.5, 'success_prob': 0.60, 'next_stage': 'Resource Update'},\n            {'name': 'Resource Update', 'cost': 5.0, 'duration': 0.5, 'success_prob': 0.75, 'next_stage': 'Preliminary Economic Assessment'},\n            {'name': 'Preliminary Economic Assessment', 'cost': 3.0, 'duration': 0.75, 'success_prob': 0.70, 'next_stage': 'Pre-Feasibility Study'},\n            {'name': 'Pre-Feasibility Study', 'cost': 10.0, 'duration': 1, 'success_prob': 0.75, 'next_stage': 'Feasibility Study'},\n            {'name': 'Feasibility Study', 'cost': 25.0, 'duration': 1.5, 'success_prob': 0.85, 'next_stage': 'Permitting'},\n            {'name': 'Permitting', 'cost': 15.0, 'duration': 2, 'success_prob': 0.80, 'next_stage': 'Financing'},\n            {'name': 'Financing', 'cost': 5.0, 'duration': 0.5, 'success_prob': 0.75, 'next_stage': 'Construction'},\n            {'name': 'Construction', 'cost': 250.0, 'duration': 2, 'success_prob': 0.93, 'next_stage': 'Production'}\n        ],\n        'terminal_value_multiple': 2.0\n    },\n    'pre_feasibility': {\n        'stages': [\n            {'name': 'Complete Pre-Feasibility', 'cost': 8.0, 'duration': 0.75, 'success_prob': 0.80, 'next_stage': 'Feasibility Study'},\n            {'name': 'Feasibility Study', 'cost': 25.0, 'duration': 1.5, 'success_prob': 0.88, 'next_stage': 'Permitting'},\n            {'name': 'Permitting', 'cost': 15.0, 'duration': 2, 'success_prob': 0.82, 'next_stage': 'Financing'},\n            {'name': 'Financing', 'cost': 5.0, 'duration': 0.5, 'success_prob': 0.78, 'next_stage': 'Construction'},\n            {'name': 'Construction', 'cost': 250.0, 'duration': 2, 'success_prob': 0.94, 'next_stage': 'Production'}\n        ],\n        'terminal_value_multiple': 1.8\n    },\n    'feasibility': {\n        'stages': [\n            {'name': 'Complete Feasibility', 'cost': 15.0, 'duration': 1, 'success_prob': 0.90, 'next_stage': 'Permitting'},\n            {'name': 'Permitting', 'cost': 15.0, 'duration': 2, 'success_prob': 0.85, 'next_stage': 'Financing'},\n            {'name': 'Financing', 'cost': 5.0, 'duration': 0.5, 'success_prob': 0.80, 'next_stage': 'Construction'},\n            {'name': 'Construction', 'cost': 250.0, 'duration': 2, 'success_prob': 0.95, 'next_stage': 'Production'}\n        ],\n        'terminal_value_multiple': 1.5\n    },\n    'permitted': {\n        'stages': [\n            {'name': 'Financing', 'cost': 5.0, 'duration': 0.5, 'success_prob': 0.82, 'next_stage': 'Construction'},\n            {'name': 'Construction', 'cost': 250.0, 'duration': 2, 'success_prob': 0.96, 'next_stage': 'Production'}\n        ],\n        'terminal_value_multiple': 1.3\n    },\n    'construction': {\n        'stages': [\n            {'name': 'Complete Construction', 'cost': 150.0, 'duration': 1.5, 'success_prob': 0.97, 'next_stage': 'Production'}\n        ],\n        'terminal_value_multiple': 1.15\n    },\n    'production': {\n        'stages': [],\n        'terminal_value_multiple': 1.0\n    }\n}\n\n\ndef calculate_emv_valuation(\n    current_stage: str,\n    terminal_value: float,\n    discount_rate: float = 0.10,\n    custom_stages: List[Dict] = None,\n    scale_factor: float = 1.0\n) -> Dict[str, Any]:\n    \"\"\"\n    Calculate Expected Monetary Value using decision tree analysis\n    \n    Args:\n        current_stage: Current project development stage\n        terminal_value: Expected project value at production ($ millions)\n        discount_rate: Discount rate for time value (decimal)\n        custom_stages: Optional custom stage definitions\n        scale_factor: Scale factor for costs (larger projects may need higher costs)\n    \n    Returns:\n        Comprehensive EMV analysis with decision tree breakdown\n    \"\"\"\n    stage_key = current_stage.lower().replace(' ', '_').replace('-', '_')\n    if stage_key not in STAGE_GATE_DEFINITIONS:\n        stage_key = 'early_exploration'\n    \n    stage_config = STAGE_GATE_DEFINITIONS[stage_key]\n    stages = custom_stages if custom_stages else stage_config['stages']\n    terminal_multiple = stage_config['terminal_value_multiple']\n    \n    total_stages = len(stages)\n    if total_stages == 0:\n        return {\n            'emv': terminal_value,\n            'probability_to_production': 1.0,\n            'recommendation': {\n                'text': 'Already in Production - Value equals operating project NPV',\n                'color': 'green'\n            }\n        }\n    \n    stage_analysis = []\n    cumulative_cost = 0\n    cumulative_time = 0\n    cumulative_probability = 1.0\n    \n    for i, stage in enumerate(stages):\n        stage_cost = stage['cost'] * scale_factor\n        stage_prob = stage['success_prob']\n        stage_duration = stage['duration']\n        \n        cumulative_cost += stage_cost\n        cumulative_time += stage_duration\n        cumulative_probability *= stage_prob\n        \n        time_discount = (1 + discount_rate) ** cumulative_time\n        \n        success_value_this_stage = 0\n        if i == total_stages - 1:\n            success_value_this_stage = terminal_value / time_discount\n        else:\n            remaining_prob = 1.0\n            for j in range(i + 1, total_stages):\n                remaining_prob *= stages[j]['success_prob']\n            remaining_time = sum(s['duration'] for s in stages[i+1:])\n            total_discount = (1 + discount_rate) ** (cumulative_time + remaining_time)\n            success_value_this_stage = (terminal_value * remaining_prob) / total_discount\n        \n        stage_emv = (success_value_this_stage * stage_prob) - stage_cost\n        \n        should_proceed = stage_emv > 0\n        \n        stage_analysis.append({\n            'stage_name': stage['name'],\n            'stage_number': i + 1,\n            'cost': round(stage_cost, 2),\n            'duration_years': stage_duration,\n            'success_probability': round(stage_prob * 100, 1),\n            'cumulative_probability': round(cumulative_probability * 100, 2),\n            'cumulative_cost': round(cumulative_cost, 2),\n            'cumulative_time': round(cumulative_time, 1),\n            'expected_value_if_success': round(success_value_this_stage, 2),\n            'stage_emv': round(stage_emv, 2),\n            'decision': 'PROCEED' if should_proceed else 'STOP/RECONSIDER',\n            'next_stage': stage.get('next_stage', 'Complete')\n        })\n    \n    overall_emv = 0\n    running_prob = 1.0\n    running_time = 0\n    \n    for stage in stages:\n        running_time += stage['duration']\n        time_discount = (1 + discount_rate) ** running_time\n        \n        failure_prob = 1 - stage['success_prob']\n        failure_cost = (stage['cost'] * scale_factor) * running_prob\n        failure_loss = -failure_cost / time_discount\n        \n        overall_emv += failure_loss * failure_prob\n        running_prob *= stage['success_prob']\n    \n    success_pv = (terminal_value * running_prob) / ((1 + discount_rate) ** running_time)\n    overall_emv += success_pv\n    \n    total_investment_if_proceed = sum(s['cost'] * scale_factor for s in stages)\n    \n    if overall_emv > total_investment_if_proceed * 0.5:\n        recommendation = \"High Value Opportunity - EMV significantly exceeds required investment\"\n        color = \"green\"\n    elif overall_emv > total_investment_if_proceed * 0.2:\n        recommendation = \"Positive EMV - Expected value exceeds risk-adjusted costs\"\n        color = \"blue\"\n    elif overall_emv > 0:\n        recommendation = \"Marginal Opportunity - Positive but low EMV relative to investment\"\n        color = \"orange\"\n    else:\n        recommendation = \"Negative EMV - Expected losses exceed potential gains\"\n        color = \"red\"\n    \n    optimal_exit_stages = []\n    for i, stage in enumerate(stage_analysis):\n        if stage['decision'] == 'STOP/RECONSIDER':\n            optimal_exit_stages.append(stage['stage_name'])\n    \n    return {\n        'valuation_summary': {\n            'emv': round(overall_emv, 2),\n            'terminal_value': round(terminal_value, 2),\n            'probability_to_production': round(cumulative_probability * 100, 2),\n            'total_investment_required': round(total_investment_if_proceed, 2),\n            'total_time_to_production': round(cumulative_time, 1),\n            'risk_adjusted_multiple': round(overall_emv / total_investment_if_proceed, 2) if total_investment_if_proceed > 0 else 0\n        },\n        'stage_gate_analysis': stage_analysis,\n        'decision_analysis': {\n            'optimal_path': 'Proceed through all stages' if not optimal_exit_stages else f'Consider exit at: {\", \".join(optimal_exit_stages)}',\n            'value_creation_milestones': [\n                f\"Stage {s['stage_number']}: {s['stage_name']} adds ${s['stage_emv']:.1f}M EMV\"\n                for s in stage_analysis if s['stage_emv'] > 0\n            ],\n            'highest_risk_stages': [\n                f\"{s['stage_name']} ({s['success_probability']:.0f}% success)\"\n                for s in stage_analysis if s['success_probability'] < 70\n            ]\n        },\n        'real_options_value': {\n            'option_to_abandon': round(sum(s['cost'] for s in stage_analysis if s['stage_emv'] < 0) * 0.5, 2),\n            'option_to_expand': round(overall_emv * 0.15, 2),\n            'option_to_defer': round(overall_emv * 0.10, 2),\n            'total_options_value': round(overall_emv * 0.25, 2)\n        },\n        'recommendation': {\n            'text': recommendation,\n            'color': color\n        },\n        'methodology_notes': [\n            f\"Decision tree with {total_stages} stages from {current_stage} to production\",\n            f\"Terminal value of ${terminal_value:.1f}M discounted at {discount_rate*100:.0f}% over {cumulative_time:.1f} years\",\n            f\"Cumulative probability of reaching production: {cumulative_probability*100:.2f}%\",\n            \"Real options value reflects flexibility to abandon, expand, or defer\"\n        ]\n    }\n\n\ndef safe_float(value, default: float = 0.0) -> float:\n    \"\"\"Safely convert value to float with default\"\"\"\n    if value is None:\n        return default\n    try:\n        return float(value)\n    except (TypeError, ValueError):\n        return default\n\n\ndef safe_int(value, default: int = 0) -> int:\n    \"\"\"Safely convert value to int with default\"\"\"\n    if value is None:\n        return default\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return default\n\n\ndef generate_emv_from_extraction(extracted_data: Dict[str, Any], income_dcf_result: Dict[str, Any] = None) -> Dict[str, Any]:\n    \"\"\"\n    Generate Decision Tree EMV analysis from AI-extracted document data\n    \n    IMPORTANT: Uses calculated NPV from Income DCF engine, NOT document-reported NPV\n    \n    Args:\n        extracted_data: Data extracted from documents by GPT-5.1\n        income_dcf_result: Pre-calculated Income DCF result (used for terminal value)\n    \n    Returns:\n        Complete EMV valuation\n    \"\"\"\n    if not extracted_data:\n        return {\n            'error': 'insufficient_data',\n            'message': 'No data available for EMV calculation'\n        }\n    \n    economics = extracted_data.get('economics', {}) or {}\n    project_info = extracted_data.get('project_info', {}) or {}\n    production = extracted_data.get('production', {}) or {}\n    \n    current_stage = project_info.get('development_stage') or 'early_exploration'\n    \n    # CRITICAL: Use calculated NPV from Income DCF as terminal value, NOT document-reported NPV\n    \n    # If Income DCF returned an error, propagate it - don't fabricate values\n    if income_dcf_result and 'error' in income_dcf_result:\n        return {\n            'error': 'insufficient_data',\n            'message': f\"Cannot calculate EMV: Income DCF failed - {income_dcf_result.get('message', 'insufficient data')}\",\n            'upstream_error': income_dcf_result.get('error'),\n            'missing_inputs': income_dcf_result.get('missing_inputs', [])\n        }\n    \n    terminal_value = 0\n    if income_dcf_result and 'valuation_summary' in income_dcf_result:\n        terminal_value = income_dcf_result['valuation_summary'].get('npv', 0)\n    \n    # If no Income DCF result, calculate terminal value with STRICT validation\n    if terminal_value <= 0:\n        annual_prod = safe_float(production.get('annual_production') or economics.get('annual_production'), 0)\n        commodity_price = safe_float(economics.get('commodity_price'), 0)\n        aisc = safe_float(economics.get('aisc') or economics.get('all_in_sustaining_cost') or economics.get('operating_cost'), 0)\n        \n        # STRICT: Require ALL THREE inputs - no fabrication\n        missing_inputs = []\n        if annual_prod <= 0:\n            missing_inputs.append('annual_production')\n        if commodity_price <= 0:\n            missing_inputs.append('commodity_price')\n        if aisc <= 0:\n            missing_inputs.append('operating_cost')\n        \n        if len(missing_inputs) > 0:\n            return {\n                'error': 'insufficient_data',\n                'message': f'Cannot calculate EMV: missing {\", \".join(missing_inputs)}',\n                'missing_inputs': missing_inputs\n            }\n        \n        # Only calculate if we have all inputs\n        mine_life = safe_int(economics.get('mine_life'), 15) or 15\n        capex = safe_float(economics.get('initial_capex'), 0)\n        annual_margin = annual_prod * (commodity_price - aisc)\n        annual_margin_millions = annual_margin / 1_000_000\n        terminal_value = max(0, annual_margin_millions * mine_life * 0.6 - capex)\n    \n    if terminal_value <= 0:\n        return {\n            'error': 'insufficient_data',\n            'message': 'Cannot calculate EMV: calculated terminal value is zero or negative',\n            'terminal_value_calculated': terminal_value\n        }\n    \n    raw_discount = safe_float(economics.get('discount_rate'), 10)\n    discount_rate = raw_discount / 100 if raw_discount > 1 else raw_discount if raw_discount > 0 else 0.10\n    \n    capex = safe_float(economics.get('initial_capex'), 200)\n    if capex > 500:\n        scale_factor = capex / 250\n    elif capex < 100:\n        scale_factor = 0.5\n    else:\n        scale_factor = 1.0\n    \n    return calculate_emv_valuation(\n        current_stage=current_stage,\n        terminal_value=terminal_value,\n        discount_rate=discount_rate,\n        scale_factor=scale_factor\n    )\n","path":null,"size_bytes":17319,"size_tokens":null},"page_modules/ai_training_page.py":{"content":"\"\"\"\nAI Training Page - Enhanced RAG System\nSimplified interface for uploading training documents that automatically improve AI accuracy.\n\"\"\"\n\nimport streamlit as st\nimport os\nfrom datetime import datetime\n\nfrom training_rag import (\n    process_training_document,\n    get_training_statistics,\n    get_training_documents,\n    delete_training_document,\n    delete_all_training\n)\n\n# Hard safety limits\nMAX_FILES_PER_UPLOAD = 80\nBATCH_SIZE = 10\n\nTRAINING_CATEGORIES = [\n    {\"name\": \"30_geoscience_data\", \"display_name\": \"30-Geoscience Data\", \"description\": \"Geoscience data and reports\"},\n    {\"name\": \"40_spatial_data\", \"display_name\": \"40-Spatial Data\", \"description\": \"Spatial and mapping data\"},\n    {\"name\": \"50_3d_analysis\", \"display_name\": \"50-3D and Analysis\", \"description\": \"3D modeling and analysis reports\"},\n    {\"name\": \"60_reporting\", \"display_name\": \"60-Reporting\", \"description\": \"Reporting and summary documents\"},\n    {\"name\": \"70_literature\", \"display_name\": \"70-Literature\", \"description\": \"Literature and reference materials\"},\n    {\"name\": \"2024_environmental_ishkoday\", \"display_name\": \"2024 Environmental Report on Ishkoday\", \"description\": \"2024 Environmental Report on Ishkoday\"},\n    {\"name\": \"geoscience_data\", \"display_name\": \"Geoscience Data\", \"description\": \"Geoscience data and findings\"},\n    {\"name\": \"metallurgical_data\", \"display_name\": \"Metallurgical Data\", \"description\": \"Metallurgical test data and results\"},\n    {\"name\": \"technical_reports\", \"display_name\": \"Technical Reports\", \"description\": \"Technical reports and studies\"},\n]\n\nCOMMODITIES = [\"All\", \"Gold\", \"Silver\", \"Copper\", \"Lithium\", \"Nickel\", \"Zinc\", \"Iron Ore\", \"Uranium\", \"Coal\", \"Platinum Group\", \"Rare Earths\", \"Other\"]\n\n\ndef render_ai_training_page(current_user):\n    \"\"\"Render the AI Training management page for admins\"\"\"\n    \n    if not current_user.get('is_admin'):\n        st.error(\"Access Denied: Administrator privileges required.\")\n        return\n    \n    st.title(\"AI Training Center\")\n    st.markdown(\"Upload mining documents to train the AI. The system automatically learns from your files to improve extraction accuracy for both Oreplot Light and Advanced.\")\n    \n    tab1, tab2, tab3 = st.tabs([\n        \"Upload & Train\",\n        \"Training Library\",\n        \"Statistics\"\n    ])\n    \n    with tab1:\n        render_upload_section(current_user)\n    \n    with tab2:\n        render_library_section(current_user)\n    \n    with tab3:\n        render_stats_section(current_user)\n\n\ndef render_upload_section(current_user):\n    \"\"\"Render the simplified document upload section\"\"\"\n    \n    st.markdown(\"### Upload Training Documents\")\n    st.markdown(\"Upload high-quality mining technical reports. The AI will automatically learn from these documents to improve its accuracy.\")\n    \n    with st.container():\n        st.markdown(\"\"\"\n        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 1rem; border-radius: 10px; color: white; margin-bottom: 1rem;\">\n            <h4 style=\"margin: 0; color: white;\">How Training Works</h4>\n            <p style=\"margin: 0.5rem 0 0 0; opacity: 0.9;\">\n                1. Upload your best mining reports (NI 43-101, feasibility studies, etc.)<br>\n                2. The AI automatically processes and learns from them<br>\n                3. When analyzing new documents, the AI uses your training data as reference<br>\n                4. Better training data = more accurate extractions\n            </p>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        category = st.selectbox(\n            \"Document Category\",\n            options=[c[\"name\"] for c in TRAINING_CATEGORIES],\n            format_func=lambda x: next((c[\"display_name\"] for c in TRAINING_CATEGORIES if c[\"name\"] == x), x),\n            help=\"Select the primary type of data in this document\"\n        )\n    \n    with col2:\n        commodity = st.selectbox(\n            \"Commodity Focus\",\n            options=COMMODITIES,\n            help=\"Select the primary commodity covered in this document\"\n        )\n    \n    st.markdown(\"---\")\n    \n    st.markdown(\"**Supported formats:** PDF, DOCX, XLSX, XLS, CSV, TIF, XML, PNG, JPG, JPEG | **Max files per upload:** 80 | **Max size per file:** 10 GB\")\n    \n    uploaded_files = st.file_uploader(\n        \"Select training documents\",\n        type=[\"pdf\", \"docx\", \"xlsx\", \"xls\", \"csv\", \"tif\", \"xml\", \"png\", \"jpg\", \"jpeg\"],\n        accept_multiple_files=True,\n        help=\"Upload high-quality mining technical reports for AI training\"\n    )\n    \n    if uploaded_files:\n        total_size = sum(f.size for f in uploaded_files)\n        st.markdown(f\"**Selected:** {len(uploaded_files)} file(s) ({total_size / (1024*1024):.1f} MB)\")\n        \n        if len(uploaded_files) > 80:\n            st.error(\"âš ï¸ Maximum 80 files per upload. Please select fewer files and try again.\")\n            uploaded_files = None\n        else:\n            with st.expander(\"View selected files\", expanded=False):\n                for f in uploaded_files:\n                    st.markdown(f\"- {f.name} ({f.size / (1024*1024):.1f} MB)\")\n    \n    if st.button(\"Start Training\", type=\"primary\", disabled=not uploaded_files, use_container_width=True):\n        if uploaded_files:\n            total_files = len(uploaded_files)\n            \n            # HARD LIMIT - Prevent crash with 240+ files\n            if total_files > MAX_FILES_PER_UPLOAD:\n                st.error(f\"âŒ Maximum {MAX_FILES_PER_UPLOAD} files allowed per upload. You selected {total_files} files.\")\n                st.stop()\n            \n            progress_container = st.container()\n            \n            with progress_container:\n                overall_progress = st.progress(0)\n                status_text = st.empty()\n                file_status = st.empty()\n                \n                successful = 0\n                failed = 0\n                \n                # Process in batches to avoid timeouts\n                for batch_num in range(0, total_files, BATCH_SIZE):\n                    batch_end = min(batch_num + BATCH_SIZE, total_files)\n                    batch_files = uploaded_files[batch_num:batch_end]\n                    \n                    status_text.markdown(f\"**Processing batch {(batch_num // BATCH_SIZE) + 1}** (files {batch_num + 1}-{batch_end})\")\n                    \n                    for i, uploaded_file in enumerate(batch_files):\n                        file_index = batch_num + i\n                        status_text.markdown(f\"**Processing file {file_index+1} of {total_files}:** {uploaded_file.name}\")\n                        \n                        try:\n                            file_bytes = uploaded_file.read()\n                            file_type = uploaded_file.name.split('.')[-1].lower()\n                            \n                            def update_progress(progress, message):\n                                file_status.text(message)\n                                file_progress = (file_index + progress) / total_files\n                                overall_progress.progress(min(file_progress, 0.99))\n                            \n                            result = process_training_document(\n                                file_bytes=file_bytes,\n                                file_name=uploaded_file.name,\n                                file_type=file_type,\n                                category=category,\n                                commodity=commodity if commodity != \"All\" else \"general\",\n                                user_id=current_user['id'],\n                                progress_callback=update_progress\n                            )\n                            \n                            if result['success']:\n                                successful += 1\n                                file_status.success(f\"âœ“ {result['chunks_created']} chunks from {uploaded_file.name}\")\n                            else:\n                                failed += 1\n                                file_status.error(f\"âœ— {uploaded_file.name}: {result.get('error', 'Unknown error')}\")\n                        except Exception as e:\n                            failed += 1\n                            file_status.error(f\"âœ— {uploaded_file.name}: {str(e)}\")\n                \n                overall_progress.progress(1.0)\n                status_text.empty()\n                \n                if successful > 0:\n                    st.success(f\"âœ“ Successfully trained on {successful} document(s)!\")\n                if failed > 0:\n                    st.warning(f\"âš  {failed} document(s) failed to process.\")\n                \n                st.info(\"The AI will now use these documents as reference when analyzing similar content.\")\n                st.rerun()\n\n\ndef render_library_section(current_user):\n    \"\"\"Render the training library showing all uploaded documents\"\"\"\n    \n    st.markdown(\"### Training Library\")\n    st.markdown(\"View and manage all documents used for AI training.\")\n    \n    documents = get_training_documents()\n    \n    if not documents:\n        st.info(\"No training documents uploaded yet. Go to the 'Upload & Train' tab to add documents.\")\n        return\n    \n    st.markdown(f\"**Total Documents:** {len(documents)}\")\n    \n    col1, col2 = st.columns([3, 1])\n    with col2:\n        if st.button(\"Clear All Training Data\", type=\"secondary\"):\n            st.session_state.confirm_delete_all = True\n    \n    if st.session_state.get('confirm_delete_all'):\n        st.warning(\"Are you sure you want to delete ALL training data? This cannot be undone.\")\n        col1, col2, col3 = st.columns([1, 1, 2])\n        with col1:\n            if st.button(\"Yes, Delete All\", type=\"primary\"):\n                if delete_all_training():\n                    st.success(\"All training data deleted.\")\n                    st.session_state.confirm_delete_all = False\n                    st.rerun()\n        with col2:\n            if st.button(\"Cancel\"):\n                st.session_state.confirm_delete_all = False\n                st.rerun()\n    \n    st.markdown(\"---\")\n    \n    for doc in documents:\n        category_display = next((c[\"display_name\"] for c in TRAINING_CATEGORIES if c[\"name\"] == doc['category']), doc['category'])\n        \n        with st.container():\n            col1, col2, col3, col4 = st.columns([3, 1, 1, 1])\n            \n            with col1:\n                st.markdown(f\"**{doc['file_name']}**\")\n                st.caption(f\"{category_display} | {doc['commodity']}\")\n            \n            with col2:\n                st.metric(\"Chunks\", doc['chunks'])\n            \n            with col3:\n                st.metric(\"Size\", f\"{doc['size_mb']} MB\")\n            \n            with col4:\n                if st.button(\"Delete\", key=f\"del_{doc['file_name']}\", type=\"secondary\"):\n                    if delete_training_document(doc['file_name']):\n                        st.success(f\"Deleted {doc['file_name']}\")\n                        st.rerun()\n                    else:\n                        st.error(\"Failed to delete document\")\n            \n            st.markdown(\"---\")\n\n\ndef render_stats_section(current_user):\n    \"\"\"Render training statistics\"\"\"\n    \n    st.markdown(\"### Training Statistics\")\n    st.markdown(\"Overview of your AI training data.\")\n    \n    stats = get_training_statistics()\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"Total Documents\", stats['total_documents'])\n    \n    with col2:\n        st.metric(\"Total Chunks\", stats['total_chunks'])\n    \n    with col3:\n        st.metric(\"Total Tokens\", f\"{stats['total_tokens']:,}\")\n    \n    with col4:\n        st.metric(\"Total Size\", f\"{stats['total_size_mb']} MB\")\n    \n    st.markdown(\"---\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.markdown(\"#### Categories\")\n        if stats['categories']:\n            for cat, count in stats['categories'].items():\n                cat_display = next((c[\"display_name\"] for c in TRAINING_CATEGORIES if c[\"name\"] == cat), cat)\n                st.markdown(f\"- **{cat_display}:** {count} chunks\")\n        else:\n            st.info(\"No category data yet\")\n    \n    with col2:\n        st.markdown(\"#### Commodities\")\n        if stats['commodities']:\n            for com, count in stats['commodities'].items():\n                st.markdown(f\"- **{com}:** {count} chunks\")\n        else:\n            st.info(\"No commodity data yet\")\n    \n    st.markdown(\"---\")\n    \n    st.markdown(\"#### Activity\")\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        if stats['last_upload']:\n            st.markdown(f\"**Last Upload:** {stats['last_upload'][:10]}\")\n        else:\n            st.markdown(\"**Last Upload:** Never\")\n    \n    with col2:\n        if stats['last_used']:\n            st.markdown(f\"**Last Used in Analysis:** {stats['last_used'][:10]}\")\n        else:\n            st.markdown(\"**Last Used in Analysis:** Never\")\n    \n    st.markdown(\"---\")\n    \n    st.markdown(\"#### How Training Improves Accuracy\")\n    st.markdown(\"\"\"\n    When you analyze documents with Oreplot Light or Advanced:\n    \n    1. The AI searches your training data for similar content\n    2. It retrieves the most relevant examples\n    3. These examples guide the AI to extract data more accurately\n    4. The more quality training data you add, the better the results\n    \n    **Tips for best results:**\n    - Upload complete, high-quality NI 43-101 reports\n    - Include documents with clear resource/reserve tables\n    - Add documents for each commodity type you analyze\n    - More diverse training data = better generalization\n    \"\"\")\n","path":null,"size_bytes":13678,"size_tokens":null},"training_integration.py":{"content":"\"\"\"\nTraining Integration Module\n\nIntegrates approved training examples into AI extraction and analysis prompts\nto improve accuracy through few-shot learning and retrieval-augmented generation.\n\"\"\"\n\nimport json\nfrom typing import List, Dict, Optional\nfrom database import get_db_session\nfrom models import TrainingExample, TrainingCategory\n\nMAX_EXAMPLES_PER_PROMPT = 3\nMIN_QUALITY_SCORE = 7.0\n\n\ndef get_approved_examples(category: str = None, subcategory: str = None, limit: int = MAX_EXAMPLES_PER_PROMPT) -> List[Dict]:\n    \"\"\"\n    Retrieve approved training examples for use in prompts.\n    \n    Args:\n        category: Filter by category (e.g., 'resource_statement', 'financial_table')\n        subcategory: Filter by subcategory\n        limit: Maximum number of examples to return\n    \n    Returns:\n        List of example dictionaries with source_text and extracted_data\n    \"\"\"\n    try:\n        with get_db_session() as db:\n            query = db.query(TrainingExample).filter(\n                TrainingExample.is_approved == True,\n                TrainingExample.quality_score >= MIN_QUALITY_SCORE\n            )\n            \n            if category:\n                query = query.filter(TrainingExample.category == category)\n            \n            if subcategory:\n                query = query.filter(TrainingExample.subcategory == subcategory)\n            \n            examples = query.order_by(\n                TrainingExample.quality_score.desc(),\n                TrainingExample.usage_count.asc()\n            ).limit(limit).all()\n            \n            result = []\n            for ex in examples:\n                result.append({\n                    'id': ex.id,\n                    'category': ex.category,\n                    'subcategory': ex.subcategory,\n                    'example_name': ex.example_name,\n                    'source_text': ex.source_text,\n                    'extracted_data': ex.extracted_data,\n                    'quality_score': ex.quality_score\n                })\n                \n                ex.usage_count = (ex.usage_count or 0) + 1\n            \n            db.commit()\n            return result\n            \n    except Exception as e:\n        return []\n\n\ndef format_examples_for_prompt(examples: List[Dict], format_type: str = 'extraction') -> str:\n    \"\"\"\n    Format training examples for inclusion in prompts.\n    \n    Args:\n        examples: List of example dictionaries\n        format_type: Type of formatting ('extraction', 'validation', 'financial')\n    \n    Returns:\n        Formatted string to include in prompts\n    \"\"\"\n    if not examples:\n        return \"\"\n    \n    formatted_parts = []\n    \n    for i, ex in enumerate(examples, 1):\n        if format_type == 'extraction':\n            formatted_parts.append(f\"\"\"\n--- EXAMPLE {i}: {ex.get('example_name', 'Training Example')} ---\nCATEGORY: {ex.get('category', 'unknown')}\n\nSOURCE TEXT:\n{ex.get('source_text', '')[:2000]}\n\nCORRECT EXTRACTION:\n{json.dumps(ex.get('extracted_data', {}), indent=2)}\n---\n\"\"\")\n        elif format_type == 'validation':\n            formatted_parts.append(f\"\"\"\nREFERENCE EXAMPLE {i} ({ex.get('category', '')}):\n- Source: {ex.get('source_text', '')[:500]}...\n- Key values: {json.dumps(ex.get('extracted_data', {}), indent=2)}\n\"\"\")\n        elif format_type == 'financial':\n            extracted = ex.get('extracted_data', {})\n            formatted_parts.append(f\"\"\"\nFINANCIAL EXAMPLE {i}:\nSource: {ex.get('source_text', '')[:1000]}\nExtracted values:\n{json.dumps(extracted, indent=2)}\n\"\"\")\n    \n    return \"\\n\".join(formatted_parts)\n\n\ndef get_resource_statement_examples(limit: int = 3) -> str:\n    \"\"\"Get formatted examples for resource statement extraction\"\"\"\n    examples = get_approved_examples(category='resource_statement', limit=limit)\n    examples.extend(get_approved_examples(category='reserve_statement', limit=limit))\n    return format_examples_for_prompt(examples[:limit], 'extraction')\n\n\ndef get_financial_table_examples(limit: int = 3) -> str:\n    \"\"\"Get formatted examples for financial table extraction\"\"\"\n    examples = get_approved_examples(category='financial_table', limit=limit)\n    return format_examples_for_prompt(examples, 'financial')\n\n\ndef get_production_schedule_examples(limit: int = 3) -> str:\n    \"\"\"Get formatted examples for production schedule extraction\"\"\"\n    examples = get_approved_examples(category='production_schedule', limit=limit)\n    return format_examples_for_prompt(examples, 'extraction')\n\n\ndef get_all_relevant_examples(commodity: str = None, limit_per_category: int = 2) -> str:\n    \"\"\"\n    Get a diverse set of examples across all categories.\n    \n    Args:\n        commodity: Optional commodity filter\n        limit_per_category: Max examples per category\n    \n    Returns:\n        Combined formatted examples string\n    \"\"\"\n    categories = ['resource_statement', 'reserve_statement', 'financial_table', \n                  'production_schedule', 'cost_breakdown']\n    \n    all_examples = []\n    for cat in categories:\n        examples = get_approved_examples(category=cat, limit=limit_per_category)\n        all_examples.extend(examples)\n    \n    if not all_examples:\n        return \"\"\n    \n    header = \"\"\"\n=== TRAINING EXAMPLES ===\nThe following are verified examples of correct data extraction from mining technical documents.\nUse these as reference patterns for accurate extraction:\n\n\"\"\"\n    return header + format_examples_for_prompt(all_examples[:6], 'extraction')\n\n\ndef build_enhanced_extraction_prompt(base_prompt: str, include_examples: bool = True) -> str:\n    \"\"\"\n    Enhance an extraction prompt with training examples.\n    \n    Args:\n        base_prompt: The original extraction prompt\n        include_examples: Whether to include training examples\n    \n    Returns:\n        Enhanced prompt with examples injected\n    \"\"\"\n    if not include_examples:\n        return base_prompt\n    \n    examples_section = get_all_relevant_examples(limit_per_category=1)\n    \n    if not examples_section:\n        return base_prompt\n    \n    enhanced_prompt = f\"\"\"{examples_section}\n\n=== YOUR TASK ===\n{base_prompt}\n\nIMPORTANT: Follow the extraction patterns shown in the examples above. Ensure numerical accuracy and proper categorization.\"\"\"\n    \n    return enhanced_prompt\n\n\ndef get_training_stats() -> Dict:\n    \"\"\"Get statistics about available training data\"\"\"\n    try:\n        with get_db_session() as db:\n            from sqlalchemy import func\n            \n            total_examples = db.query(func.count(TrainingExample.id)).scalar() or 0\n            approved_examples = db.query(func.count(TrainingExample.id)).filter(\n                TrainingExample.is_approved == True\n            ).scalar() or 0\n            high_quality = db.query(func.count(TrainingExample.id)).filter(\n                TrainingExample.is_approved == True,\n                TrainingExample.quality_score >= MIN_QUALITY_SCORE\n            ).scalar() or 0\n            \n            categories = db.query(\n                TrainingExample.category,\n                func.count(TrainingExample.id)\n            ).filter(TrainingExample.is_approved == True).group_by(TrainingExample.category).all()\n            \n            return {\n                'total_examples': total_examples,\n                'approved_examples': approved_examples,\n                'high_quality_examples': high_quality,\n                'categories': {cat: count for cat, count in categories}\n            }\n    except Exception:\n        return {\n            'total_examples': 0,\n            'approved_examples': 0,\n            'high_quality_examples': 0,\n            'categories': {}\n        }\n","path":null,"size_bytes":7615,"size_tokens":null},"training_rag.py":{"content":"\"\"\"\nEnhanced RAG Training System for Oreplot\nUses vector embeddings for automatic document learning and retrieval.\n\"\"\"\n\nimport os\nimport json\nimport hashlib\nimport numpy as np\nfrom typing import List, Dict, Optional, Tuple\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom database import get_db_session\nfrom models import TrainingEmbedding, TrainingStats\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nCHUNK_SIZE = 1500\nCHUNK_OVERLAP = 200\nMAX_CHUNKS_PER_QUERY = 5\nSIMILARITY_THRESHOLD = 0.5\nMIN_RESULTS_FALLBACK = 2\nMAX_PARALLEL_EMBEDDINGS = 3\n\n\ndef get_openai_client():\n    \"\"\"Get OpenAI client with API key\"\"\"\n    import openai\n    # Check Replit AI Integrations first, then fallback to standard key\n    api_key = os.environ.get('AI_INTEGRATIONS_OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY')\n    if not api_key:\n        return None\n    \n    base_url = os.environ.get('AI_INTEGRATIONS_OPENAI_BASE_URL')\n    if base_url:\n        return openai.OpenAI(api_key=api_key, base_url=base_url)\n    return openai.OpenAI(api_key=api_key)\n\n\ndef create_embedding(text: str) -> Optional[List[float]]:\n    \"\"\"Create embedding vector for text using OpenAI\"\"\"\n    client = get_openai_client()\n    if not client:\n        return None\n    \n    try:\n        text = text.replace(\"\\n\", \" \").strip()\n        if not text:\n            return None\n        \n        response = client.embeddings.create(\n            model=EMBEDDING_MODEL,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Embedding error: {e}\")\n        return None\n\n\ndef create_embedding_for_chunk(args: Tuple[int, str]) -> Tuple[int, str, Optional[List[float]]]:\n    \"\"\"Create embedding for a single chunk (for parallel processing)\"\"\"\n    chunk_index, chunk_text = args\n    embedding = create_embedding(chunk_text)\n    return (chunk_index, chunk_text, embedding)\n\n\ndef create_embeddings_parallel(chunks: List[str], progress_callback=None) -> List[Tuple[int, str, Optional[List[float]]]]:\n    \"\"\"Create embeddings for multiple chunks in parallel\"\"\"\n    results = []\n    total = len(chunks)\n    \n    chunk_args = [(i, chunk) for i, chunk in enumerate(chunks)]\n    \n    with ThreadPoolExecutor(max_workers=MAX_PARALLEL_EMBEDDINGS) as executor:\n        futures = {executor.submit(create_embedding_for_chunk, args): args[0] for args in chunk_args}\n        completed = 0\n        \n        for future in as_completed(futures):\n            result = future.result()\n            results.append(result)\n            completed += 1\n            \n            if progress_callback:\n                progress = 0.4 + (0.5 * (completed / total))\n                progress_callback(progress, f\"Embedding chunk {completed}/{total}...\")\n    \n    results.sort(key=lambda x: x[0])\n    return results\n\n\ndef chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n    \"\"\"Split text into overlapping chunks for better retrieval\"\"\"\n    if not text:\n        return []\n    \n    chunks = []\n    start = 0\n    text_length = len(text)\n    \n    while start < text_length:\n        end = start + chunk_size\n        \n        if end < text_length:\n            break_point = text.rfind('\\n\\n', start, end)\n            if break_point == -1 or break_point <= start:\n                break_point = text.rfind('\\n', start, end)\n            if break_point == -1 or break_point <= start:\n                break_point = text.rfind('. ', start, end)\n            if break_point != -1 and break_point > start:\n                end = break_point + 1\n        \n        chunk = text[start:end].strip()\n        if chunk and len(chunk) > 50:\n            chunks.append(chunk)\n        \n        start = end - overlap if end < text_length else text_length\n    \n    return chunks\n\n\ndef cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n    if not vec1 or not vec2:\n        return 0.0\n    \n    a = np.array(vec1)\n    b = np.array(vec2)\n    \n    dot_product = np.dot(a, b)\n    norm_a = np.linalg.norm(a)\n    norm_b = np.linalg.norm(b)\n    \n    if norm_a == 0 or norm_b == 0:\n        return 0.0\n    \n    return dot_product / (norm_a * norm_b)\n\n\ndef generate_file_id(file_bytes: bytes, file_name: str) -> str:\n    \"\"\"Generate a unique file ID based on content hash and filename\"\"\"\n    content_hash = hashlib.md5(file_bytes).hexdigest()[:8]\n    return f\"{file_name}_{content_hash}\"\n\n\ndef process_training_document(\n    file_bytes: bytes,\n    file_name: str,\n    file_type: str,\n    category: str,\n    commodity: str,\n    user_id: int,\n    progress_callback=None\n) -> Dict:\n    \"\"\"\n    Process a training document: extract text, chunk it, and create embeddings.\n    \n    Returns:\n        Dict with processing results\n    \"\"\"\n    from document_extractor import DocumentExtractor\n    \n    unique_file_id = generate_file_id(file_bytes, file_name)\n    \n    result = {\n        'success': False,\n        'file_name': file_name,\n        'file_id': unique_file_id,\n        'chunks_created': 0,\n        'error': None\n    }\n    \n    try:\n        if progress_callback:\n            progress_callback(0.1, \"Extracting text...\")\n        \n        if file_type.lower() == 'pdf':\n            extracted_text = DocumentExtractor.extract_text_from_pdf(file_bytes)\n        elif file_type.lower() == 'docx':\n            extracted_text = DocumentExtractor.extract_text_from_docx(file_bytes)\n        elif file_type.lower() in ['xlsx', 'xls']:\n            extracted_text = DocumentExtractor.extract_text_from_xlsx(file_bytes)\n        elif file_type.lower() in ['csv', 'txt']:\n            extracted_text = DocumentExtractor.extract_text_from_txt(file_bytes)\n        elif file_type.lower() in ['png', 'jpg', 'jpeg', 'tif']:\n            extracted_text = DocumentExtractor.extract_text_from_image(file_bytes)\n        elif file_type.lower() == 'xml':\n            extracted_text = file_bytes.decode('utf-8', errors='ignore')\n        else:\n            result['error'] = f\"Unsupported file type: {file_type}\"\n            return result\n        \n        if not extracted_text or len(extracted_text) < 100:\n            result['error'] = \"Could not extract sufficient text from document\"\n            return result\n        \n        if progress_callback:\n            progress_callback(0.3, \"Chunking document...\")\n        \n        chunks = chunk_text(extracted_text)\n        \n        if not chunks:\n            result['error'] = \"No valid chunks created from document\"\n            return result\n        \n        if progress_callback:\n            progress_callback(0.4, f\"Creating embeddings for {len(chunks)} chunks (parallel mode: {MAX_PARALLEL_EMBEDDINGS} concurrent)...\")\n        \n        embedding_results = create_embeddings_parallel(chunks, progress_callback)\n        \n        embeddings_created = 0\n        \n        with get_db_session() as db:\n            for chunk_index, chunk_text_content, embedding in embedding_results:\n                if embedding:\n                    training_embedding = TrainingEmbedding(\n                        file_name=unique_file_id,\n                        file_type=file_type,\n                        file_size=len(file_bytes),\n                        chunk_index=chunk_index,\n                        chunk_text=chunk_text_content,\n                        chunk_tokens=len(chunk_text_content.split()),\n                        category=category,\n                        commodity=commodity,\n                        embedding=embedding,\n                        embedding_model=EMBEDDING_MODEL,\n                        chunk_metadata={\n                            'total_chunks': len(chunks),\n                            'original_length': len(extracted_text),\n                            'original_filename': file_name\n                        },\n                        uploaded_by=user_id\n                    )\n                    db.add(training_embedding)\n                    embeddings_created += 1\n            \n            db.commit()\n            \n            update_training_stats(db)\n        \n        if progress_callback:\n            progress_callback(1.0, \"Complete!\")\n        \n        result['success'] = True\n        result['chunks_created'] = embeddings_created\n        result['total_chunks'] = len(chunks)\n        \n    except Exception as e:\n        result['error'] = str(e)\n    \n    return result\n\n\ndef update_training_stats(db):\n    \"\"\"Update global training statistics\"\"\"\n    try:\n        total_chunks = db.query(TrainingEmbedding).count()\n        \n        from sqlalchemy import func\n        total_tokens = db.query(func.sum(TrainingEmbedding.chunk_tokens)).scalar() or 0\n        \n        file_sizes = db.query(\n            TrainingEmbedding.file_name,\n            func.max(TrainingEmbedding.file_size).label('size')\n        ).group_by(TrainingEmbedding.file_name).all()\n        total_size = sum(fs.size or 0 for fs in file_sizes)\n        \n        unique_files = len(file_sizes)\n        \n        category_counts = {}\n        categories = db.query(TrainingEmbedding.category, func.count(TrainingEmbedding.id)).group_by(TrainingEmbedding.category).all()\n        for cat, count in categories:\n            if cat:\n                category_counts[cat] = count\n        \n        commodity_counts = {}\n        commodities = db.query(TrainingEmbedding.commodity, func.count(TrainingEmbedding.id)).group_by(TrainingEmbedding.commodity).all()\n        for com, count in commodities:\n            if com:\n                commodity_counts[com] = count\n        \n        stats = db.query(TrainingStats).first()\n        if not stats:\n            stats = TrainingStats()\n            db.add(stats)\n        \n        stats.total_documents = unique_files\n        stats.total_chunks = total_chunks\n        stats.total_tokens = total_tokens\n        stats.total_size_bytes = total_size\n        stats.categories_count = category_counts\n        stats.commodities_count = commodity_counts\n        stats.last_upload_at = datetime.utcnow()\n        \n        db.commit()\n        \n    except Exception as e:\n        print(f\"Error updating training stats: {e}\")\n\n\ndef retrieve_relevant_training(\n    query_text: str,\n    category: Optional[str] = None,\n    commodity: Optional[str] = None,\n    limit: int = MAX_CHUNKS_PER_QUERY,\n    threshold: float = SIMILARITY_THRESHOLD\n) -> List[Dict]:\n    \"\"\"\n    Retrieve the most relevant training chunks for a given query.\n    \n    Args:\n        query_text: The text to find relevant training for\n        category: Filter by category (optional)\n        commodity: Filter by commodity (optional)\n        limit: Maximum number of results\n        threshold: Minimum similarity threshold\n    \n    Returns:\n        List of relevant training chunks with similarity scores\n    \"\"\"\n    query_embedding = create_embedding(query_text[:8000])\n    if not query_embedding:\n        return []\n    \n    try:\n        with get_db_session() as db:\n            query = db.query(TrainingEmbedding)\n            \n            if category:\n                query = query.filter(TrainingEmbedding.category == category)\n            if commodity:\n                query = query.filter(TrainingEmbedding.commodity == commodity)\n            \n            all_embeddings = query.all()\n            \n            if not all_embeddings:\n                return []\n            \n            all_results = []\n            for emb in all_embeddings:\n                if emb.embedding:\n                    similarity = cosine_similarity(query_embedding, emb.embedding)\n                    all_results.append({\n                        'id': emb.id,\n                        'chunk_text': emb.chunk_text,\n                        'file_name': emb.file_name,\n                        'category': emb.category,\n                        'commodity': emb.commodity,\n                        'similarity': float(similarity)\n                    })\n            \n            all_results.sort(key=lambda x: x['similarity'], reverse=True)\n            \n            results = [r for r in all_results if r['similarity'] >= threshold]\n            \n            if len(results) < MIN_RESULTS_FALLBACK and len(all_results) >= MIN_RESULTS_FALLBACK:\n                results = all_results[:MIN_RESULTS_FALLBACK]\n            \n            stats = db.query(TrainingStats).first()\n            if stats:\n                stats.last_training_used_at = datetime.utcnow()\n                db.commit()\n            \n            return results[:limit]\n            \n    except Exception as e:\n        print(f\"Error retrieving training: {e}\")\n        return []\n\n\ndef normalize_filter(value: Optional[str]) -> Optional[str]:\n    \"\"\"Normalize category/commodity filter values\"\"\"\n    if not value:\n        return None\n    value = value.strip().lower()\n    if value in ('all', 'general', 'any', ''):\n        return None\n    return value\n\n\ndef build_enhanced_context(\n    document_text: str,\n    category: Optional[str] = None,\n    commodity: Optional[str] = None\n) -> str:\n    \"\"\"\n    Build enhanced context by retrieving relevant training content.\n    \n    Args:\n        document_text: The document being analyzed\n        category: Filter training by category\n        commodity: Filter training by commodity\n    \n    Returns:\n        Enhanced context string to prepend to prompts\n    \"\"\"\n    if not document_text or len(document_text) < 100:\n        return \"\"\n    \n    client = get_openai_client()\n    if not client:\n        return \"\"\n    \n    normalized_category = normalize_filter(category)\n    normalized_commodity = normalize_filter(commodity)\n    \n    sample = document_text[:8000] if len(document_text) > 8000 else document_text\n    sample = ' '.join(sample.split())\n    \n    try:\n        relevant_chunks = retrieve_relevant_training(\n            query_text=sample,\n            category=normalized_category,\n            commodity=normalized_commodity,\n            limit=3,\n            threshold=0.5\n        )\n    except Exception:\n        return \"\"\n    \n    if not relevant_chunks:\n        return \"\"\n    \n    context_parts = [\n        \"=== TRAINING REFERENCE MATERIAL ===\",\n        \"The following are examples from similar mining technical documents that demonstrate accurate data extraction patterns:\",\n        \"\"\n    ]\n    \n    for i, chunk in enumerate(relevant_chunks, 1):\n        context_parts.append(f\"--- Reference {i} (from {chunk['file_name']}, similarity: {chunk['similarity']:.0%}) ---\")\n        text = chunk['chunk_text']\n        if len(text) > 2000:\n            text = text[:2000] + \"...\"\n        context_parts.append(text)\n        context_parts.append(\"\")\n    \n    context_parts.append(\"=== END TRAINING REFERENCE ===\")\n    context_parts.append(\"Use these reference patterns to guide accurate extraction from the document below.\")\n    context_parts.append(\"\")\n    \n    return \"\\n\".join(context_parts)\n\n\ndef get_training_statistics() -> Dict:\n    \"\"\"Get current training system statistics\"\"\"\n    try:\n        with get_db_session() as db:\n            stats = db.query(TrainingStats).first()\n            \n            if not stats:\n                return {\n                    'total_documents': 0,\n                    'total_chunks': 0,\n                    'total_tokens': 0,\n                    'total_size_mb': 0,\n                    'categories': {},\n                    'commodities': {},\n                    'last_upload': None,\n                    'last_used': None\n                }\n            \n            return {\n                'total_documents': stats.total_documents or 0,\n                'total_chunks': stats.total_chunks or 0,\n                'total_tokens': stats.total_tokens or 0,\n                'total_size_mb': round((stats.total_size_bytes or 0) / (1024 * 1024), 2),\n                'categories': stats.categories_count or {},\n                'commodities': stats.commodities_count or {},\n                'last_upload': stats.last_upload_at.isoformat() if stats.last_upload_at else None,\n                'last_used': stats.last_training_used_at.isoformat() if stats.last_training_used_at else None\n            }\n    except Exception as e:\n        print(f\"Error getting training stats: {e}\")\n        return {\n            'total_documents': 0,\n            'total_chunks': 0,\n            'total_tokens': 0,\n            'total_size_mb': 0,\n            'categories': {},\n            'commodities': {},\n            'last_upload': None,\n            'last_used': None,\n            'error': str(e)\n        }\n\n\ndef delete_training_document(file_name: str) -> bool:\n    \"\"\"Delete all training embeddings for a specific document\"\"\"\n    try:\n        with get_db_session() as db:\n            deleted = db.query(TrainingEmbedding).filter(\n                TrainingEmbedding.file_name == file_name\n            ).delete(synchronize_session='fetch')\n            db.commit()\n            \n            if deleted > 0:\n                update_training_stats(db)\n            \n            return deleted > 0\n    except Exception as e:\n        print(f\"Error deleting training document: {e}\")\n        return False\n\n\ndef delete_all_training() -> bool:\n    \"\"\"Delete all training data\"\"\"\n    try:\n        with get_db_session() as db:\n            db.query(TrainingEmbedding).delete()\n            \n            stats = db.query(TrainingStats).first()\n            if stats:\n                stats.total_documents = 0\n                stats.total_chunks = 0\n                stats.total_tokens = 0\n                stats.total_size_bytes = 0\n                stats.categories_count = {}\n                stats.commodities_count = {}\n            \n            db.commit()\n            return True\n    except Exception as e:\n        print(f\"Error deleting all training: {e}\")\n        return False\n\n\ndef get_training_documents() -> List[Dict]:\n    \"\"\"Get list of all training documents with stats\"\"\"\n    try:\n        with get_db_session() as db:\n            from sqlalchemy import func\n            \n            docs = db.query(\n                TrainingEmbedding.file_name,\n                TrainingEmbedding.file_type,\n                TrainingEmbedding.category,\n                TrainingEmbedding.commodity,\n                func.count(TrainingEmbedding.id).label('chunks'),\n                func.sum(TrainingEmbedding.chunk_tokens).label('tokens'),\n                func.max(TrainingEmbedding.file_size).label('size'),\n                func.max(TrainingEmbedding.created_at).label('uploaded_at')\n            ).group_by(\n                TrainingEmbedding.file_name,\n                TrainingEmbedding.file_type,\n                TrainingEmbedding.category,\n                TrainingEmbedding.commodity\n            ).order_by(func.max(TrainingEmbedding.created_at).desc()).all()\n            \n            return [{\n                'file_name': d.file_name,\n                'file_type': d.file_type,\n                'category': d.category,\n                'commodity': d.commodity,\n                'chunks': d.chunks,\n                'tokens': d.tokens or 0,\n                'size_mb': round((d.size or 0) / (1024 * 1024), 2),\n                'uploaded_at': d.uploaded_at.isoformat() if d.uploaded_at else None\n            } for d in docs]\n            \n    except Exception as e:\n        print(f\"Error getting training documents: {e}\")\n        return []\n","path":null,"size_bytes":19277,"size_tokens":null},"format_utils.py":{"content":"\"\"\"Utility functions for formatting financial numbers.\"\"\"\n\ndef format_currency(value: float, decimals: int = 1) -> str:\n    \"\"\"\n    Format a financial value as currency with appropriate B/M suffix.\n    \n    Args:\n        value: Numeric value (typically in millions or billions)\n        decimals: Number of decimal places (default: 1)\n    \n    Returns:\n        Formatted string like \"$1.79B\", \"$45.2M\"\n    \n    Examples:\n        format_currency(1792.6) â†’ \"$1.79B\"\n        format_currency(45.2) â†’ \"$45.2M\"\n        format_currency(850000) â†’ \"$850.0B\"\n    \"\"\"\n    if value is None or value == 0:\n        return \"$0M\"\n    \n    # Handle very large numbers (assumed to be in millions already, convert to billions)\n    if abs(value) >= 1000:\n        billions = value / 1000\n        return f\"${billions:.{decimals}f}B\"\n    else:\n        # Already in millions\n        return f\"${value:.{decimals}f}M\"\n\n\ndef format_currency_detailed(value: float, original_unit: str = 'M', decimals: int = 1) -> str:\n    \"\"\"\n    Format currency with knowledge of original unit.\n    \n    Args:\n        value: Numeric value\n        original_unit: 'M' for millions or 'B' for billions (default: 'M')\n        decimals: Number of decimal places\n    \n    Returns:\n        Formatted string like \"$1.79B\", \"$45.2M\"\n    \"\"\"\n    if value is None or value == 0:\n        return f\"$0{original_unit}\"\n    \n    if original_unit == 'M':\n        # Converting from millions\n        if abs(value) >= 1000:\n            return format_currency(value, decimals)\n        else:\n            return f\"${value:.{decimals}f}M\"\n    elif original_unit == 'B':\n        # Already in billions\n        return f\"${value:.{decimals}f}B\"\n    else:\n        return f\"${value:.{decimals}f}{original_unit}\"\n","path":null,"size_bytes":1743,"size_tokens":null}},"version":2}